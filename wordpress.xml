<?xml version="1.0" encoding="UTF-8" ?>
<!-- This is a WordPress eXtended RSS file generated by WordPress as an export of your site. -->
<!-- It contains information about your site's posts, pages, comments, categories, and other content. -->
<!-- You may use this file to transfer that content from one site to another. -->
<!-- This file is not intended to serve as a complete backup of your site. -->

<!-- To import this information into a WordPress site follow these steps: -->
<!-- 1. Log in to that site as an administrator. -->
<!-- 2. Go to Tools: Import in the WordPress admin panel. -->
<!-- 3. Install the "WordPress" importer from the list. -->
<!-- 4. Activate & Run Importer. -->
<!-- 5. Upload this file using the form provided on that page. -->
<!-- 6. You will first be asked to map the authors in this export file to users -->
<!--    on the site. For each author, you may choose to map to an -->
<!--    existing user on the site or to create a new user. -->
<!-- 7. WordPress will then import each of the posts, pages, comments, categories, etc. -->
<!--    contained in this file into your site. -->

<!-- generator="WordPress/4.3.1" created="2015-11-20 20:27" -->
<rss version="2.0"
	xmlns:excerpt="http://wordpress.org/export/1.2/excerpt/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:wp="http://wordpress.org/export/1.2/"
>

<channel>
	<title>Emma&#039;s Research and Television Blog</title>
	<link>http://blogs.umass.edu/etosch</link>
	<description>I &#60;3 Science and T.V.</description>
	<pubDate>Fri, 20 Nov 2015 20:27:51 +0000</pubDate>
	<language>en-US</language>
	<wp:wxr_version>1.2</wp:wxr_version>
	<wp:base_site_url>http://blogs.umass.edu/</wp:base_site_url>
	<wp:base_blog_url>http://blogs.umass.edu/etosch</wp:base_blog_url>

	<wp:author><wp:author_id>20775</wp:author_id><wp:author_login>etosch</wp:author_login><wp:author_email>etosch@cns.umass.edu</wp:author_email><wp:author_display_name><![CDATA[Emma Tosch]]></wp:author_display_name><wp:author_first_name><![CDATA[Emma]]></wp:author_first_name><wp:author_last_name><![CDATA[Tosch]]></wp:author_last_name></wp:author>

	<wp:category><wp:term_id>1098</wp:term_id><wp:category_nicename>conferences</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Conferences]]></wp:cat_name><wp:category_description><![CDATA[Posts relating to conferences I have attended to plan to attend.]]></wp:category_description></wp:category>
	<wp:category><wp:term_id>149628</wp:term_id><wp:category_nicename>coq</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Coq]]></wp:cat_name></wp:category>
	<wp:category><wp:term_id>1546</wp:term_id><wp:category_nicename>film</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Film]]></wp:cat_name></wp:category>
	<wp:category><wp:term_id>149630</wp:term_id><wp:category_nicename>formal-methods</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Formal Methods]]></wp:cat_name></wp:category>
	<wp:category><wp:term_id>68</wp:term_id><wp:category_nicename>research</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Research]]></wp:cat_name><wp:category_description><![CDATA[Posts about research.]]></wp:category_description></wp:category>
	<wp:category><wp:term_id>149638</wp:term_id><wp:category_nicename>stats</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[stats]]></wp:cat_name></wp:category>
	<wp:category><wp:term_id>149609</wp:term_id><wp:category_nicename>surveyman</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Television]]></wp:cat_name><wp:category_description><![CDATA[Posts about television.]]></wp:category_description></wp:category>
	<wp:category><wp:term_id>1</wp:term_id><wp:category_nicename>uncategorized</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Uncategorized]]></wp:cat_name></wp:category>
	<wp:tag><wp:term_id>149622</wp:term_id><wp:tag_slug>obt2014</wp:tag_slug><wp:tag_name><![CDATA[#OBT2014]]></wp:tag_name></wp:tag>
	<wp:tag><wp:term_id>149618</wp:term_id><wp:tag_slug>pldi2014</wp:tag_slug><wp:tag_name><![CDATA[#pldi2014]]></wp:tag_name></wp:tag>
	<wp:tag><wp:term_id>149620</wp:term_id><wp:tag_slug>popl2014</wp:tag_slug><wp:tag_name><![CDATA[#POPL2014]]></wp:tag_name></wp:tag>
	<wp:tag><wp:term_id>149632</wp:term_id><wp:tag_slug>amt</wp:tag_slug><wp:tag_name><![CDATA[amt]]></wp:tag_name></wp:tag>
	<wp:tag><wp:term_id>149640</wp:term_id><wp:tag_slug>casellaberger</wp:tag_slug><wp:tag_name><![CDATA[casella&amp;berger]]></wp:tag_name></wp:tag>
	<wp:tag><wp:term_id>149634</wp:term_id><wp:tag_slug>crowdsourcing</wp:tag_slug><wp:tag_name><![CDATA[crowdsourcing]]></wp:tag_name></wp:tag>
	<wp:tag><wp:term_id>149614</wp:term_id><wp:tag_slug>experiman</wp:tag_slug><wp:tag_name><![CDATA[experiman]]></wp:tag_name></wp:tag>
	<wp:tag><wp:term_id>149626</wp:term_id><wp:tag_slug>greys-anatomy</wp:tag_slug><wp:tag_name><![CDATA[grey's anatomy]]></wp:tag_name></wp:tag>
	<wp:tag><wp:term_id>1117</wp:term_id><wp:tag_slug>scandal</wp:tag_slug><wp:tag_name><![CDATA[scandal]]></wp:tag_name></wp:tag>
	<wp:tag><wp:term_id>149612</wp:term_id><wp:tag_slug>surveyman-2</wp:tag_slug><wp:tag_name><![CDATA[surveyman]]></wp:tag_name><wp:tag_description><![CDATA[SurveyMan is a language and runtime for designing, debugging, and deploying surveys on the web at scale. For more information, see <a href="http://surveyman.org">surveyman.org</a>]]></wp:tag_description></wp:tag>

	<generator>http://wordpress.org/?v=4.3.1</generator>

	<item>
		<title>Survey with a Loop</title>
		<link>http://blogs.umass.edu/etosch/2014/03/11/on-keeping-the-survey-a-dag/survey_loop/</link>
		<pubDate>Tue, 11 Mar 2014 18:44:12 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/survey_loop.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[The probability model of a survey with a loop differs from the model of a survey without one.]]></excerpt:encoded>
		<wp:post_id>116</wp:post_id>
		<wp:post_date>2014-03-11 18:44:12</wp:post_date>
		<wp:post_date_gmt>2014-03-11 18:44:12</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>survey_loop</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>114</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/survey_loop-e1394563716747.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/survey_loop-e1394563716747.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:346;s:6:"height";i:243;s:4:"file";s:38:"2014/03/survey_loop-e1394563716747.png";s:5:"sizes";a:3:{s:9:"thumbnail";a:4:{s:4:"file";s:38:"survey_loop-e1394563716747-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:38:"survey_loop-e1394563716747-300x210.png";s:5:"width";i:300;s:6:"height";i:210;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:23:"survey_loop-624x423.png";s:5:"width";i:624;s:6:"height";i:423;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_backup_sizes</wp:meta_key>
			<wp:meta_value><![CDATA[a:4:{s:9:"full-orig";a:3:{s:5:"width";i:918;s:6:"height";i:623;s:4:"file";s:15:"survey_loop.png";}s:14:"thumbnail-orig";a:4:{s:4:"file";s:23:"survey_loop-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:11:"medium-orig";a:4:{s:4:"file";s:23:"survey_loop-300x203.png";s:5:"width";i:300;s:6:"height";i:203;s:9:"mime-type";s:9:"image/png";}s:19:"post-thumbnail-orig";a:4:{s:4:"file";s:23:"survey_loop-624x423.png";s:5:"width";i:624;s:6:"height";i:423;s:9:"mime-type";s:9:"image/png";}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>twoVersions1</title>
		<link>http://blogs.umass.edu/etosch/2014/03/19/creating-sophisticated-online-surveys-a-case-study/twoversions1/</link>
		<pubDate>Wed, 19 Mar 2014 00:28:15 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/twoVersions1.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>536</wp:post_id>
		<wp:post_date>2014-03-19 00:28:15</wp:post_date>
		<wp:post_date_gmt>2014-03-19 00:28:15</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>twoversions1</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>534</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/twoVersions1.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/twoVersions1.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:592;s:6:"height";i:355;s:4:"file";s:24:"2014/03/twoVersions1.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:24:"twoVersions1-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:24:"twoVersions1-300x179.png";s:5:"width";i:300;s:6:"height";i:179;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>twoVersions2</title>
		<link>http://blogs.umass.edu/etosch/2014/03/19/creating-sophisticated-online-surveys-a-case-study/twoversions2/</link>
		<pubDate>Wed, 19 Mar 2014 01:14:19 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/twoVersions2.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>546</wp:post_id>
		<wp:post_date>2014-03-19 01:14:19</wp:post_date>
		<wp:post_date_gmt>2014-03-19 01:14:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>twoversions2</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>534</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/twoVersions2-e1395191847653.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/twoVersions2-e1395191847653.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:284;s:6:"height";i:186;s:4:"file";s:39:"2014/03/twoVersions2-e1395191847653.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:39:"twoVersions2-e1395191847653-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:24:"twoVersions2-300x179.png";s:5:"width";i:300;s:6:"height";i:179;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_backup_sizes</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:9:"full-orig";a:3:{s:5:"width";i:592;s:6:"height";i:355;s:4:"file";s:16:"twoVersions2.png";}s:14:"thumbnail-orig";a:4:{s:4:"file";s:24:"twoVersions2-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:11:"medium-orig";a:4:{s:4:"file";s:24:"twoVersions2-300x179.png";s:5:"width";i:300;s:6:"height";i:179;s:9:"mime-type";s:9:"image/png";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>twoVersions2</title>
		<link>http://blogs.umass.edu/etosch/2014/03/19/creating-sophisticated-online-surveys-a-case-study/twoversions2-2/</link>
		<pubDate>Wed, 19 Mar 2014 01:26:50 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/twoVersions21.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>568</wp:post_id>
		<wp:post_date>2014-03-19 01:26:50</wp:post_date>
		<wp:post_date_gmt>2014-03-19 01:26:50</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>twoversions2-2</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>534</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/twoVersions21.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/twoVersions21.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:316;s:6:"height";i:211;s:4:"file";s:25:"2014/03/twoVersions21.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:25:"twoVersions21-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:25:"twoVersions21-300x200.png";s:5:"width";i:300;s:6:"height";i:200;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>twoVersions3</title>
		<link>http://blogs.umass.edu/etosch/2014/03/19/creating-sophisticated-online-surveys-a-case-study/twoversions3/</link>
		<pubDate>Wed, 19 Mar 2014 01:44:49 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/twoVersions3.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>578</wp:post_id>
		<wp:post_date>2014-03-19 01:44:49</wp:post_date>
		<wp:post_date_gmt>2014-03-19 01:44:49</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>twoversions3</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>534</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/twoVersions3.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/twoVersions3.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:410;s:6:"height";i:382;s:4:"file";s:24:"2014/03/twoVersions3.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:24:"twoVersions3-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:24:"twoVersions3-300x279.png";s:5:"width";i:300;s:6:"height";i:279;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>twoVersions4</title>
		<link>http://blogs.umass.edu/etosch/2014/03/19/creating-sophisticated-online-surveys-a-case-study/twoversions4/</link>
		<pubDate>Wed, 19 Mar 2014 02:04:39 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/twoVersions4.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>588</wp:post_id>
		<wp:post_date>2014-03-19 02:04:39</wp:post_date>
		<wp:post_date_gmt>2014-03-19 02:04:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>twoversions4</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>534</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/twoVersions4.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/twoVersions4.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:410;s:6:"height";i:382;s:4:"file";s:24:"2014/03/twoVersions4.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:24:"twoVersions4-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:24:"twoVersions4-300x279.png";s:5:"width";i:300;s:6:"height";i:279;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>twoVersions5</title>
		<link>http://blogs.umass.edu/etosch/2014/03/19/creating-sophisticated-online-surveys-a-case-study/twoversions5/</link>
		<pubDate>Wed, 19 Mar 2014 02:35:18 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/twoVersions5.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>606</wp:post_id>
		<wp:post_date>2014-03-19 02:35:18</wp:post_date>
		<wp:post_date_gmt>2014-03-19 02:35:18</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>twoversions5</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>534</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/twoVersions5.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/twoVersions5.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:381;s:6:"height";i:420;s:4:"file";s:24:"2014/03/twoVersions5.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:24:"twoVersions5-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:24:"twoVersions5-272x300.png";s:5:"width";i:272;s:6:"height";i:300;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>orderings</title>
		<link>http://blogs.umass.edu/etosch/2014/03/22/top-level-orderings/orderings/</link>
		<pubDate>Sat, 22 Mar 2014 03:33:59 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/orderings.pdf</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>724</wp:post_id>
		<wp:post_date>2014-03-22 03:33:59</wp:post_date>
		<wp:post_date_gmt>2014-03-22 03:33:59</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>orderings</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>722</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/orderings.pdf</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/orderings.pdf]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>orderings</title>
		<link>http://blogs.umass.edu/etosch/2014/03/22/top-level-orderings/orderings-2/</link>
		<pubDate>Sat, 22 Mar 2014 03:35:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/orderings.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>728</wp:post_id>
		<wp:post_date>2014-03-22 03:35:00</wp:post_date>
		<wp:post_date_gmt>2014-03-22 03:35:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>orderings-2</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>722</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/orderings.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/orderings.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:627;s:6:"height";i:693;s:4:"file";s:21:"2014/03/orderings.png";s:5:"sizes";a:3:{s:9:"thumbnail";a:4:{s:4:"file";s:21:"orderings-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:21:"orderings-271x300.png";s:5:"width";i:271;s:6:"height";i:300;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:21:"orderings-624x689.png";s:5:"width";i:624;s:6:"height";i:689;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>flow</title>
		<link>http://blogs.umass.edu/etosch/2014/03/22/runtime/flow/</link>
		<pubDate>Sat, 22 Mar 2014 14:28:47 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/flow.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>744</wp:post_id>
		<wp:post_date>2014-03-22 14:28:47</wp:post_date>
		<wp:post_date_gmt>2014-03-22 14:28:47</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>flow</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>710</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/flow.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/flow.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:485;s:6:"height";i:479;s:4:"file";s:16:"2014/03/flow.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:16:"flow-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:16:"flow-300x296.png";s:5:"width";i:300;s:6:"height";i:296;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Screen Shot 2014-03-24 at 6.59.37 PM</title>
		<link>http://blogs.umass.edu/etosch/2014/03/25/experiment-report-i-breakoff-bot-detection-and-correlation-analysis-for-flat-likert-scale-surveys/screen-shot-2014-03-24-at-6-59-37-pm/</link>
		<pubDate>Mon, 24 Mar 2014 23:00:12 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/Screen-Shot-2014-03-24-at-6.59.37-PM.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>924</wp:post_id>
		<wp:post_date>2014-03-24 23:00:12</wp:post_date>
		<wp:post_date_gmt>2014-03-24 23:00:12</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>screen-shot-2014-03-24-at-6-59-37-pm</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>266</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/Screen-Shot-2014-03-24-at-6.59.37-PM.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/Screen-Shot-2014-03-24-at-6.59.37-PM.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:384;s:6:"height";i:128;s:4:"file";s:48:"2014/03/Screen-Shot-2014-03-24-at-6.59.37-PM.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:48:"Screen-Shot-2014-03-24-at-6.59.37-PM-150x128.png";s:5:"width";i:150;s:6:"height";i:128;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:48:"Screen-Shot-2014-03-24-at-6.59.37-PM-300x100.png";s:5:"width";i:300;s:6:"height";i:100;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Screen Shot 2014-03-24 at 7.02.27 PM</title>
		<link>http://blogs.umass.edu/etosch/2014/03/25/experiment-report-i-breakoff-bot-detection-and-correlation-analysis-for-flat-likert-scale-surveys/screen-shot-2014-03-24-at-7-02-27-pm/</link>
		<pubDate>Mon, 24 Mar 2014 23:03:03 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/03/Screen-Shot-2014-03-24-at-7.02.27-PM.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>930</wp:post_id>
		<wp:post_date>2014-03-24 23:03:03</wp:post_date>
		<wp:post_date_gmt>2014-03-24 23:03:03</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>screen-shot-2014-03-24-at-7-02-27-pm</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>266</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/03/Screen-Shot-2014-03-24-at-7.02.27-PM.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/03/Screen-Shot-2014-03-24-at-7.02.27-PM.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:369;s:6:"height";i:124;s:4:"file";s:48:"2014/03/Screen-Shot-2014-03-24-at-7.02.27-PM.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:48:"Screen-Shot-2014-03-24-at-7.02.27-PM-150x124.png";s:5:"width";i:150;s:6:"height";i:124;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:48:"Screen-Shot-2014-03-24-at-7.02.27-PM-300x100.png";s:5:"width";i:300;s:6:"height";i:100;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Screen Shot 2014-07-31 at 7.04.57 PM</title>
		<link>http://blogs.umass.edu/etosch/2014/07/31/socialsci/screen-shot-2014-07-31-at-7-04-57-pm/</link>
		<pubDate>Thu, 31 Jul 2014 23:07:37 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/07/Screen-Shot-2014-07-31-at-7.04.57-PM.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1920</wp:post_id>
		<wp:post_date>2014-07-31 19:07:37</wp:post_date>
		<wp:post_date_gmt>2014-07-31 23:07:37</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>screen-shot-2014-07-31-at-7-04-57-pm</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>1900</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/07/Screen-Shot-2014-07-31-at-7.04.57-PM.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/07/Screen-Shot-2014-07-31-at-7.04.57-PM.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:1440;s:6:"height";i:900;s:4:"file";s:48:"2014/07/Screen-Shot-2014-07-31-at-7.04.57-PM.png";s:5:"sizes";a:5:{s:9:"thumbnail";a:4:{s:4:"file";s:48:"Screen-Shot-2014-07-31-at-7.04.57-PM-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:48:"Screen-Shot-2014-07-31-at-7.04.57-PM-300x187.png";s:5:"width";i:300;s:6:"height";i:187;s:9:"mime-type";s:9:"image/png";}s:5:"large";a:4:{s:4:"file";s:49:"Screen-Shot-2014-07-31-at-7.04.57-PM-1024x640.png";s:5:"width";i:1024;s:6:"height";i:640;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:48:"Screen-Shot-2014-07-31-at-7.04.57-PM-600x375.png";s:5:"width";i:600;s:6:"height";i:375;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:48:"Screen-Shot-2014-07-31-at-7.04.57-PM-624x390.png";s:5:"width";i:624;s:6:"height";i:390;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Screen Shot 2014-09-29 at 9.46.33 PM</title>
		<link>http://blogs.umass.edu/etosch/2014/09/30/what-happened-to-mark-brendanawicz/screen-shot-2014-09-29-at-9-46-33-pm/</link>
		<pubDate>Tue, 30 Sep 2014 05:02:59 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-9.46.33-PM.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2228</wp:post_id>
		<wp:post_date>2014-09-30 01:02:59</wp:post_date>
		<wp:post_date_gmt>2014-09-30 05:02:59</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>screen-shot-2014-09-29-at-9-46-33-pm</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2224</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-9.46.33-PM.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/09/Screen-Shot-2014-09-29-at-9.46.33-PM.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:515;s:6:"height";i:200;s:4:"file";s:48:"2014/09/Screen-Shot-2014-09-29-at-9.46.33-PM.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:48:"Screen-Shot-2014-09-29-at-9.46.33-PM-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:48:"Screen-Shot-2014-09-29-at-9.46.33-PM-300x116.png";s:5:"width";i:300;s:6:"height";i:116;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Screen Shot 2014-09-29 at 9.47.17 PM</title>
		<link>http://blogs.umass.edu/etosch/?attachment_id=2230</link>
		<pubDate>Tue, 30 Sep 2014 05:03:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-9.47.17-PM.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2230</wp:post_id>
		<wp:post_date>2014-09-30 01:03:00</wp:post_date>
		<wp:post_date_gmt>2014-09-30 05:03:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>screen-shot-2014-09-29-at-9-47-17-pm</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-9.47.17-PM.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/09/Screen-Shot-2014-09-29-at-9.47.17-PM.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:513;s:6:"height";i:195;s:4:"file";s:48:"2014/09/Screen-Shot-2014-09-29-at-9.47.17-PM.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:48:"Screen-Shot-2014-09-29-at-9.47.17-PM-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:48:"Screen-Shot-2014-09-29-at-9.47.17-PM-300x114.png";s:5:"width";i:300;s:6:"height";i:114;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Screen Shot 2014-09-29 at 9.58.17 PM</title>
		<link>http://blogs.umass.edu/etosch/2014/09/30/what-happened-to-mark-brendanawicz/screen-shot-2014-09-29-at-9-58-17-pm/</link>
		<pubDate>Tue, 30 Sep 2014 05:03:10 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-9.58.17-PM.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2232</wp:post_id>
		<wp:post_date>2014-09-30 01:03:10</wp:post_date>
		<wp:post_date_gmt>2014-09-30 05:03:10</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>screen-shot-2014-09-29-at-9-58-17-pm</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2224</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-9.58.17-PM.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/09/Screen-Shot-2014-09-29-at-9.58.17-PM.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:506;s:6:"height";i:175;s:4:"file";s:48:"2014/09/Screen-Shot-2014-09-29-at-9.58.17-PM.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:48:"Screen-Shot-2014-09-29-at-9.58.17-PM-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:48:"Screen-Shot-2014-09-29-at-9.58.17-PM-300x103.png";s:5:"width";i:300;s:6:"height";i:103;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Screen Shot 2014-09-29 at 10.10.01 PM</title>
		<link>http://blogs.umass.edu/etosch/2014/09/30/what-happened-to-mark-brendanawicz/screen-shot-2014-09-29-at-10-10-01-pm/</link>
		<pubDate>Tue, 30 Sep 2014 05:13:39 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-10.10.01-PM.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2238</wp:post_id>
		<wp:post_date>2014-09-30 01:13:39</wp:post_date>
		<wp:post_date_gmt>2014-09-30 05:13:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>screen-shot-2014-09-29-at-10-10-01-pm</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2224</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-10.10.01-PM.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/09/Screen-Shot-2014-09-29-at-10.10.01-PM.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:509;s:6:"height";i:193;s:4:"file";s:49:"2014/09/Screen-Shot-2014-09-29-at-10.10.01-PM.png";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:49:"Screen-Shot-2014-09-29-at-10.10.01-PM-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:49:"Screen-Shot-2014-09-29-at-10.10.01-PM-300x113.png";s:5:"width";i:300;s:6:"height";i:113;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Screen Shot 2014-11-02 at 9.08.27 PM</title>
		<link>http://blogs.umass.edu/etosch/2014/11/03/the-one-where-im-a-debbie-downer/screen-shot-2014-11-02-at-9-08-27-pm/</link>
		<pubDate>Mon, 03 Nov 2014 05:09:58 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/11/Screen-Shot-2014-11-02-at-9.08.27-PM.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2270</wp:post_id>
		<wp:post_date>2014-11-03 00:09:58</wp:post_date>
		<wp:post_date_gmt>2014-11-03 05:09:58</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>screen-shot-2014-11-02-at-9-08-27-pm</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2260</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/11/Screen-Shot-2014-11-02-at-9.08.27-PM.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/11/Screen-Shot-2014-11-02-at-9.08.27-PM.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:982;s:6:"height";i:706;s:4:"file";s:48:"2014/11/Screen-Shot-2014-11-02-at-9.08.27-PM.png";s:5:"sizes";a:4:{s:9:"thumbnail";a:4:{s:4:"file";s:48:"Screen-Shot-2014-11-02-at-9.08.27-PM-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:48:"Screen-Shot-2014-11-02-at-9.08.27-PM-300x215.png";s:5:"width";i:300;s:6:"height";i:215;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:48:"Screen-Shot-2014-11-02-at-9.08.27-PM-600x431.png";s:5:"width";i:600;s:6:"height";i:431;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:48:"Screen-Shot-2014-11-02-at-9.08.27-PM-624x448.png";s:5:"width";i:624;s:6:"height";i:448;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Screen Shot 2014-12-18 at 10.56.58 AM</title>
		<link>http://blogs.umass.edu/etosch/2014/12/18/what-code-review-should-actually-be-like-and-related-problems-with-ai/screen-shot-2014-12-18-at-10-56-58-am/</link>
		<pubDate>Thu, 18 Dec 2014 15:57:48 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/12/Screen-Shot-2014-12-18-at-10.56.58-AM.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2312</wp:post_id>
		<wp:post_date>2014-12-18 10:57:48</wp:post_date>
		<wp:post_date_gmt>2014-12-18 15:57:48</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>screen-shot-2014-12-18-at-10-56-58-am</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2310</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/12/Screen-Shot-2014-12-18-at-10.56.58-AM.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/12/Screen-Shot-2014-12-18-at-10.56.58-AM.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:784;s:6:"height";i:505;s:4:"file";s:49:"2014/12/Screen-Shot-2014-12-18-at-10.56.58-AM.png";s:5:"sizes";a:4:{s:9:"thumbnail";a:4:{s:4:"file";s:49:"Screen-Shot-2014-12-18-at-10.56.58-AM-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:49:"Screen-Shot-2014-12-18-at-10.56.58-AM-300x193.png";s:5:"width";i:300;s:6:"height";i:193;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:49:"Screen-Shot-2014-12-18-at-10.56.58-AM-600x386.png";s:5:"width";i:600;s:6:"height";i:386;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:49:"Screen-Shot-2014-12-18-at-10.56.58-AM-624x401.png";s:5:"width";i:624;s:6:"height";i:401;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Screen Shot 2014-12-18 at 11.00.34 AM</title>
		<link>http://blogs.umass.edu/etosch/2014/12/18/what-code-review-should-actually-be-like-and-related-problems-with-ai/screen-shot-2014-12-18-at-11-00-34-am/</link>
		<pubDate>Thu, 18 Dec 2014 16:01:35 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2014/12/Screen-Shot-2014-12-18-at-11.00.34-AM.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2316</wp:post_id>
		<wp:post_date>2014-12-18 11:01:35</wp:post_date>
		<wp:post_date_gmt>2014-12-18 16:01:35</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>screen-shot-2014-12-18-at-11-00-34-am</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2310</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2014/12/Screen-Shot-2014-12-18-at-11.00.34-AM.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2014/12/Screen-Shot-2014-12-18-at-11.00.34-AM.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:933;s:6:"height";i:818;s:4:"file";s:49:"2014/12/Screen-Shot-2014-12-18-at-11.00.34-AM.png";s:5:"sizes";a:4:{s:9:"thumbnail";a:4:{s:4:"file";s:49:"Screen-Shot-2014-12-18-at-11.00.34-AM-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:49:"Screen-Shot-2014-12-18-at-11.00.34-AM-300x263.png";s:5:"width";i:300;s:6:"height";i:263;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:49:"Screen-Shot-2014-12-18-at-11.00.34-AM-600x526.png";s:5:"width";i:600;s:6:"height";i:526;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:49:"Screen-Shot-2014-12-18-at-11.00.34-AM-624x547.png";s:5:"width";i:624;s:6:"height";i:547;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:10:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>On random sampling in surveys</title>
		<link>http://blogs.umass.edu/etosch/?p=23</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=23</guid>
		<description></description>
		<content:encoded><![CDATA[I've had some recent discussions with <a href="http://people.umass.edu/ppizzo/">Presley</a>regarding features in SurveyMan that would make it a better tool for experimentation. One of these features is
The <a href="https://github.com/etosch/SurveyMan/wiki/Csv-Spec">SurveyMan CSV spec</a> was designed as a kind of]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>23</wp:post_id>
		<wp:post_date>2014-02-25 14:23:43</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Why Research *and* Television?</title>
		<link>http://blogs.umass.edu/etosch/why-research-and-television/</link>
		<pubDate>Sat, 01 Mar 2014 01:43:34 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?page_id=48</guid>
		<description></description>
		<content:encoded><![CDATA[One day in my lab, a colleague asked me what my favorite film was. I prefaced my answer with "You will judge me" and replied, <em>Team America : World Police</em>. I explained that the reason I consider it my favorite film is because it's the film I've watched the most number of times. This was meant to be a shameful admission, but instead of judging my obviously unsophisticated taste, he remarked that my justification was unusual and refreshingly honest. Our self-reported preferences are so often about social expectations, rather than data. 

I spend the vast majority of my day thinking about my research. I've love to report that when I need to unwind, I enjoy reading Thomas Pynchon and learning to play <em>Gaspard de la Nuit</em>. If these things were true, I might command the instant respect and jealousy of all the plebes who dare engage me in dialogues of wit and charm. If these things were true, I'd probably be better read and speak something other than the East Coast dialect of American English. I'd know something about wine and Scotch and even if I still preferred Cheddar to Gruyere, I'd have a better explanation than my ability to actually pick out the former from a cheese platter. 
 
The truth is, I prefer spending time with the creations of Shonda Rhimes and J.J. Abrams and 100% do not care about mastering my understanding of esoteric works of 20th century (post)?modernist masters. I spend a lot of time thinking about how television communicates values and appeals to us (or rather than "us," people like me?) on an emotional level. I think of the characters in my shows as my friends. I admire Peggy Olsen and Veronica Palmer. I wish J.J. gave Astrid Farnsworth more air time and appreciate how Amy Wong can be a wants-for-nothing ditzy physicist with a huge heart and an open mind. I've thought about the idea of having a greater diversity of people in STEM on television since I started in Computer Science (recently read an article about Juan Gilbert on this topic) and since I spend so much time thinking about television, I thought it would be worth including in this blog. 

It's important to write every day. While I'm not there yet, I hope to intersperse posts about cool papers and my current research with thoughts on various sitcoms and dramedies. That's what this blog is about. Enjoy!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>48</wp:post_id>
		<wp:post_date>2014-03-01 01:43:34</wp:post_date>
		<wp:post_date_gmt>2014-03-01 01:43:34</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>why-research-and-television</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>page</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_page_template</wp:meta_key>
			<wp:meta_value><![CDATA[default]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>16</wp:comment_id>
			<wp:comment_author><![CDATA[SK]]></wp:comment_author>
			<wp:comment_author_email>sarackingsley@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>71.192.207.194</wp:comment_author_IP>
			<wp:comment_date>2014-03-19 05:30:37</wp:comment_date>
			<wp:comment_date_gmt>2014-03-19 05:30:37</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[How to do a research blog well.  Kudos, and great writing!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[true]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1395207037.9962680339813232421875;s:7:"message";s:35:"Akismet caught this comment as spam";s:5:"event";s:10:"check-spam";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1395208162.525495052337646484375;s:7:"message";s:40:"etosch reported this comment as not spam";s:5:"event";s:10:"report-ham";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_user_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_user</wp:meta_key>
				<wp:meta_value><![CDATA[etosch]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1395208168.926786899566650390625;s:7:"message";s:45:"etosch changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Finding bad actors for Likert Scales</title>
		<link>http://blogs.umass.edu/etosch/?p=130</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=130</guid>
		<description></description>
		<content:encoded><![CDATA[One of the key features of our Adversary detection is ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>130</wp:post_id>
		<wp:post_date>2014-03-11 19:47:24</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Bug Detection I : Bad Actors</title>
		<link>http://blogs.umass.edu/etosch/?p=282</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=282</guid>
		<description></description>
		<content:encoded><![CDATA[The ability to flag bad actors automatically is a crucial component of SurveyMan's quality control and runtime system. Validating that someone is a bad actor, however, is not trivial -- we don't have an oracle to tell us if someone is gaming the system, and we can't just contact a worker and ask them if they were behaving badly. In a <a href="http://blogs.umass.edu/etosch/2014/03/13/adversaries/">previous post</a>, we talked about adversary models for bad respondents. The purpose of that post was to describe their behavior.

We also need to consider the cost associated with making a mistake in our classification. Classifying all respondents as valid when there exist bots and spammers is not good for obvious reasons, especially when the proportion of bad actors drowns out the signal from our target population. If we expect the sample to draw from a single population, rather than subpopulations, an aggressive bot-detection policy would be tempting. SurveyMan makes it easy to re-run surveys, and since the cost is so low, why not? 

Unfortunately, crowdsourcing platforms such as AMT use rejection of work as a measure of a worker's ability. Other requesters may filter workers on this basis. Since workers on crowdsourcing platforms value their ratings higher than the payment for any individual job, aggressive rejection of work has a greater impact on the workers than just the lost income. Furthermore, the presence of websites like www.mturkforum.com and tools like turkopticon give workers a platform to share experiences with bad requesters. The cost of making mistakes is high -- the requester may be blacklisted by the workers (this is not a hypothetical concern; I have anecdotes!).
 Indeed, other researchers have shied away from outright rejecting work : "Generally, we do not advocate excluding participants except under the most extremely obvious situations of abuse (e.g., pushing the same button the entire time)." (<a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0057410">Crump et al 2013</a>). 

Any adversary detection must have a two tiered classification scheme : when we are sure that a respondent is a bot or a spammer, we should reject. If the respondent merely looks lazy, we should flag them.

MaxEnt, MaxLogLikelihood, and Bias all implicitly assume something we know is not true : that the questions are independent. This assumption of independence makes us falsely assume that certain operations are linear, which causes us to perform lossy calculations on the data.

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>282</wp:post_id>
		<wp:post_date>2014-03-15 17:06:04</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Bug Detection II : Breakoff</title>
		<link>http://blogs.umass.edu/etosch/?p=286</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=286</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
Finally, it is important to monitor and record the rate at which people begin an experiment but do not finish. This is typically not a problem in laboratory studies since the social pressure of getting up a walking out of the lab is much higher than it is online. However, dropout rates can interact in complex ways with dependent measures such as accuracy (low performing individuals may be more likely to drop out). We recommend that, perhaps unlike a typical laboratory study, all Internet experiments report dropout rates as a function of condition.

Dropout rate may also depend on task length, financial incentive, and other motivations to complete the task. Our studies validated a range of task lengths from 530 min with a range of relatively low financial incentives. Across tasks, dropout rates were not prohibitively high, and we expect that these rates would naturally change to the extent that subjects are given incentive to complete the task at hand. We did not conduct lengthier experiments (e.g., more than one hour long, or multi-day experiments); however, our experience leads us to believe that these types of experiments could be conducted by increasing pay and restricting the experiment to highly motivated and accomplished workers. (<a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0057410">Crump et al 2013</a>)
</blockquote>

Our algorithm for detecting breakoff is quite simple, but implementing it has presented some challenges. AMT does not allow requesters to record information from a HIT if the worker has not elected to submit their work. This makes measure abandonment difficult. Typically researchers have dealt with this by pointing workers to externally hosted surveys. Services like SurveyMonkey and Qualtrics allow users to generate confirmation codes, which will be presented to respondents upon completion of the survey. The worker then enters that code into the HIT as evidence of having completed the survey. The requester verifies the code and on that basis awards payment. 

We would like to keep the respondent on the same page as the ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>286</wp:post_id>
		<wp:post_date>2014-03-15 15:20:30</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>How susceptible are our surveys to bots?</title>
		<link>http://blogs.umass.edu/etosch/?p=304</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=304</guid>
		<description></description>
		<content:encoded><![CDATA[http://userscripts.org/scripts/review/2327
Does this script assume that all the fields are already there?

http://thebot.net/bot-requests/3405-survey-taker-bot/]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>304</wp:post_id>
		<wp:post_date>2014-03-15 15:26:41</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Block Randomization : What are the eligible indices?</title>
		<link>http://blogs.umass.edu/etosch/?p=602</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=602</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>602</wp:post_id>
		<wp:post_date>2014-03-19 02:21:21</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Detecting Bugs : Variants and Question Order</title>
		<link>http://blogs.umass.edu/etosch/?p=876</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=876</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>876</wp:post_id>
		<wp:post_date>2014-03-24 20:27:57</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>accuracy-entropy-ratio</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/accuracy-entropy-ratio/</link>
		<pubDate>Wed, 20 May 2015 02:42:39 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2015/05/accuracy-entropy-ratio.pdf</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2717</wp:post_id>
		<wp:post_date>2015-05-19 22:42:39</wp:post_date>
		<wp:post_date_gmt>2015-05-20 02:42:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>accuracy-entropy-ratio</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2220</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2015/05/accuracy-entropy-ratio.pdf</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2015/05/accuracy-entropy-ratio.pdf]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>accuracy-entropy-ratio</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/accuracy-entropy-ratio-2/</link>
		<pubDate>Wed, 20 May 2015 02:43:39 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2015/05/accuracy-entropy-ratio.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2721</wp:post_id>
		<wp:post_date>2015-05-19 22:43:39</wp:post_date>
		<wp:post_date_gmt>2015-05-20 02:43:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>accuracy-entropy-ratio-2</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2220</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2015/05/accuracy-entropy-ratio.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2015/05/accuracy-entropy-ratio.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:2679;s:6:"height";i:1150;s:4:"file";s:34:"2015/05/accuracy-entropy-ratio.png";s:5:"sizes";a:5:{s:9:"thumbnail";a:4:{s:4:"file";s:34:"accuracy-entropy-ratio-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:34:"accuracy-entropy-ratio-300x129.png";s:5:"width";i:300;s:6:"height";i:129;s:9:"mime-type";s:9:"image/png";}s:5:"large";a:4:{s:4:"file";s:35:"accuracy-entropy-ratio-1024x440.png";s:5:"width";i:1024;s:6:"height";i:440;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:34:"accuracy-entropy-ratio-600x258.png";s:5:"width";i:600;s:6:"height";i:258;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:34:"accuracy-entropy-ratio-624x268.png";s:5:"width";i:624;s:6:"height";i:268;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:11:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";s:11:"orientation";i:0;}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>main</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/main/</link>
		<pubDate>Wed, 20 May 2015 02:54:53 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2015/05/main.pdf</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2725</wp:post_id>
		<wp:post_date>2015-05-19 22:54:53</wp:post_date>
		<wp:post_date_gmt>2015-05-20 02:54:53</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>main</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2220</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2015/05/main.pdf</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2015/05/main.pdf]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>empirical-entropy-accuracy</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/empirical-entropy-accuracy/</link>
		<pubDate>Wed, 20 May 2015 03:38:36 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2015/05/empirical-entropy-accuracy.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2739</wp:post_id>
		<wp:post_date>2015-05-19 23:38:36</wp:post_date>
		<wp:post_date_gmt>2015-05-20 03:38:36</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>empirical-entropy-accuracy</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2220</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2015/05/empirical-entropy-accuracy.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2015/05/empirical-entropy-accuracy.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:2679;s:6:"height";i:1150;s:4:"file";s:38:"2015/05/empirical-entropy-accuracy.png";s:5:"sizes";a:5:{s:9:"thumbnail";a:4:{s:4:"file";s:38:"empirical-entropy-accuracy-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:38:"empirical-entropy-accuracy-300x129.png";s:5:"width";i:300;s:6:"height";i:129;s:9:"mime-type";s:9:"image/png";}s:5:"large";a:4:{s:4:"file";s:39:"empirical-entropy-accuracy-1024x440.png";s:5:"width";i:1024;s:6:"height";i:440;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:38:"empirical-entropy-accuracy-600x258.png";s:5:"width";i:600;s:6:"height";i:258;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:38:"empirical-entropy-accuracy-624x268.png";s:5:"width";i:624;s:6:"height";i:268;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:11:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";s:11:"orientation";i:0;}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>roc20perc</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/roc20perc/</link>
		<pubDate>Wed, 20 May 2015 03:48:57 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2015/05/roc20perc.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2743</wp:post_id>
		<wp:post_date>2015-05-19 23:48:57</wp:post_date>
		<wp:post_date_gmt>2015-05-20 03:48:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>roc20perc</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2220</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2015/05/roc20perc.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2015/05/roc20perc.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:2679;s:6:"height";i:1150;s:4:"file";s:21:"2015/05/roc20perc.png";s:5:"sizes";a:5:{s:9:"thumbnail";a:4:{s:4:"file";s:21:"roc20perc-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:21:"roc20perc-300x129.png";s:5:"width";i:300;s:6:"height";i:129;s:9:"mime-type";s:9:"image/png";}s:5:"large";a:4:{s:4:"file";s:22:"roc20perc-1024x440.png";s:5:"width";i:1024;s:6:"height";i:440;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:21:"roc20perc-600x258.png";s:5:"width";i:600;s:6:"height";i:258;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:21:"roc20perc-624x268.png";s:5:"width";i:624;s:6:"height";i:268;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:11:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";s:11:"orientation";i:0;}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>accuracy_entropy</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/accuracy_entropy/</link>
		<pubDate>Wed, 20 May 2015 12:06:01 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2015/05/accuracy_entropy.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2747</wp:post_id>
		<wp:post_date>2015-05-20 08:06:01</wp:post_date>
		<wp:post_date_gmt>2015-05-20 12:06:01</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>accuracy_entropy</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2220</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2015/05/accuracy_entropy.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2015/05/accuracy_entropy.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:643;s:6:"height";i:276;s:4:"file";s:28:"2015/05/accuracy_entropy.png";s:5:"sizes";a:4:{s:9:"thumbnail";a:4:{s:4:"file";s:28:"accuracy_entropy-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:28:"accuracy_entropy-300x129.png";s:5:"width";i:300;s:6:"height";i:129;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:28:"accuracy_entropy-600x258.png";s:5:"width";i:600;s:6:"height";i:258;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:28:"accuracy_entropy-624x268.png";s:5:"width";i:624;s:6:"height";i:268;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:11:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";s:11:"orientation";i:0;}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>roc</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/roc/</link>
		<pubDate>Wed, 20 May 2015 13:31:23 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2015/05/roc.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2765</wp:post_id>
		<wp:post_date>2015-05-20 09:31:23</wp:post_date>
		<wp:post_date_gmt>2015-05-20 13:31:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>roc</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2220</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2015/05/roc.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2015/05/roc.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:643;s:6:"height";i:276;s:4:"file";s:15:"2015/05/roc.png";s:5:"sizes";a:4:{s:9:"thumbnail";a:4:{s:4:"file";s:15:"roc-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:15:"roc-300x129.png";s:5:"width";i:300;s:6:"height";i:129;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:15:"roc-600x258.png";s:5:"width";i:600;s:6:"height";i:258;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:15:"roc-624x268.png";s:5:"width";i:624;s:6:"height";i:268;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:11:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";s:11:"orientation";i:0;}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>roc2</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/roc2/</link>
		<pubDate>Wed, 20 May 2015 13:43:24 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2015/05/roc2.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2769</wp:post_id>
		<wp:post_date>2015-05-20 09:43:24</wp:post_date>
		<wp:post_date_gmt>2015-05-20 13:43:24</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>roc2</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2220</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2015/05/roc2.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2015/05/roc2.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:643;s:6:"height";i:276;s:4:"file";s:16:"2015/05/roc2.png";s:5:"sizes";a:4:{s:9:"thumbnail";a:4:{s:4:"file";s:16:"roc2-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:16:"roc2-300x129.png";s:5:"width";i:300;s:6:"height";i:129;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:16:"roc2-600x258.png";s:5:"width";i:600;s:6:"height";i:258;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:16:"roc2-624x268.png";s:5:"width";i:624;s:6:"height";i:268;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:11:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";s:11:"orientation";i:0;}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>entropy_ratio_accuracy</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/entropy_ratio_accuracy/</link>
		<pubDate>Wed, 20 May 2015 15:04:08 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2015/05/entropy_ratio_accuracy.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2771</wp:post_id>
		<wp:post_date>2015-05-20 11:04:08</wp:post_date>
		<wp:post_date_gmt>2015-05-20 15:04:08</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>entropy_ratio_accuracy</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2220</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2015/05/entropy_ratio_accuracy.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2015/05/entropy_ratio_accuracy.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:643;s:6:"height";i:276;s:4:"file";s:34:"2015/05/entropy_ratio_accuracy.png";s:5:"sizes";a:4:{s:9:"thumbnail";a:4:{s:4:"file";s:34:"entropy_ratio_accuracy-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:34:"entropy_ratio_accuracy-300x129.png";s:5:"width";i:300;s:6:"height";i:129;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:34:"entropy_ratio_accuracy-600x258.png";s:5:"width";i:600;s:6:"height";i:258;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:34:"entropy_ratio_accuracy-624x268.png";s:5:"width";i:624;s:6:"height";i:268;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:11:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";s:11:"orientation";i:0;}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>entropy_ratio_accuracy</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/entropy_ratio_accuracy-2/</link>
		<pubDate>Wed, 20 May 2015 15:40:24 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2015/05/entropy_ratio_accuracy1.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2775</wp:post_id>
		<wp:post_date>2015-05-20 11:40:24</wp:post_date>
		<wp:post_date_gmt>2015-05-20 15:40:24</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>entropy_ratio_accuracy-2</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2220</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2015/05/entropy_ratio_accuracy1.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2015/05/entropy_ratio_accuracy1.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:643;s:6:"height";i:276;s:4:"file";s:35:"2015/05/entropy_ratio_accuracy1.png";s:5:"sizes";a:4:{s:9:"thumbnail";a:4:{s:4:"file";s:35:"entropy_ratio_accuracy1-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:35:"entropy_ratio_accuracy1-300x129.png";s:5:"width";i:300;s:6:"height";i:129;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:35:"entropy_ratio_accuracy1-600x258.png";s:5:"width";i:600;s:6:"height";i:258;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:35:"entropy_ratio_accuracy1-624x268.png";s:5:"width";i:624;s:6:"height";i:268;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:11:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";s:11:"orientation";i:0;}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>entropy_ratio_precision</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/entropy_ratio_precision/</link>
		<pubDate>Wed, 20 May 2015 15:42:08 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/files/2015/05/entropy_ratio_precision.png</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2779</wp:post_id>
		<wp:post_date>2015-05-20 11:42:08</wp:post_date>
		<wp:post_date_gmt>2015-05-20 15:42:08</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>entropy_ratio_precision</wp:post_name>
		<wp:status>inherit</wp:status>
		<wp:post_parent>2220</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>attachment</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:attachment_url>http://blogs.umass.edu/etosch/files/2015/05/entropy_ratio_precision.png</wp:attachment_url>
		<wp:postmeta>
			<wp:meta_key>_wp_attached_file</wp:meta_key>
			<wp:meta_value><![CDATA[2015/05/entropy_ratio_precision.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{s:5:"width";i:643;s:6:"height";i:276;s:4:"file";s:35:"2015/05/entropy_ratio_precision.png";s:5:"sizes";a:4:{s:9:"thumbnail";a:4:{s:4:"file";s:35:"entropy_ratio_precision-150x150.png";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/png";}s:6:"medium";a:4:{s:4:"file";s:35:"entropy_ratio_precision-300x129.png";s:5:"width";i:300;s:6:"height";i:129;s:9:"mime-type";s:9:"image/png";}s:22:"wysija-newsletters-max";a:4:{s:4:"file";s:35:"entropy_ratio_precision-600x258.png";s:5:"width";i:600;s:6:"height";i:258;s:9:"mime-type";s:9:"image/png";}s:14:"post-thumbnail";a:4:{s:4:"file";s:35:"entropy_ratio_precision-624x268.png";s:5:"width";i:624;s:6:"height";i:268;s:9:"mime-type";s:9:"image/png";}}s:10:"image_meta";a:11:{s:8:"aperture";i:0;s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";i:0;s:9:"copyright";s:0:"";s:12:"focal_length";i:0;s:3:"iso";i:0;s:13:"shutter_speed";i:0;s:5:"title";s:0:"";s:11:"orientation";i:0;}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Why blog?</title>
		<link>http://blogs.umass.edu/etosch/why-blog/</link>
		<pubDate>Tue, 02 Jun 2015 15:17:01 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?page_id=2807</guid>
		<description></description>
		<content:encoded><![CDATA[Before the internet, academics and intellectuals shared their musings and worked out problems via letter writing. Read any math "paper" before 1920 and you will notice the casual tone and seeming lack of formality in the notation. These letters were not public, but they were not exactly private either. I see today's blogs as an extension of that letter-writing tradition.

Now, not everyone will agree with me. For some, blogging is more formal -- it functions as a compendium of advice and pithy explanation, or it is an avenue to push work that may not have a home elsewhere. For others, it is more informal. This blog is somewhere between open letters to no one and what might otherwise end up lost in a research notebook or in casual conversation in front of a whiteboard. 

To make blogging truly worth it, it helps to have comments. I've noticed that some graduate students turn off comments in their blogs and prefer to hold the conversation over email. I welcome comments over email, but would prefer if they happened in the open, where others can participate, if they wish. 

So, please enjoy, and be in touch.

Emma]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2807</wp:post_id>
		<wp:post_date>2015-06-02 11:17:01</wp:post_date>
		<wp:post_date_gmt>2015-06-02 15:17:01</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>why-blog</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>page</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_page_template</wp:meta_key>
			<wp:meta_value><![CDATA[default]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>On the relationship between n and your instrument.</title>
		<link>http://blogs.umass.edu/etosch/?p=29</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=29</guid>
		<description></description>
		<content:encoded><![CDATA[One of the issues we would like to be able to address eventuallyin SurveyMan is the the problem of automatically determining the sample size for a survey. Last year, <a href="http://emeryblogger.com/">Emery</a> and I discussed the idea of running a survey until we converged upon a distribution. We eventually moved away from this problem, since it was not the primary concern SurveyMan was designed to address. Our focus has been on bugs in the instrument. The idea was that if there aren't enough sample points to make inferences, our tool will simply report that. We found that when it came to the kinds of surveys we were running, we did not have sufficient data points to return a meaningful confidence interval for the point estimate. Since we wanted to show that the tool was an appropriate substitute for the domain-specific tools we'd encountered (e.g. <a href="http://spellout.net/ibexfarm/">Ibex Farm</a>), we did not focus our attention on power analysis.

Now I'm starting to think about how some of Emery's static analysis ideas and how they actually relate back to this problem of determining the correct sample $$n$ and the related problem of power analysis.

Consider entropy in the survey. We had discussed how one might go about finding bounds on the size of the space of answers and its relationship with the sample obtained during a pilot study. If the survey consists of 5 yes/no questions, there are 32 unique complete survey answers*



&nbsp;

* If you allow breakoff and have full randomization, then for number of questions in your survey $$n$$ and number of answer options $$m$$, you actually have $$ \sum_{i=1}^n m^i \cdot i! $$]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>29</wp:post_id>
		<wp:post_date>2014-09-29 00:35:24</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Experiment Report II : This time, with Entropy</title>
		<link>http://blogs.umass.edu/etosch/?p=1048</link>
		<pubDate>Tue, 25 Mar 2014 19:08:55 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1048</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1048</wp:post_id>
		<wp:post_date>2014-03-25 15:08:55</wp:post_date>
		<wp:post_date_gmt>2014-03-25 19:08:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Rethinking Survey Entropy</title>
		<link>http://blogs.umass.edu/etosch/?p=1162</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1162</guid>
		<description></description>
		<content:encoded><![CDATA[We currently use entropy to classify bots in our surveys. I'll eventually get to a post on the various methods we tried, their ROC curves, and their mathematical justifications, but for now let's just say that using entropy and an entropy-like calculation had the most consistent error rate across runs. 

We report maximum entropy as one of the static analyses on surveys. I would like to run a simulation that randomly generates surveys of varying structure and investigate further the relationship between entropy, survey structure, bot detection, and other bug detection. 

One of the features that makes analysis of entropy of surveys nontrivial is that we permit breakoff. At what point do we rule legitimate respondents who break off early equivalent to bots who answer randomly? What percentage of bad actors can we tolerate before we cannot tell the difference between friend and foe? Our first impulse was to try to normalize the entropy calculation: since the number of bits needed to represent a survey is a function of its length (as well as the distribution of answers per question), we run into a problem when we're looking for respondents who require more bits to represent the survey. Why more bits you say? Because in the case where everyone answers the entire survey, we are interested in those whose responses look the most like a random respondent. Unfortunately, in the case where many respondents break off, those who stick it out are flagged as bots. We saw this happen with the wage survey : respondents who answered the most questions (none which were the complete set of questions ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1162</wp:post_id>
		<wp:post_date>2014-05-01 22:02:19</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>What is the relationship between entropy and survey bugs? </title>
		<link>http://blogs.umass.edu/etosch/?p=1250</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1250</guid>
		<description></description>
		<content:encoded><![CDATA[We've made the argument that more entropy in a survey translates to more easily identifying bugs in a survey. But what exactly is the relationship between entropy and debugging?

Before we collect any data for a survey, we run a static analyzer on it. One of the pieces of data that our static analyzer returns is the maximum number of bits necessary to describe the survey's distribution of responses. How should users interpret this number? For what values is ento]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1250</wp:post_id>
		<wp:post_date>2014-09-29 00:33:09</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>A quick rundown on survey tools...</title>
		<link>http://blogs.umass.edu/etosch/?p=1278</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1278</guid>
		<description></description>
		<content:encoded><![CDATA[The related work for SurveyMan is a cross between survey and experimental languages, online survey tools, crowdsourcing platforms, and domain-specific combinations of the former. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1278</wp:post_id>
		<wp:post_date>2014-05-17 09:10:40</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>On the role of Control.</title>
		<link>http://blogs.umass.edu/etosch/?p=1282</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1282</guid>
		<description></description>
		<content:encoded><![CDATA[Last weekend, I ran into <a href="http://people.csail.mit.edu/pkrafft/">Peter Krafft</a> at <a href="http://people.cs.umass.edu/~boucher/">Tommy Boucher</a>'s wedding. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1282</wp:post_id>
		<wp:post_date>2014-05-17 09:25:05</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Extant methods for quality control in crowdsourcing systems</title>
		<link>http://blogs.umass.edu/etosch/?p=1366</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1366</guid>
		<description></description>
		<content:encoded><![CDATA[Last summer, when I was working on the quality control mechanisms for SurveyMan, <a href="http://www.ic.unicamp.br/~tachard/">Alex</a> sent me a paper to appear at NIPS 2013 on <a href="http://www.ics.uci.edu/~qliu1/PDF/main_nips2013.pdf">determining the optimal number of control questions</a> for a crowdsourced-task. 

Their setup is a bit different from ours, but the I wanted to see if we could use any of their techniques for our purposes.

The paper is interested in three variables : $$k$$, the number of control questions; $$\mu_\mathcal{T}^*$$, the true values of the ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1366</wp:post_id>
		<wp:post_date>2014-05-19 10:36:36</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Phonology, Part II</title>
		<link>http://blogs.umass.edu/etosch/?p=1374</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1374</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1374</wp:post_id>
		<wp:post_date>2014-05-31 11:32:29</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>FoldIt for the Social Sciences?</title>
		<link>http://blogs.umass.edu/etosch/?p=1774</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1774</guid>
		<description></description>
		<content:encoded><![CDATA[Searching through the entire space of hypotheses is hard, but if we can visualize the space, we can let researchers figure out the balance between exploration and exploitation.

Difference between this and AutoTask -- AutoTask only shows humans their small task. It doesn't actually leverage the part of search and computation humans are "good at" -- reasoning about complex relationships. A lot of computation goes into evaluating the potential payoff for nodes in a search tree and deciding whether or not to expand it. If we could use humans to make this decision, we could perform more efficient evaluations.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1774</wp:post_id>
		<wp:post_date>2014-06-29 13:06:53</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Sequential Tests</title>
		<link>http://blogs.umass.edu/etosch/?p=1822</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1822</guid>
		<description></description>
		<content:encoded><![CDATA[The gap between statistics and computer science is much larger than it has to be. I suspect the cause of this gap is strongly cultural. From what I understand, there was some resistance to applied statistics at the top statistics departments in the 90s. The focus on computation and algorithms in CS is distinctly applied -- an emphasis on *how* to solve a problem has a distinctly applied smell to it. Of course, today statisticians use an assortment of more or less well designed programming languages to perform analysis. I remember using SPSS (or was it SAS?) in my freshman-year stats class. I don't know if things have changed since then, but from what I remember, most of the effort was in data entry and interpreting the results. Actually running a program (whether a regression or ANOVA) was just a line of code. Of course, these things are just a line of code in R, but at least with R, you can interact with the data. Granted, it's possible I just didn't know how to interact with/manipulate data in SPSS then, so it's possible I'm maligning these tools unfairly. 

In any case, most of the programming languages for statistics I'm aware of are pretty weak on the CS side of things. These programming languages might strive to look like math, but issues with binding variables (Python, though I'm told this problem is fixed in 3.0) in what ought to be closures (I'm still not clear on the closure story in R) and a complete absence of types (save Julia. love Julia) makes the flow from CS to stats seem pretty sad. 

Conversely, it sometimes feels like the simplicity of Bayes' Rule has made all these newfangled data scientists throw design to the wind. Somewhere in the history of CS and stats there was a divergence; for example, I associate MDPs with CS more than stats, even though the problem is clearly a statistical one. 

Maybe it's the desire for stats to be more like probability theory? Statistics is clearly empirical in nature. It's well-suited to have close ties with CS. There certainly are close ties, but I've found it curious that those ties aren't closer. The story, the set of techniques, and the theory behind statistical methods employed by CS -- these things aren't unified in any way. Maybe that's why data science has really caught hold?

Enough of ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1822</wp:post_id>
		<wp:post_date>2014-07-15 09:07:55</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>What does it mean for a survey to be &quot;correct&quot;? Part II.</title>
		<link>http://blogs.umass.edu/etosch/?p=1824</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1824</guid>
		<description></description>
		<content:encoded><![CDATA[SurveyMan was designed to be used in an iterative process of identifying survey bugs and refining survey "programs." Since there is considerable variability in both survey instruments and survey populations, I don't think it's appropriate to provide a cookbook for debugging surveys. Instead, we should approach the problem from (at least) two different perspectives: the formal approach, which I began in Part I, and a more empirical approach, which looks more like data analysis. 

The formal approach tackles survey correctness from a programming languages an more broadly computer science perspective. It focuses on properties of the survey instrument. This is an important perspective to consider because the survey instrument is what researchers have the most control over. 

The more empirical approach I will describe in this post discusses properties of the data and how the survey design should change in response to the data. 

<h3>Entropy</h3>
One of the questions we'd like to be able to answer is: how does your]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1824</wp:post_id>
		<wp:post_date>2014-07-16 17:29:24</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Correlation for Fun and Profit.</title>
		<link>http://blogs.umass.edu/etosch/?p=2098</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2098</guid>
		<description></description>
		<content:encoded><![CDATA[
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2098</wp:post_id>
		<wp:post_date>2014-08-01 09:25:32</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The Crotchety Schemer</title>
		<link>http://blogs.umass.edu/etosch/?p=2294</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2294</guid>
		<description></description>
		<content:encoded><![CDATA[- languages like python are making progrmming mre democratic
- however, many poeple still view the internals of these things as magic

- step by step instructions on how i solved a particular problem in python

it's not always clear what makes a solution clever

some quip about peter norvig calling python a lisp]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2294</wp:post_id>
		<wp:post_date>2014-11-14 13:23:16</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Jeeves</title>
		<link>http://blogs.umass.edu/etosch/?p=2476</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2476</guid>
		<description></description>
		<content:encoded><![CDATA[Jean Yang visited UMass during my last week at Google, so I didn't have the chance to see her talk. However, I did just finish reading her paper on Jeeves. I had skimmed it previously, but this time I took a closer look. I thought there might be some interesting applications for SurveyMan that could use her approach to protecting sensitive data.

Jeeves is a DSL for describing how sensitive data may be viewed. This DSL separates core implementation of an application from the privacy logic. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2476</wp:post_id>
		<wp:post_date>2014-12-29 19:50:30</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Matt Might&#039;s Reading List, Part I</title>
		<link>http://blogs.umass.edu/etosch/?p=2478</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2478</guid>
		<description></description>
		<content:encoded><![CDATA[http://matt.might.net/articles/books-papers-materials-for-graduate-students/]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2478</wp:post_id>
		<wp:post_date>2015-01-01 16:01:20</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Multiple comparisons</title>
		<link>http://blogs.umass.edu/etosch/?p=2600</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2600</guid>
		<description></description>
		<content:encoded><![CDATA[One of the 
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2600</wp:post_id>
		<wp:post_date>2015-01-27 09:10:59</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Brainstorming ways to speed up dynamic analyses</title>
		<link>http://blogs.umass.edu/etosch/?p=2603</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2603</guid>
		<description></description>
		<content:encoded><![CDATA[No matter what analysis we use when ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2603</wp:post_id>
		<wp:post_date>2015-02-09 12:16:35</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Survey Inference</title>
		<link>http://blogs.umass.edu/etosch/?p=2683</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2683</guid>
		<description></description>
		<content:encoded><![CDATA[Thus far, we have not tackled survey inference itself in SurveyMan. The techniques we've been using to eliminate bad actors are fairly coarse-grained and aggressive. Cibele and I are currently analyzing ways to customize ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2683</wp:post_id>
		<wp:post_date>2015-05-03 14:01:07</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Simple Classification</title>
		<link>http://blogs.umass.edu/etosch/?p=2685</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2685</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2685</wp:post_id>
		<wp:post_date>2015-05-03 14:32:40</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Differential Privacy and PL in the face of that crazy falsified data story</title>
		<link>http://blogs.umass.edu/etosch/?p=2805</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2805</guid>
		<description></description>
		<content:encoded><![CDATA[By now everyone even a little bit related to social science has heard of <a href="http://www.huffingtonpost.com/2015/05/28/science-retracts-gay-marriage-study_n_7463098.html">that fraudulent study</a> about changing voters opinions through the gift of gab. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2805</wp:post_id>
		<wp:post_date>2015-06-02 11:01:23</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>SurveyMan front-end development in Python begins!</title>
		<link>http://blogs.umass.edu/etosch/2014/01/22/surveyman-front-end-development-in-python-begins/</link>
		<pubDate>Wed, 22 Jan 2014 14:41:22 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=9</guid>
		<description></description>
		<content:encoded><![CDATA[Today I met with Molly to discuss a new project she's working on -- she'll be designing a new front-end for SurveyMan. It will be a pip-installable Python library, available in May.

She'll be <a href="http://blogs.umass.edu/mmcma0/">blogging</a> about it over the course of the semester.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>9</wp:post_id>
		<wp:post_date>2014-01-22 14:41:22</wp:post_date>
		<wp:post_date_gmt>2014-01-22 14:41:22</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>surveyman-front-end-development-in-python-begins</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>SurveyMan&#039;s debut</title>
		<link>http://blogs.umass.edu/etosch/2014/01/28/surveymans-debut/</link>
		<pubDate>Tue, 28 Jan 2014 20:00:31 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=15</guid>
		<description></description>
		<content:encoded><![CDATA[Dear Internet Diary,

This past weekend, we presented the SurveyMan work for the first time, at the <a href="http://popl-obt-2014.cs.brown.edu/">Off the Beaten Track</a> workshop at POPL. I first want to say that <a href="http://popl-obt-2014.cs.brown.edu/">PLASMA</a> seriously represented. We had talks in each of the sessions. Though I didn't have the chance to see <a href="http://people.cs.umass.edu/~charlie/">Charlie</a>'s talk on Causal Profiling, <a href="http://people.cs.umass.edu/~dbarowy/">Dan</a> said it definitely engendered discussion and that people in the audience were "nodding vigorously" in response to the work. <a href="http://people.cs.umass.edu/~gochev/">Dimitar</a> presented Data Debugging, which people clearly found provocative.

I was surprised by the audience's response to my talk; I know Emery had said that people whom he talked to were excited about this space, but sometime that's hard to believe when you're a grad student chugging away at the implementation and theory behind the work. It was invigorating to be able to describe what we've done so far and hear enthusiastic feedback. In all my practice talks, I had focused on the language itself, but for OBT, at the behest of my colleagues, I took the debugging angle instead. Most of the people in the audience had used surveys for their research and were quite familiar with these problems. While language designers have tried to tackle surveys before, they frequently come from the perspective of embedding it in a language *they* already use. The approach we take leverages tools that our target audience uses. We limit the expressivity of the language and make statistical guarantees, which is what our users care about the most.

I had a few really interesting questions about system features. Someone made the point that bias cannot be entirely removed through redundancy -- that we can't know if we've found enough ways of expressing a question to control for the underlying different interpretations. In response, I suggested that we could think about using approaches from cross-language models to determine whether we have categorically the same questions. The idea is that if a set of questions produces the same distribution of responses, it is sufficiently similar. Of course, this approach neglects the non-local effects of question wording. Whether or not this can be controlled through question order randomization is something I'll have to think about more.

As a followup question, I was also asked if we could reverse-engineer the distributions we get from the variants to identify different concepts. This was definitely not something I had considered before. I wasn't sure we would, in practice, have sufficient variants and responses to produce meaningful results, but it's something to consider as future work.

A lot of the other questions I had were about features of the system that I did not highlight. For example, I did not go into any detail about the language and its control flow. I was also asked if we were considering adding clustering and other automated domain-independent analyses, which I am working on right now. Quite a few of the concerns are addressed by our preference for breakoff over item-nonresponse. There was also an interesting ethics question about using our system to manipulate results. Of course, SurveyMan requires active participation from the survey designer; the idea is not to prevent the end-user from adding bias, but to illuminate its presence.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>15</wp:post_id>
		<wp:post_date>2014-01-28 20:00:31</wp:post_date>
		<wp:post_date_gmt>2014-01-28 20:00:31</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>surveymans-debut</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="obt2014"><![CDATA[#OBT2014]]></category>
		<category domain="post_tag" nicename="popl2014"><![CDATA[#POPL2014]]></category>
		<category domain="category" nicename="conferences"><![CDATA[Conferences]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>2</wp:comment_id>
			<wp:comment_author><![CDATA[Presley]]></wp:comment_author>
			<wp:comment_author_email>ppizzo@linguist.umass.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>71.192.24.73</wp:comment_author_IP>
			<wp:comment_date>2014-03-06 18:01:47</wp:comment_date>
			<wp:comment_date_gmt>2014-03-06 18:01:47</wp:comment_date_gmt>
			<wp:comment_content><![CDATA["As a followup question, I was also asked if we could reverse-engineer the distributions we get from the variants to identify different concepts. This was definitely not something I had considered before. I wasn't sure we would, in practice, have sufficient variants and responses to produce meaningful results, but it's something to consider as future work."

Maybe we should think about this as a possible data science application of SurveyMan. It's still a way to get training labels, but there's a distinction between tapping into respondents' conscious knowledge and tapping into the way they're biased by question wording, question order, etc. SurveyMan is good for the former because it controls for bias better than other survey systems, and it's good for the latter because the ability to test those things is built right in.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>6</wp:comment_id>
			<wp:comment_author><![CDATA[Emma Tosch]]></wp:comment_author>
			<wp:comment_author_email>etosch@cns.umass.edu</wp:comment_author_email>
			<wp:comment_author_url>https://cs.umass.edu/~etosch</wp:comment_author_url>
			<wp:comment_author_IP>50.187.4.237</wp:comment_author_IP>
			<wp:comment_date>2014-03-14 16:40:26</wp:comment_date>
			<wp:comment_date_gmt>2014-03-14 16:40:26</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yeah, I think this sort of thing is ripe for knowledge discovery. I would have to look more into the literature to say anything worthwhile about it. I know Emery's been discussing some of this work with David Jensen, who does work in causality, and he's quite interested in what we're doing.

The above suggestion also dovetails with what Dan plans to work on over the summer. A key feature that the MSR people want is the ability to search through space of hypotheses, generate new hypotheses automatically, and test them automatically. Dan was taking his AutoMan work in this direction, trying to do open-ended search. It's definitely a hard problem, and one where you easily get stuck in a basin of doom. Generating hypotheses via clustering in the survey (use the extra columns as features), or learning latent variables across questions that are allegedly semantically equivalent seems like it would be a huge win for knowledge discovery, and data science (by extension).]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>20775</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1394815226.5438039302825927734375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Surveys : A History</title>
		<link>http://blogs.umass.edu/etosch/2014/03/04/surveys-a-history/</link>
		<pubDate>Tue, 04 Mar 2014 16:23:27 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=56</guid>
		<description></description>
		<content:encoded><![CDATA[<h2>What *is* a survey?</h2>
Everyone has seen a survey -- we've all had the customer satisfaction pop-up on a webpage, or have been asked by a college student PIRG worker to answer some questions about the environment. We tend to think of surveys as a series of questions designed to gauge opinion on a topic. Sometimes the answers are drawn from a pre-specific list of options (e.g. the so-called <a href="http://en.wikipedia.org/wiki/Likert_scale">Likert Scales</a>); sometimes the response is free-form. 

What distinguishes surveys from other, similar "<a href="http://www.census.gov/srd/papers/pdf/rsm2006-13.pdf">instruments</a>" is that surveys (a) typically return a distribution of valid responses and (b) surveys are observational. Or rather, surveys are <em>supposed</em> to be observational. That is, surveys are meant to <a href="http://www.uvm.edu/~dguber/POLS234/articles/zaller_feldman.pdf">reveal preferences</a> or underlying assumptions, behaviors, etc. and not sway the respondent to answer in one way or another. There are some similar-looking instruments that are not meant to be observational. Some of these instruments fall under the umbrella of what's called an "experiment" in the statistics literature.

<h2>Why surveys?</h2>

Perhaps in the future we won't have a need for surveys anymore -- <a href="http://andrewgelman.com/2014/03/01/moving-era-private-data-public-analyses-one-public-data-private-analyses-just-learned-cautious-data-missing-may-cautiou/">all of our data will be floating around on the web, free to anyone who wants to analyze it</a>. If, after all, surveys are really about observational studies, we should be able to just apply some clustering, learn a model, do some k-fold validation, etc. 

There are many problems when attempting to just use data available in the wild. First of all, though we may be in the era of "big data," there are plenty of cases where the specific data you want is sparse. A worse situation is when the sparsity can be characterized by a <a href="http://en.wikipedia.org/wiki/Zipf's_law">Zipfian distribution</a> - depending upon how you set up your study and what your prior information is, it's possible that you will never sample from the tail of this distribution and may never know that it is sparse. This leads us to a second problem with simply mining data : we cannot control the conditions under which the data is obtained. When conducting a survey, researchers typically use <a href="http://en.wikipedia.org/wiki/Survey_sampling#Probability_sampling">probability sampling</a> (the popularity of <a href="http://en.wikipedia.org/wiki/Sampling_(statistics)#Accidental_sampling">convenience sampling</a> for web surveys will be discussed in a later post). This allows them to adequately estimate the denominator and estimate error due to people opting out of the survey (so-called "unit nonresponse").

Finally, conducting a survey explicitly, rather than mining data gives the researcher a more complete view of the context of the survey. As we will discuss later, understanding context is critical and can lead to unpredictable responses.

<h2>A brief history of survey modes</h2>

While simple surveys such as a census have been around for <a href="http://en.wikipedia.org/wiki/Census#Egypt">thousands of years</a>, the customer satisfaction or politic survey of today is a more recent development. Market research and political forecasting are products of capitalism and require access to resources to conduct and make use of surveys. Survey research is intrinsically tied to the technologies used to conduct that research and the statistical methodologies that are available and understood gat the time the survey is conducted. Before mail service, a survey would have to have been conducted in person. Although random sampling is a very old idea, it was not until <a href="http://en.wikipedia.org/wiki/Laplace#Inductive_probability">Laplace</a> that tight bounds were calculated on the number of samples needed to estimate a parameter of a population.

Centralized mail service helped lower the cost of conducting surveys. The response time for surveys was lowered with widespread adoption of telephones. Mail and telephone surveys dominated survey modes in the latter half of the 20th century. Since landline telephones are associated with an address, these now-traditional survey modes relied on accurate demographic information. The introduction of the World Wide Web and increasingly wide-spread use of cellular phones prompted survey designers to reconsider traditional instruments in favor of ones better suited to growing technologies. 

A <a href="http://schonlau.net/publication/02fieldmethods.pdf">2002 paper from the RAND corporation</a> describes growing interest among researchers about using the Web to conduct surveys. The paper addresses the assertion that internet surveys have higher response rates than traditional mail or telephone surveys. They found this to not be the case, except for technologically savvy populations (e.g. employees of Bell Labs). However, they noted that the web is only going to become more pervasive and they recommended that survey designers keep the web in mind. 

The RAND paper describes web surveys not as web forms, but simply as online-distributed paper surveys. The surveys were sent over email, in a model that exactly mirrors mail surveys. They noted that spam could be an issue over time and specifically stated that it was possible that the populations with higher response rates to emailed surveys were those who may have had a lower junk-to-relevant mail ratio for email than for snail mail. 

The view of web surveys from twelve years ago predated "web 2.0". It also predated the widespread use of cellular phones. Six years ago Pew Research published <a href="http://www.pewresearch.org/2007/06/19/how-serious-is-pollings-cellonly-problem/">an article</a> on the growing proportion of cell-phone-only households. They found that this population still only comprised a small proportion of the total US population, and so for polls that targeted the entire US population, unit nonresponse from cell phone users could be explained by typical error estimates. However, cell phone users were found to have a distinct population profile from the total US population. Therefore, any stratified sampling needed to take cell phone users more seriously.

<h2>Why Web Surveys?</h2>

As technology changes, the mode used to collect survey information changes. Clearly the rising use of smart phones makes web surveys increasing attractive for researchers.  

On top of the obvious appeal of being able to reach more people, web surveys afford researchers unique advantages that other modes do not allow, or are prohibitively expensive to implement. Web surveys do not require people to administer them (while many organizations use automated calling services for phone surveys, the is still an associated cost for the service, as well as <a href="http://lifehacker.com/why-you-should-hang-up-immediately-when-you-get-a-roboc-1269375265">growing discontent for robocalls</a> (okay, that still supports argument one). 

Web surveys allow for rapid design modification, cheap pilot studies, and (what we believe to be most important) the ability to control for known problems in survey design. What problems could there be, other than not being able to reach people? Consider the dominant view of survey design:

<blockquote>
The goal is to present a uniform stimulus to respondents so that their responses are comparable. Research showing that small changes in question wording or order can substantially affect responses has reinforced the assumption that questions must be asked exactly as worded, and in the same order, to produce comparable data. (<a href="http://www.census.gov/srd/papers/pdf/rsm2006-13.pdf">Martin 2006 </a>)
</blockquote>

We believe that this view -- this static view -- of survey design leads to overly complicated models and cumbersome statistical analyses that arise solely from only being able to perform post-hoc data analysis. Our view is that, since there are so many variables that may affect the outcome of a particular survey response, we should not try to control for everything, since <em>controlling for everything is impossible</em>. Instead, we randomize aspects of the instrument over a population, promote a "debug phase" akin to pilot studies, and encourage easy replication experiments. If we can reduce known biases in survey design to noise, we can perform more robust analyses. Now what do more robust analyses give us? Better science! Who wouldn't want that? ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>56</wp:post_id>
		<wp:post_date>2014-03-04 16:23:27</wp:post_date>
		<wp:post_date_gmt>2014-03-04 16:23:27</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>surveys-a-history</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Observational Studies, Surveys, Quasi-Experiments, and Experiments</title>
		<link>http://blogs.umass.edu/etosch/2014/03/11/observational-studies-surveys-quasi-experiments-and-experiments/</link>
		<pubDate>Tue, 11 Mar 2014 15:57:28 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=106</guid>
		<description></description>
		<content:encoded><![CDATA[Across the sciences, researchers use a spectrum of tools or "instruments'' to collect information and then make inferences about human preferences and behavior. These tools vary in the degree of control the researcher traditionally has had over the conditions of data collection. Surveys are an instance of such an instrument. Though widely used across social science, business, and even in computer science as user studies, surveys are known to have bugs. Although there are many tools for designing web surveys, few address known problems in survey design.

They have also traditionally varied in their media and the conditions under which they are administered. Some tools we consider are: 

<h2>Observational studies</h2> 
Allowing no control over how data are gathered, observational studies are analogous to data mining -- if the information is not readily available, the researcher simply cannot get it. 

<h2>Surveys</h2> 
The next best approach is to run a survey. Surveys have similar intent as observational studies, in that they are not meant to have an impact on the subject(s) being studied. However, surveys are known to have flaws that bias results. These flaws are typically related to the language of individual survey questions and the structure and control flow of the survey instrument itself.

<h2>True Experiments</h2> 
If a research is in the position of having a high degree of control over all variables of the experiment, they can randomly assign treatments and perform what is known as a "true experiment". These experiments require little modeling, since the researcher can simply using hypothesis testing to distinguish between effect and noise. 

<h2>Quasi-Experiments</h2> 
Quasi-experiments are similar to true experiments, except they relax some of the requirements of true experiments and are typically concerned with understanding causality. 

<p>In the past, there has been little fluidity between these four approaches to data collection, since the media used to implement each was dramatically different. However, with the proliferation of data on the web and the ease of issuing questionnaires on such platforms as facebook, SurveyMonkey, and Mechanical Turk, the implementation of these studies of human preferences and behavior have come to share many core features.</p>

Despite similarities between these tools, quality control techniques for experiments have been largely absent from the design and deployment of surveys. There has been an outpouring of web tools and services for designing and hosting web surveys, aimed at non-programmers. While there are some tools and services available for experiments, they tend to be domain-specific and targeted to niche populations of researchers. The robust statistical approaches used in experimental design should inform survey design, and the general, programmatic approaches to web survey design should be available for experimental design. 
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>106</wp:post_id>
		<wp:post_date>2014-03-11 15:57:28</wp:post_date>
		<wp:post_date_gmt>2014-03-11 15:57:28</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>observational-studies-surveys-quasi-experiments-and-experiments</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="experiman"><![CDATA[experiman]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>4</wp:comment_id>
			<wp:comment_author><![CDATA[Presley]]></wp:comment_author>
			<wp:comment_author_email>ppizzo@linguist.umass.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>71.192.98.16</wp:comment_author_IP>
			<wp:comment_date>2014-03-12 21:26:09</wp:comment_date>
			<wp:comment_date_gmt>2014-03-12 21:26:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This is good, I just got confused on the observational part. I guess by observational you mean what in linguistics we call a corpus study - you just search and analyze information that's already "out there", which may involve gathering what's out there in some systematic way first, and possibly annotating it. But there's no control over the frequencies of the situations, which leads to sparsity in some areas. The upsides, though, are that things are more naturalistic, you lack task effects (there's that phrase you wanted to look up), and you can get way more data. With sophisticated statistics (I think data mining is a synonym of that?), you can account for the uncontrolled distribution at least to a point. And when you control your distribution, you run the risk of introducing sampling bias. I don't know if you want to go into observational studies very much since that's not what SurveyMan is about, but some of these ideas might help. Also, I don't know what a quasi-experiment is.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[true]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1394659569.7398769855499267578125;s:7:"message";s:35:"Akismet caught this comment as spam";s:5:"event";s:10:"check-spam";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1394812297.8880069255828857421875;s:7:"message";s:40:"etosch reported this comment as not spam";s:5:"event";s:10:"report-ham";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_user_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_user</wp:meta_key>
				<wp:meta_value><![CDATA[etosch]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1394812304.3264939785003662109375;s:7:"message";s:45:"etosch changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>8</wp:comment_id>
			<wp:comment_author><![CDATA[Emma Tosch]]></wp:comment_author>
			<wp:comment_author_email>etosch@cns.umass.edu</wp:comment_author_email>
			<wp:comment_author_url>https://cs.umass.edu/~etosch</wp:comment_author_url>
			<wp:comment_author_IP>50.187.4.237</wp:comment_author_IP>
			<wp:comment_date>2014-03-14 16:48:59</wp:comment_date>
			<wp:comment_date_gmt>2014-03-14 16:48:59</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I think you're right about all the synonyms for observational studies -- I think unifying techniques and terminology should be a goal of data science, since many disciplines are doing the same thing, but in slightly different ways. 

Regarding what SurveyMan has to offer, it really isn't focused on observational studies/data mining. There was a picture I drew on the white board for Molly that illustrated how previously experiments and quasi-experiments were grouped together because they were conducted in highly controlled environments (i.e. a lab), whereas surveys and observational studies were conducted in the wild, where you have very little control over the environment. Surveys have traditionally aspired to be like observational studies, but inherently suffer from the "probe effect". We're shifting surveys over to the (quasi-)?experiments category, since we're exercising a higher degree of control than we had before. What's interesting to me is that this is quite clearly a product of being able to deploy surveys on the web. While this new technology has made surveys more robust, using the same platforms has actually degraded the integrity of experiments -- where before they were conducted in a lab, now you have no idea what the conditions are under which the person is taking them. The assumption here is that you'll drown out the noise caused by these uncontrolled environments by gathering significantly more data from a significantly broader population than before.

re : quasi-experiments. They're used either when you cannot randomly assign a variable (e.g. it's hard to reassign sex), or when you hold other variables constant on purpose in order to determine causality. This is something Emery's quite keen on right now.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>20775</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1394815739.353477001190185546875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>On Keeping the Survey a DAG</title>
		<link>http://blogs.umass.edu/etosch/2014/03/11/on-keeping-the-survey-a-dag/</link>
		<pubDate>Tue, 11 Mar 2014 19:31:48 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=114</guid>
		<description></description>
		<content:encoded><![CDATA[A topic that came up during my SurveyMan lab talk in October was our lack of support for looping questions. <a href="http://people.cs.umass.edu/~brun/">Yuriy</a> had raised the objection that there will be cases where we will want to repeat a question, such as providing information on employment. We argued that, since we were emulating paper surveys (at the time), the user could provide an upper bound on the number of entries and ask the user whether they wanted to add another entry for a category. A concern I had was that, since we're interested in role of survey length in the quality of responses, and since we allow breakoff, when we have a loop in a question, it becomes much more difficult to tell whether the question is a problem or if the length of the survey is a problem. Where previously we treated each question as a random variable, we would now need to model a repeating question as an unknown sum of random variables.[caption id="attachment_116" align="aligncenter" width="300"]<a href="http://blogs.umass.edu/etosch/files/2014/03/survey_loop-e1394563716747.png"><img src="http://blogs.umass.edu/etosch/files/2014/03/survey_loop-e1394563716747-300x210.png" alt="The probability model of a survey with a loop differs from the model of a survey without one." width="300" height="210" class="size-medium wp-image-116" /></a> The probability model of a survey with a loop differs from the model of a survey without one. Note that while both random variables corresponding to the responses to question Q2 may be modeled by the same distribution, they will have different parameters. [/caption]

This issue came up again during the OBT talk. The expanded version of <a href="http://topsl.sourceforge.net/pubs/sw2004.pdf">Topsl</a> that appeared in the <a href="http://redex.racket-lang.org/">PLT Redex</a> book described a semantics for a survey that was allowed to have these kinds of repeated questions.

We do not think it is appropriate to model such questions as loops. Loops are fundamentally necessary to express computable functions.  Since the kinds of questions these loops are modeling are more accurately described as having finite, unknown length, we do not want to encode the ability to loop forever. 

Aside from this semantic difference, we see another problem with the potentially perpetual loop. Consider the use-case for such a question: in the case of the lab talk, it was Yuriy's suggestion that we allow people to enter an employment history of unknown length. In the case of Topsl, it was self-reporting relationship history. If a respondent's employment or relationship history is very long, they may be tempted to under-report the number of instances. This might be curtailed if the respondent is required to first answer* a question that asks for the number of jobs or relationships they** have had. Then responses in the loop could be correlated with the previous question, or the length of the loop could be bounded. In our setting, where we do not respondents to skip questions, the former would need to be implemented if we were to allow loops at all. 

Alternatively, instead of presenting each response to what is semantically the same question as if it were a separate question, we could first ask the question for the number of jobs or relationships, and then ask a followup question on a page that takes the response to the previous question, and displays that number of text boxes on the page. We would still bound the total number of responses, but instead of presenting each question separately, we would present them as a single question.

In the analysis of a survey we ran, we found statistically significant breakoff at the freetext question. We'd like to test whether freetext questions in general are correlated with high breakoff. If this is the case, we believe it provides further evidence that the approach to "loop questions" is better implemented using our approach.

* I just wanted to note that I love <a href="http://www.quickanddirtytips.com/education/grammar/split-infinitives">splitting infinitives</a>. 
** While I'm at it, I also support <a href="http://en.wikipedia.org/wiki/Gender-specific_and_gender-neutral_pronouns#Singular_they">gender-neutral pronouns</a>. Political grammar FTW!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>114</wp:post_id>
		<wp:post_date>2014-03-11 19:31:48</wp:post_date>
		<wp:post_date_gmt>2014-03-11 19:31:48</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-keeping-the-survey-a-dag</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Adversaries </title>
		<link>http://blogs.umass.edu/etosch/2014/03/13/adversaries/</link>
		<pubDate>Thu, 13 Mar 2014 22:07:04 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=134</guid>
		<description></description>
		<content:encoded><![CDATA[Bad actors are a key threat to validity that cannot be controlled directly through better survey design. That is, unlike the case of bias in wording or order, we cannot eliminate bugs through the survey design. What we can do is use the design to make it easier to identify these adversaries. 

<h3> Bots </h3>

Bots are computer programs that fill out surveys automatically. We assume that bots have a policy for choosing answers that is either completely independent of the question, or is based upon some positional preference. 
<p><b>No positional preference</b> A bot that chooses responses randomly is an example of one that answers questions independently from their content.</p>
</p><b>Positional preference</b> A bot that always chooses the first question or always chooses the last question, or alternates positions on the basis of the number of available choices: for example, "Christmas tree-ing" a multiple choice survey. </p>

<h3> Lazy Respondents </h3>

We define a lazy respondent as a human who is behaving in a bot-like way. In the literature these individuals are called <em>spammers</em> and according to a study from 2010, <a href="http://lorrie.cranor.org/pubs/note1552-downs.pdf">almost 40% of the population sampled</a> failed a screening task that only required basic reading comprehension. There are two key differences between human adversaries and software adversaries : (1) we hypothesize that individual human adversaries are less likely to choose responses randomly and (2) that when human adversaries have a positional preference, they are more likely make small variations in their otherwise consistent responses. Regarding (1), while there is <a href="http://cocosci.berkeley.edu/tom/papers/hard.pdf">no</a> <a href="http://etd.lsu.edu/docs/available/etd-07092004-113059/unrestricted/Armstrong_thesis.pdf">end</a> <a href="http://emlab.berkeley.edu/users/rabin/GHFTA_RESf.pdf">to</a> <a href="http://psych.cornell.edu/sites/default/files/Gilo.Vallone.Tversky.pdf">the</a> <a href="http://journal.sjdm.org/10/91117/jdm91117.html">number</a> <a href="http://psycnet.apa.org/journals/bul/76/2/105/">of</a> <a href="http://www.jstor.org/discover/10.2307/187083?uid=2&amp;uid=4&amp;sid=21103742630243">studies</a> and <a href="http://www.nytimes.com/2007/02/13/health/13iht-faces.html?pagewanted=all">amount</a> <a href="http://www.scientificamerican.com/article/patternicity-finding-meaningful-patterns/">of</a> <a href="http://online.wsj.com/news/articles/SB10001424052702304071004579409071015745370">press</a> devoted to <a href="http://psycnet.apa.org/journals/xge/62/4/385/">humans'</a> <a href="http://psycnet.apa.org/journals/xge/65/2/213/">inability</a> to identify</a> <a href="http://psycnet.apa.org/psycinfo/2009-02580-008">randomness</a>, there has been <a href="http://www.sciencedirect.com/science/article/pii/S0306987705001143">some</a> <a href="http://www.sciencedirect.com/science/article/pii/S030698770700480X">debate</a> over whether humans can actually generate sequences of random numbers. Regarding (2), while a bot can be programmed to make small variations in positional preference, we believe that humans will make much more strategic deviations in their positional preferences.

Both humans and bots may have policies that depend on the surface text of a question and/or its answer options. An example of a policy that chooses answers on the basis of surface text might be one that prefers the lexicographically first option, or one that always chooses surface strings equal to a value (e.g. contains "agree"). These adversaries are significantly stronger than the ones mentioned above. 

It's possible that some could see directly modeling a set of adversaries as overkill; after all, services such as AMT rely on reputable respondents for their systems to attract users (<a href="http://www.behind-the-enemy-lines.com/2010/07/mechanical-turk-low-wages-and-market.html">or not?</a>). While AMT has provided <a href="http://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkRequester/Concepts_QualificationsArticle.html">means for requesters to filter the population</a>, this system <a href="http://www.behind-the-enemy-lines.com/2010/10/be-top-mechanical-turk-worker-you-need.html">can easily be gamed</a>. <a href="https://www.ischool.utexas.edu/~cse2010/slides/alonso-ECIR2010-tutorial.pdf">This tutorial</a> from 2010 describes best practices for maximizing the quality of AMT jobs. Unfortunately, injecting "attention check" or gold standard questions is insufficient to ward off bad actors. Surveys are a prime target for bad actors because the basic assumption is that the person posting the survey doesn't know what the underlying distribution of answers ought to look like -- otherwise, why would they post a survey? <a href="https://github.com/sckingsley">Sara Kingsley</a> recently pointed us to an article from <a href="http://www.npr.org/blogs/alltechconsidered/2014/03/05/279669610/post-a-survey-on-mechanical-turk-and-watch-the-results-roll-in">All Things Considered</a>. Emery found the following comment:

<blockquote>
I've been doing Mechanical Turk jobs for about 4 months now.

I think the quality of the survey responses are correlated to the amount of money that the requester is paying. If the requester is paying very little, I will go as fast as I can through the survey making sure to pass their attention checks, so that I'm compensated fairly.

Conversely, if the requester wants to pay a fair wage, I will take my time and give a more thought out and non random response.
</blockquote>

A key problem that the above quote illustrates is that modeling individual users is fruitless. <a href="http://aclweb.org/anthology/N/N13/N13-1132.pdf">MACE</a> is a seemingly promising tool that uses <em>post hoc</em> generative models of annotator behavior to "learn whom to trust and when." This work notably does not cite prior work by <a href="http://www.ipeirotis.com/">Panos Ipeirotis</a> on <a href="http://www.ipeirotis.com/wp-content/uploads/2012/01/hcomp2010.pdf">modeling users with EM</a> and considered variability in workers' annotations. 

The problem with directly modeling individual users is that it cannot account for the myriad latent variables that lead a worker to behave badly. In order to do so, we would need to explicitly model every individual's utility function. This function would incorporate not only the expected payment for the task, but also the workers' subjective assessment of the ease of the task, the aesthetics of the task, or their judgement of the worthiness of the task. Not all workers behave consistently across tasks of the same type (e.g. annotations), let alone across tasks of differing types. Are workers who accept HITs that cause them dissatisfaction more likely to return the HIT, or to complete the minimum amount of work required to convince the requester to accept their work? ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>134</wp:post_id>
		<wp:post_date>2014-03-13 22:07:04</wp:post_date>
		<wp:post_date_gmt>2014-03-13 22:07:04</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>adversaries</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Experiment Report I : Breakoff, Bot Detection, and Correlation Analysis for Flat, Likert-scale Surveys</title>
		<link>http://blogs.umass.edu/etosch/2014/03/25/experiment-report-i-breakoff-bot-detection-and-correlation-analysis-for-flat-likert-scale-surveys/</link>
		<pubDate>Tue, 25 Mar 2014 07:41:55 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=266</guid>
		<description></description>
		<content:encoded><![CDATA[We ran a survey previously conducted by Brian Smith four times, to test our techniques against a gold-standard data set.

<table>
<tr><td>Date launched</td><td>Time of Day launched (EST)</td><td>Total<br /> Responses</td><td>Unique<br>Respondents</td><td>Breakoff?</td></tr></b>
<tr><td>Tue Sep 17 2013</td><td>Morning 9:53:35 AM EST</td><td>43</td><td>38</td><td>No</td></tr>
<tr><td>Fri Nov 15 2013</td><td>Night</td><td>90</td><td>67</td><td>Yes</td></tr>
<tr><td>Fri Jan 10 2013</td><td>Morning</td><td>148</td><td>129</td><td>Yes</td></tr>
<tr><td>Thu Mar 13 2014</td><td>Night 11:49:18 PM EST</td><td>157</td><td>157</td><td>Yes</td></tr>
</table>

This survey consists of three blocks. The first block asks demographic questions : age and whether the respondent is a native speaker of English. The second block contains 96 Likert-scale questions. The final block consists of one freetext question, asking the respondent to provide any feedback they might have about the survey. 
 
Each of the 96 questions in the second block asks the respondent to read aloud an English word suffixed with either of the pairs "-thon/-athon" or "licious/-alicious" and judge which sounds more like an English word.

<a href="http://blogs.umass.edu/etosch/files/2014/03/Screen-Shot-2014-03-24-at-7.02.27-PM.png"><img src="http://blogs.umass.edu/etosch/files/2014/03/Screen-Shot-2014-03-24-at-7.02.27-PM-300x100.png" alt="Screen Shot 2014-03-24 at 7.02.27 PM" width="300" height="100" class="aligncenter size-medium wp-image-930" /></a>

<h3>First Run</h3>
The first time we ran the survey was early in SurveyMan's development. We had not yet devised a way to measure breakoff and had no quality control mechanisms. Question and option position were not returned We sent the data we collected to Joe Pater and Brian Smith. Brian reported back :
<blockquote>
The results don't look amazing, but this is also not much data compared to the last experiment. This one contains six items for each group, and only 26 speakers (cf. to 107 speakers and 10 items for each group in the Language paper). 

Also, these distributions are *much* closer to 50-50 than the older results were. The fact that -athon is only getting 60% in the final-stress context is kind of shocking, given that it gets 90% schwa in my last experiment and the corpus data. Some good news though -- the finding that schwa is more likely in -(a)thon than -(a)licious is repeated in the MTurk data.

Recall that the predictions of the Language-wide Constraints Hypothesis are that:
1. final stress (Final) should be greater than non-final stress (noFinal), for all contexts. This prediction looks like it obtains here.
2. noRR should be greater than RR for -(a)licious, but not -(a)thon. Less great here. We find an effect for -thon, and a weaker effect for -licious.

licious (proportion schwaful)

             noRR        RR
Final   0.5276074 0.5182927
noFinal 0.4887218 0.5031056

thon (proportion schwaful)

             noRR        RR
Final   0.5950920 0.5670732
noFinal 0.5522388 0.4938272
</blockquote>

Our colleagues felt that this made a strong case for automating some quality control. 

<h3>Second Run</h3>
The second time we ran this experiment, we permitted breakoff and used a Mechanical Turk qualification to screen respondents. We required that respondents have completed at least one HIT and have an approval rate of at least 80% (this is actually quite a low approval rate by AMT standards). We asked respondents to refrain from accepting this HIT twice, but did not reject their work if they did so. Although we could have used qualifications to screen respondents on the basis of country, we instead permitted non-native speakers to answer, and then screened them from our analysis. In future versions, we would recommend making the native speaker question a branch question instead. 

We performed three types of analyses on this second run of the survey : we filtered suspected bots, we flagged breakoff questions and positions, and we did correlation analysis. 

Of the 67 unique respondents in this second run of the phonology survey, we had 46 self-reported native English speakers. We flagged 3 respondents as bad actors. Since we do not have full randomization for Likert scale questions, we use positional preference to flag potential adversaries. Since symmetric positions are equally likely to hold one extreme or another, we say that we expect the number of responses in either of the symmetric positions to be equal. If there is a disproportionate number of responses in a particular position, we consider this bad behavior and will flag it. Note that this method does not address random respondents. 

Our three flagged respondents had the following positional frequencies (compared with the expected number, used as our classification threshold). The total answered are out of the 96 questions that comprise the core of the study.

<table>
<tr><td>Position 1</td><td>Position 2</td><td>Position 3</td><td>Position 4</td><td>Total Answered</td></tr>
<tr><td>82 &gt;= 57.829081</td><td>4</td><td>10</td><td>0</td><td>96</td>
<tr><td>0</td><td>84 &gt;= 64.422350</td><td>9</td><td>3</td><td>96</td>
<tr><td>28 &gt;= 24.508119</td><td>10</td><td>0</td><td>1</td><td>39</td>
</table>

We calculated statistically significant breakoff for both question and position at counts above 1. We use the bootstrap to calculate confidence intervals and round the counts up to the nearest whole number. Due to the small sample size in comparison with the survey length, these particular results should be viewed cautiously:

<table>
<tr><td>Position</td><td>Count</td></tr>
<tr><td>40</td><td>2</td></tr>
<tr><td>44</td><td>2</td></tr>
<tr><td>49</td><td>3</td></tr>
<tr><td>66</td><td>2</td></tr> 
<tr><td>97</td><td>20</td></tr>
</table>

<table>
<tr><td>Wording Instance<br />(Question)</td><td>Suffix</td><td>Count</td></tr>
<tr><td>'marine'</td><td>'thon'</td><td>2</td></tr>
<tr><td>'china'</td><td>'thon'</td><td>2</td></tr>
<tr><td>'drama'</td><td>'thon'</td><td>2<td></tr>
<tr><td>'estate'</td><td>'thon'</td><td>2</td></tr>
<tr><td>'antidote'</td><td>'thon'</td><td>4</td></tr>
<tr><td>'office'</td><td>'licious'</td><td>2</td></tr>
<tr><td>'eleven'</td><td>'thon'</td><td>2</td></tr>
<td>'affidavit'</td><td>'licious'</td><td>2</td></tr>
</table> 

Two features jump out at us for the positional breakoff table. We clearly have significant breakoff at question 98 (index 97). Recall that we have 96 questions in the central block, two questions in the first block, and one question in the last block. Clearly a large number of people are submitting responses without feedback. The other feature we'd like to note is how there is some clustering in the 40s - this might indicate that there is a subpopulation who does not want to risk nonpayment due to our pricing scheme and has determined that breakoff is optimal at this point. Since we do not advertise the number of questions or the amount we will pay for bonuses, respondents must decide whether the risk of not knowing the level of compensation is worth continuing.

Like Cramer's $$V$$, we flag cases where Spearman's $$\rho$$ are greater than 0.5. We do not typically have sufficient data to perform hypothesis testing on whether or not there is a correlation, so we flag all found strong correlations. 

The correlation results from this run of the survey were not good. We had 6 schwa-final words and 6 vowel-final words. There are 15 unique comparisons for each set. Only 5 pairs of schwa-final words and 3 pairs of vowel-final were found to be correlated in the -thon responses. 9 pairs of schwa-final words and 1 pair of vowel-final words were found to be correlated in the -licious responses. If we raised the correlation threshold to 0.7, none of the schwa-final pairs were flagged and only 1 of the vowel-final pairs was flagged in each case. Seven additional pairs were flagged as correlated for -thon and 3 additional pairs were flagged for -licious.

<h3>Third Run</h3>
The third run of the survey used no qualifications. We had hoped to attract bots with this run of the survey. Recall that in the previous survey we filtered respondents using AMT Qualifications. Our hypothesis was that bots would submit results immediately.

This run of the survey was the first in this series to be launched during the work day (EST). We obtained 148 total responses, of which 129 were unique. 113 unique respondents claimed to be native English speakers. Of these, we classified 8 as bad actors.

<table>
<tr><td>Position 1</td><td>Position 2</td><td>Position 3</td><td>Position 4</td><td>Total Answered</td></tr>
<tr><td>23 &gt;= 21.104563</td><td>41</td><td>31</td><td>1</td><td>96</td>
<tr><td>0</td><td>3</td><td>51 &gt;= 40.656844</td><td>38 &gt;= 30.456324</td><td>92</td>
<tr><td>25 &gt;= 22.476329</td><td>39</td><td>31</td><td>1</td><td>96</td>
<tr><td></td><td>29</td><td>67 &gt;= 66.209126</td><td></td><td>96</td>
<tr><td>6</td><td>12</td><td>43 &gt;= 41.282716</td><td>21</td><td>82</td>
<tr><td>0</td><td>4</td><td>42 &gt;= 35.604696</td><td>50 &gt;= 38.141304</td><td>96</td>
<tr><td>6</td><td>28</td><td>32</td><td>30 &gt;= 29.150767</td><td>96</td>
<tr><td>25</td><td>0</td><td>3</td><td>68 &gt;= 64.422350</td><td>96</td>
</table>

Clearly there are some responses that are just barely past the threshold for adversary detection ; the classification scheme we use is conservative. 

Interestingly, we did not get the behavior we were courting for breakoff. Only the penultimate index had statistically significant breakoff; 51 respondents did not provide written feedback. We found 9 words with statistically significant breakoff at abandonment counts greater than 2 (they all had counts of 3 or 4). The only words to overlap with the previous run were "estate" and "antidote". The endings for both words differed between runs.

As in the previous run, only 5 pairs of schwa-final words plus -thon had correlations above 0.5. Fewer vowel-final pairs (2, as opposed to 3) plus -thon were considered correlated. For the -licious suffix, 10 out of 15 pairs of schwa-final words had significant correlation, compared with 9 out of 15 in the previous run. As in the previous run, only 1 pair of vowel-final words plus -licious had a correlation coefficient above 0.5. This results do not differ considerably from the previous run.

<h3>Fourth Run</h3>
This run was executed close to midnight EST on a Friday. Of the 157 respondents, 98 reported being native English speakers. We found 83 responses that were not classified as adversaries. Below are the 15 bad actors' responses:

<table>
<tr><td>Position 1</td><td>Position 2</td><td>Position 3</td><td>Position 4</td><td>Total Answered</td></tr>
<tr><td>65 &gt;= 64.422350</td><td>3</td><td>0</td><td>28</td><td>96</td>
<tr><td>29 &gt;= 25.847473</td><td>2</td><td>0</td><td>2</td><td>33</td>
<tr><td>0</td><td>5</td><td>87 &gt;= 63.825733</td><td>4</td><td>96</td>
<tr><td>9</td><td>18</td><td>19</td><td>37 &gt;= 35.604696</td><td>83</td>
<tr><td>13</td><td>2</td><td>18</td><td>52 &gt;= 47.483392</td><td>85</td>
<tr><td>53 &gt;= 40.029801</td><td>2</td><td>1</td><td>0</td><td>56</td>
<tr><td>96 &gt;= 66.209126</td><td></td><td></td><td>0</td><td>96</td>
<tr><td>3</td><td>12</td><td>40 &gt;= 39.401554</td><td>41 &gt;= 34.327636</td><td>96</td>
<tr><td>1</td><td>3</td><td>5</td><td>17 &gt;= 16.884783</td><td>26</td>
<tr><td>3</td><td>1</td><td>1</td><td>91 &gt;= 65.018449</td><td>96</td>
<tr><td>36 &gt;= 33.044204</td><td>42 &gt;= 40.656844</td><td>12</td><td>6</td><td>96</td>
<tr><td>6</td><td>32 &gt;= 29.804578</td><td>5</td><td>0</td><td>43</td>
<tr><td>35 &gt;= 30.456324</td><td>41</td><td>17</td><td>3</td><td>96</td>
<tr><td>20 &gt;= 19.716955</td><td>18</td><td>11</td><td>2</td><td>51</td>
<tr><td>0</td><td>0</td><td>5</td><td>91 &gt;= 63.228589</td><td>96</td>
</table>

For the -thon pairs, 1 out of the 15 schwa correlations was correctly detected. None of the vowel correlations correctly were detected. For -licious, 2 schwa pair correlations were correctly detected and 4 vowel pair correlations  where correctly detected.


For this survey we calculated statistically significant breakoff for individual questions when their counts were above 2 and for positions when their counts were above 1. The penultimate question had 38 instances of breakoff. Fourteen questions had breakoff. The maximum cases were 4 counts each for "cayenne" and "hero" for the suffix -licious. 

<h3>Entropy Comparison</h3>
We observed the following entropies over the 96 questions of interest. Note that the maximum entropy for this section of the survey is 192.

<table>
<tr><td>Instance</td><td>Initial Entropy</td><td>Entropy after<br />removing adversaries</td></tr>
<tr><td>Fourth Run</td><td>186.51791933021704</td><td>183.64368029571745</td></tr>
<tr><td>Third Run</td><td>173.14288208850118</td><td>169.58411284759356</td></tr>
<tr><td>Second Run</td><td> 172.68764654915321</td><td>169.15343722609836</td></tr>
</table>
<h4>Notes</h4>
Due to a bug in the initial analysis in the last survey (we changed formats between the third and the fourth), the first run of the analysis did not filter out any non-native English speakers and ran the analysis on 137 respondents. There were 20 adversaries calculated in total and only a handful of correlations detected. The entropy before filtering was 190.0 and after, 188.0. We also counted higher thresholds for breakoff. We believe this illustrates the impact of a few bad actors on the whole analysis.

Note that we compute breakoff using the last question answered. A respondent cannot submit results without selecting some option; without doing this, the "Submit Early" button generally will not appear. However, for the first three runs of the survey, we supplied custom Javascript to allow users to submit without writing in the the text box for the last question.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>266</wp:post_id>
		<wp:post_date>2014-03-25 07:41:55</wp:post_date>
		<wp:post_date_gmt>2014-03-25 07:41:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>experiment-report-i-breakoff-bot-detection-and-correlation-analysis-for-flat-likert-scale-surveys</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The pricing problem in SurveyMan</title>
		<link>http://blogs.umass.edu/etosch/2014/03/14/the-pricing-problem-in-surveyman/</link>
		<pubDate>Fri, 14 Mar 2014 16:34:40 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=270</guid>
		<description></description>
		<content:encoded><![CDATA[This is going to be a short post that I'll expand on more, post-portfolio...

One of the nice features of <a href="http://www.automan-lang.org">AutoMan</a> is that it manages the pricing of a task for you. The user only needs to specify a maximum amount they're willing to pay, and AutoMan will return the result at the optimal price. It first computes the number of agreeing responses needed to have high confidence that the result is correct. Then it starts at an initial baseline assignment duration (i.e. the time expected to complete the task). The initial duration may be provided by the user; the default setting is 30 seconds. AutoMan uses this time to compute the wage, which is tied to the US federal minimum wage. The assignment is posted on Mechanical Turk for some lifetime set to one hundred times the task duration. If no results come back during that lifetime, it doubles the task time and reposts the job.

Of course, there's one caveat : since AutoMan relies on there being a single answer, if the population is in disagreement, the budget will be used up and no results will be returned. 

One of the original motivations for SurveyMan was to address the idea of returning distributions of results, rather than point estimates of results. We were also interested in computing end-to-end confidence intervals for chained AutoMan computations. We realized that the underlying structure of chained distributions of functions exactly modeled surveys. Thus, SurveyMan.

SurveyMan today is quite far from this original motivation. Since we began collaborating with social scientists, we veered into experimental design and discovery, rather than static analysis or speculative execution of programs having functions returning probabilistic results. 

Determining the optimal price for a SurveyMan task is a feature from AutoMan that I'd like to see in SurveyMan. AutoMan's automated pricing scheme was possible because the system could calculate the number of responses needed to determine if the answer had been found. This is significantly more challenging for SurveyMan. If we treat a survey as a the joined probability distribution of each of its questions, then determining the sample size of "good" respondents boils down to power analysis. However, the techniques here are somewhat different; power analysis was designed for either the case where the probability of certain conditions could be computed directly, or in <em>post hoc</em> data analysis, once the data had already been collected. A first-pass consideration of what we're really looking for here is an online algorithm to determine the convergence of distribution of the joint probabilities of the questions. I don't think this is a bad start, but I worry about a decision procedure that relies on such complex data. For example, if we consider a survey that's flat, this means we treat each question as exchangeable. This does not however mean that the questions are independent. Suppose for a moment that they were; then we could say something like, the survey is a random variable defined to be the sum of the random variables representing the questions:

$$ S = Q_{1} + Q_{2} + ... + Q_{n} $$

This survey has $$n$$ questions and each of the random variables $$Q_i, 1 \leq i \leq n$$ corresponds to the distribution of the answer texts -- that is, it has no notion of its own position in the survey. 

We could then decide that our stopping condition is the case where our expectation converges. Since expectation is linear, we can look at the convergence of each question's distribution and make our decision then.

Okay, so if the questions were actually independent, I don't see being such a bad approach. I guess if we assume that the underlying population can be represented as the sum of some unknown number of independent normal distributions, we can say that the mean is a sufficient statistic and call it a day.

Of course, we have little reason to believe that the questions are independent. While randomizing the order of the questions simplifies our identification of bugs, it's quite different when we consider the convergence of distribution. We would need to consider each instance of the survey as a Bayesian network, where the previous questions are parents of the following questions. We already perform pairwise correlation tests; we could use some of this information to determine independence. If we could simplify the model sufficiently, we might be able to converge on an optimal number of samples. We could use the independence assumption to calculate a lower bound on the number of responses needed.

Anyway, this was meant to be a short post -- the idea is to present some of the difficulties of automatically determining pricing in SurveyMan. The point is that the pricing mechanism itself depends on knowing how many "good" responses we need and answering that is hard. Even if we could answer that question (and we should -- it's certainly been on the minds of our colleagues in linguistics), we would then need to consider the effects of allowing breakoff, which further complicates things.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>270</wp:post_id>
		<wp:post_date>2014-03-14 16:34:40</wp:post_date>
		<wp:post_date_gmt>2014-03-14 16:34:40</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-pricing-problem-in-surveyman</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>10</wp:comment_id>
			<wp:comment_author><![CDATA[Emma Tosch]]></wp:comment_author>
			<wp:comment_author_email>etosch@cns.umass.edu</wp:comment_author_email>
			<wp:comment_author_url>https://cs.umass.edu/~etosch</wp:comment_author_url>
			<wp:comment_author_IP>50.187.4.237</wp:comment_author_IP>
			<wp:comment_date>2014-03-15 13:20:59</wp:comment_date>
			<wp:comment_date_gmt>2014-03-15 13:20:59</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Note that determining your sample size/pricing requires real-time decision making and any future work in this area should draw on the literature from online algorithms.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>20775</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1394889659.699719905853271484375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Static Analysis</title>
		<link>http://blogs.umass.edu/etosch/2014/03/19/static-analysis/</link>
		<pubDate>Wed, 19 Mar 2014 05:49:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=308</guid>
		<description></description>
		<content:encoded><![CDATA[Tell a PL researcher that you have a new programming language and one of the first things they want to know is whether you can do static analysis on it. Lucky for us, we can! But first, some background in this space:

<h3>Prior work on static analysis for surveys</h3>
I mentioned <a href="http://blogs.umass.edu/etosch/2014/03/11/on-keeping-the-survey-a-dag/">earlier</a> that there is some <a href="http://plt.eecs.northwestern.edu/matthews-phd.pdf">prior work</a> on using programming languages to address survey problems.   

Static analysis in Topsl is constrained by the fact that Topsl permits users to define questions whose text depends on the answers to previous questions. Matthews is primarily concerned that each question is asked. While it can support randomization, this feature belongs to the containing language, Scheme, and so is not considered part of of the Topsl core. Although the 2004 paper mentions randomization as a feature that can be implemented, there is no formal treatment in any of the three Topsl-related publications.

If we also consider type-checking answer fields or enforce constraints on responses as static analysis, then most online survey tools, Blaise, QPL, and probably many more tools and services perform some kind of static analysis.

<h3 id="static_analysis">Static Analysis in SurveyMan</h3>

Surveys are interpreted by a finite state machine implemented in Javascript. To ensure that we don't cause undefined behavior, we must check that the csv is well-formed. After parsing, the SurveyMan runtime performs the following checks:

<ul>
<li><b>Forward Branch</b> Since we <a href="http://blogs.umass.edu/etosch/2014/03/11/on-keeping-the-survey-a-dag/">require the survey to be a DAG</a>, all branching must be forward. We can do this easily by comparing branch destination ids with the ids of the branch question's containing block. This check takes time linear in the block's depth.</li>

<li><b>Top Level Branch</b> At the moment, we only allow branching to <a href="http://blogs.umass.edu/etosch/2014/03/18/survey-language-essentials/#control_flow">top level blocks</a>. This check is constant. </li>

<li><b>Consistent Branch Paradigms</b> Having one "true" branch question per top level block is critically important for our survey language. Every block has a flag for its "branch paradigm." This flag indicates whether there is no branching in the block, one branch question, or whether the block should emulate sampling behavior. This check ensures that for every block, if it has a parent or siblings the following relations hold:
<p id="lattice">
[table id=1 /]
</p>
We use the following classification algorithm to assign branch paradigms:
<code id="allQuestions">
function getAllQuestionsForBlock
&nbsp; input : a block
&nbsp; output : a list of questions
begin
&nbsp; questions &lt;- this block&#039;s questions
&nbsp; if this block&#039;s branch paradigm is SAMPLE
&nbsp; &nbsp; q &lt;- randomly select one of questions
&nbsp; &nbsp; return [ q ]
&nbsp; else
&nbsp; &nbsp; qList &lt;- shuffled questions
&nbsp; &nbsp; for each subblock b in this block
&nbsp; &nbsp; begin
&nbsp; &nbsp; &nbsp; bqList &lt;- getAllQuestionsForBlock(b)
&nbsp; &nbsp; &nbsp; append bqList to qList
&nbsp; &nbsp; end
&nbsp; &nbsp; return qList
&nbsp; fi
end
</code>
<code>
function classifyBlockParadigm
&nbsp; input : a block
&nbsp; output : one of { NONE, ONE, SAMPLE }
begin
&nbsp; questions &lt;- this block&#039;s questions
&nbsp; subblocks &lt;- this block&#039;s subblocks
&nbsp; if every q in questions has a branch map
&nbsp; &nbsp; branchMap &lt;- select one branch map
&nbsp; &nbsp; if every target block in branchMap is NULL
&nbsp; &nbsp; &nbsp; return SAMPLE
&nbsp; &nbsp; else 
&nbsp; &nbsp; &nbsp; return ONE
&nbsp; else if one q in questions has a branch map
&nbsp; &nbsp; return ONE
&nbsp; else if there is no q in questions having a branch map
&nbsp; &nbsp; for each b in subblocks
&nbsp; &nbsp; begin
&nbsp; &nbsp; &nbsp; paradigm &lt;- classifyBlockParadigm(b)
&nbsp; &nbsp; &nbsp; if paradigm is ONE
&nbsp; &nbsp; &nbsp; &nbsp; return ONE
&nbsp; &nbsp; end
&nbsp; &nbsp; return NONE
&nbsp; fi
end
</code>
The ONE branch paradigm gets propagated up the block hierarchy and pushes constraints down the block hierarchy. The SAMPLE branch paradigm pushes constraints down the block hierarchy, but has no impact on parent blocks. Finally, NONE is the weakest paradigm, as it imposes no constraints on its parents or children. All blocks are set to NONE by default and are overwritten by ONE when appropriate.
</li>

<li><b>No Duplicates</b> We check the features of each question to ensure that there are no duplicate questions. Duplicates can creep in if surveys are constructed programmatically.</li>

<li><b>Compactness</b> Checks whether we have any gaps in the order of blocks. Block ordering begins at 1. This check is linear in the total number of blocks. (This check should be deprecated, due to randomized blocks.)</li>

<li><b>No branching to or from top level randomized blocks</b> We can think of the survey as a container for blocks, and blocks as containers for questions and other blocks. The top-level blocks are those immediately contained by the survey. While we permit branching from randomizable blocks that are sub-blocks of some containing block, we do not allow branching to or from top-level randomized blocks. The check for whether we have branched from a top-level block is linear in the number of top-level blocks. The check for whether we try branching to a top-level randomizable block is linear in the number of questions.
</li> 

<li><b>Branch map uniformity</b> If a block has more than one branch question, then all of its questions must be branch questions. The options and their targets are expected to be aligned. In the notation we used in a <a href="http://blogs.umass.edu/etosch/2014/03/18/survey-language-essentials/">previous post</a>, for a survey csv $$S$$, if one question, $$q_1$$ in the block spans indices $$i,...,j$$ and another question $$q_2$$ spans indices $$k,...,l$$, then we expect $$S[BLOCK][i] = S[BLOCK][k] \wedge ... \wedge S[BLOCK][j] = S[BLOCK][l]$$.
</li>

<li><b>Exclusive Branching</b> Branching is only permitted on questions where <small>EXCLUSIVE</small> is true. 
</li>
</ul>

The above are required for correctness of the input program. We also report back the following information, which can be used to estimate some of the dynamic behavior of the survey:

<ul>
<li><b>Maximum Entropy</b>
For a survey of $$n$$ questions each having $$m_i$$ responses, we can trivially obtain a gross upper bound on the entropy on the survey : $$n \log_2 (\text{max}(\lbrace m_i : 1 \leq i \leq n \rbrace))$$.
</li>
<li><b>Path Lengths</b>
<ul>
<li><p><em>Minimum Path Length</em> Branching in surveys is typically related to some kind of division in the underlying population. In the previous post, we showed how branching could be used to run two versions of a survey at the same time. More often branching is used to restrict questions by relevance. It will be appropriate for some subpopulations to see certain questions, but not others.</p>

<p>When surveys have sufficient branching, it may be possible for some respondents to answer far fewer questions than the survey designer intended -- they may "short circuit" the survey. Sometimes this is by design; if we are running a survey but are only interested in curly-haired respondents, we have no way to screen the population over the web. We may design the survey so that answering "no" to "Do you have curly hair?" sends the respondent straight to the end. In other cases, this is not the intended effect and is either a typographical error or is a case of poor survey design.</p>

We can compute minimum path length using a greedy algorithm:
<code id="minPath">
function minPathLength
&nbsp; input : survey
&nbsp; output : minimum path length through the survey
begin
&nbsp; size &lt;- 0
&nbsp; randomizableBlocks &lt;- randomizable top level blocks
&nbsp; staticBlocks &lt;- static top level blocks 
&nbsp; branchDestination &lt;- stores block that we will branch to
&nbsp; for block b in randomizableBlocks
&nbsp; begin
&nbsp; &nbsp; size &lt;- size + length(getAllQuestionsForBlock(b))
&nbsp; end
&nbsp; for block b in staticBlocks
&nbsp; begin
&nbsp; &nbsp; paradigm &lt;- b&#039;s branch paradigm
&nbsp; &nbsp; if branchDestination is initialized but b is not branchDestination
&nbsp; &nbsp; &nbsp; continue
&nbsp; &nbsp; fi
&nbsp; &nbsp; size &lt;- size + length(getAllQuestionsForBlock(b))
&nbsp; &nbsp; if branchDestination is initialized and set to b
&nbsp; &nbsp; &nbsp; unset branchDestination
&nbsp; &nbsp; fi
&nbsp; &nbsp; if paradigm = ONE
&nbsp; &nbsp; &nbsp; branchMapDestinations &lt;- b&#039;s branch question&#039;s branch map values
&nbsp; &nbsp; &nbsp; possibleBlockDestinations &lt;- sort branchMapDestinations ascending
&nbsp; &nbsp; &nbsp; branchDestination &lt;- last(possibleBlockDestinations)
&nbsp; &nbsp; fi
&nbsp; end
&nbsp; return size
end
</code>

</li>
<li><p><em>Maximum Path Length</em> Maximum path length can be used to estimate breakoff. This information may also be of interest to the survey designer -- surveys that are too long may require additional analysis for inattentive and lazy responses.</p>
<p> Maximum path length is computed using almost exactly the same algorithm as min path length, except we choose the first block in the list of branch targets, rather than the last.</p>
<p> We verified these algorithms empirically by simulating 1,000 random respondents and returning the minimum path.
</p>
</li>
<li><p><em>Average Path Length</em> Backends such as AMT require the requester to provide a time limit on surveys and a payment. While we currently <a href="http://blogs.umass.edu/etosch/2014/03/14/the-pricing-problem-in-surveyman/">cannot compute the optimal payment</a> in SurveyMan, we can use the average path length through the survey to estimate the time it would take to complete, and from that compute the baseline payment. </p>
</p> The average path length is computed empirically. We have implemented a random respondent that chooses paths on the basis of positional preferences. One of the profiles implemented is a Uniform respondent; this profile simply selects one of the options with uniform probability over the total possible options. We run 5,000 iterations of this respondent to compute the average path length. 
</p>
<p>
Note that average path length is the average over possible paths; the true average path length will depend upon the preferences of the underlying population.
</p>
</li>
</ul>
</li>
</ul>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>308</wp:post_id>
		<wp:post_date>2014-03-19 05:49:00</wp:post_date>
		<wp:post_date_gmt>2014-03-19 05:49:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>static-analysis</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Survey Language Essentials</title>
		<link>http://blogs.umass.edu/etosch/2014/03/18/survey-language-essentials/</link>
		<pubDate>Tue, 18 Mar 2014 02:33:55 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=310</guid>
		<description></description>
		<content:encoded><![CDATA[As discussed in a <a href="http://blogs.umass.edu/etosch/2014/03/11/observational-studies-surveys-quasi-experiments-and-experiments/">previous post</a>, web surveys are increasingly moving toward designs that more closely resemble experiments. A major goal in the SurveyMan work is to capture the underlying abstractions of surveys and experiments. These abstractions are then represented in the Survey language and runtime system.
<h3>Language</h3>
When the topic of a programming language for surveys came up, I initially proposed a SQL-like approach. Emery quite strongly suggested that SQL was, in general, a non-starter. He suggested a tabular language as an alternative approach that would capture the features I was so keen on, but in a more accessible format. To get started, he suggested I look at <a href="http://pages.cs.wisc.edu/~dbbook/openAccess/thirdEdition/qbe.pdf">Query by Example</a>.

The current language can be written as a csv in a spreadsheet program. This csv is the input to the SurveyMan runtime system, which checks for correctness of the survey and performs some lightweight static analysis. The runtime system then sends the survey to the chosen platform and processes results until the stopping condition is met.

The csv format has two mandatory columns and 9 optional semantically meaningful columns. Most surveys are only written with a subset of the available columns. The most up-to-date information can be found on the Surveyman <a href="https://github.com/etosch/SurveyMan/wiki/Csv-Spec">wiki</a>. The mandatory columns are <small>QUESTION</small> and <small>OPTIONS</small>. Column names can appear in any case and any columns that are not semantically meaningful to the SurveyMan runtime system will be pushed through from the input csv to the output csv.

Columns may appear in any order. Rows are partitioned into questions; all rows belonging to a particular question must be grouped together, but questions may be appear in any order. Questions use multiple rows to represent different answer options. For example, if a question asks what your preferred ice cream flavor is and the options are vanilla, chocolate, and strawberry, each of these will appear in their own row, with the exception of the first option, which will appear on the same row as the question. More formally, if we let the csv be represented by a dictionary indexed on column name and row numbers, then for Survey $$S$$ having column set $$c$$, some question $$Q_i$$ spanning rows $$\lbrace i, i+1, ..., j\rbrace, i &lt; j $$, would be represented by a $$|c|\times (j - i + 1)$$ matrix: $$S[:][i:j+1]$$, and its answer options would be a vector represented by $$S[OPTIONS][i:j+1]$$.
<h4>Display and Quality Control Abstractions</h4>
The order of the successive rows in a question matters if the order of the options matter. In our ice cream example, the order does not matter, so we may enter the options in any order. If we were instead to ask "Rate how strongly you agree with the statement, 'I love chocolate ice cream.'", the options would be what's called a <a href="http://en.wikipedia.org/wiki/Likert_scale">Likert scale</a>. In this case, the order in which the options are entered is semantically meaningful. The <small>ORDERED</small> column functions as a flag for this interpretation. Its default value is false, so if the column is omitted entirely, every question's answer options will be interpreted as unordered.

We do not express display properties in the csv at all. We will discuss how questions are displayed in the <a href="#runtime">runtime section</a> of this post. We had considered adding a <small>CLASS</small> column to the csv to associate certain questions with individual display properties, but this would have the effect of not only introducing, but encouraging non-uniformity in the question display. This introduces additional factors that we would have to model in our quality control. Collaborators who are interested in running web experiments are the most keen on this feature; as we expand from surveys into experiments, we would need to understand what kinds of features they hope to implement with custom display classes, and determine what the underlying abstractions are.

That said, it happens that two columns we use for quality control also provides once piece of meaningful display information. <small>EXCLUSIVE</small> is a boolean-valued column indicating whether a respondent can only pick one of the answer options, or if they can choose multiple answer options. When <small>EXCLUSIVE</small> is true for a question, its answer options appear as a radio button inputs and for $$m$$ options, the total number of unique responses is $$m$$. When <small>EXCLUSIVE</small> is set to false, the answer options appear as radio buttons and the total number of unique responses is $$2^m - 1$$ (we don't allow users to skip over answering questions). We also support a <small>FREETEXT</small> column, which can be interpreted as a boolean-valued column or a regular expression. When <small>FREETEXT</small> ought to represent a regular expression $$r$$, it should be entered into the appropriate cell as $$\#\lbrace r \rbrace$$.

We also provide a <small>CORRELATED</small> column to assist in quality control. Our bug detection checks for correlations between questions to see if any redundancy can be removed. However, sometimes correlations are desired, whether to confirm a hypothesis or as a form of quality control. We allow sets of questions to be marked with an arbitrary symbol, indicating that we expect them to be correlated. As we'll discuss in later posts, this information can be critical in identifying human adversaries.
<h4 id="control_flow">Control Flow</h4>
Prior work on survey languages has focused on how surveys, like programs, have control flow. We would be remiss in our language design if we failed to address this.

The most basic survey design is a flat survey. The default behavior in SurveyMan is to randomize the order of the questions. This means that for a survey of $$n$$ questions, there are $$n$$ factorial possible orderings (aside : WordPress math mode doesn't allow "!"?!?!?). It is not always desirable to display one of every possible ordering to a respondent. There are cases where we will want to group questions together. These groups are called "blocks."

Blocks are a critical basic unit for both surveys and experiments. Conventional wisdom in survey design dictates that topically similar questions be grouped together. We recently launched a survey on wage negotiation that has three topical units : demographic information, work history, and negotiation experience. Experiments on learning require a baseline set of questions followed by the treatment questions and follow-up questions. Question can be grouped together using the <small>BLOCK</small> column.

Blocks are represented by the regular expression <code> _?[1-9][0-9]*(\._?[1-9][0-9]*)* </code>. Numbering begins at 1. Like an outline, the period indicates hierarchy: blocks numbered 1.1 and 1.2 are contained in block 1. We can think of block numbers as arrays of integer. We define the following concepts:


<code>
function <b>depth</b>
&nbsp; input : A survey block
&nbsp; output : integer indicating depth
begin
&nbsp; idArray &lt;- block id as an array
&nbsp; return <b>length</b>(idArray)
end
</code>



<code>
function <b>questionsForBlock</b>
&nbsp; input : A surveyBlock
&nbsp; output : A set of questions contained in this particular block
begin
&nbsp; topLevelQuestions &lt;- a list of questions contained directly in this block
&nbsp; subblocks &lt;- all of the blocks contained in this block
&nbsp; blocks &lt;- return value initialized with topLevelQuestions
&nbsp; for block in subblocks
&nbsp; do
&nbsp; &nbsp; blocks &lt;- blocks + questionsForBlock(block) 
&nbsp; done
&nbsp; return blocks
end
</code>


We can say that blocks have the following two properties:

<ul>
<li><b> Ordering </b> : Let the id for a block $$b$$ of depth $$d$$ be represented by an array of length $$d$$, which we shall call $$id$$. Let $$index_d$$ be a function of a question representing its dynamic index in the survey (that is, the index at which the question appears to the user). Then the block ordering property states that <br />
<p>
$$\forall b_1 \forall b_2 \bigl( d_1 = d_2 \; \wedge \; id_{b_1}[\mathbf{\small depth}(b_1)-1] &lt; id_{b_2}[\mathbf{\small depth}(b_2)-1] \; \longrightarrow \quad \forall q_1 \in \mathbf{\small questionsForBlock}(b_1)\;\forall q_2 \in \mathbf{\small questionsForBlock}(b_2) \; \bigl( index_d(q_1) &lt; index_d(q_2) \bigr) \bigr)$$ 
</p>
This imposes a partial ordering on the questions; top level questions in a block may appear in any order, but for two blocks at a particular depth, all of the questions in the lower numbered block must be displayed before any of the questions in the block at the higher number.  
</li>
<li><b> Containment</b> : If there exists a block $$b$$ of depth $$d &gt; 1$$, then for all $$i$$ from 0 to $$d$$, there must also exist blocks with ids $$[id_b[0]],..,[id_b[0],.., id[d-1]]$$. Each of these blocks is said to contain $$b$$. If $$b_1$$ is a containing block for $$b_2$$, then for all top level questions $$q_1$$ in $$b_1$$, it is never the case that 
$$\exists q_i, q_j, q_k \bigl(\lbrace q_i, q_j \rbrace \subset \mathbf{\small questionsForBlock}(b_2) \; \wedge \; q_k \in  \mathbf{\small questionsForBlock}(b_1) \; \wedge \; index_d(q_i) &lt; index_d(q_k) &lt; index_d(q_j)$$ 
That is, none of the top level questions may be interleaved with a subblock's question. Note that we do not require the survey design to enumerate containing blocks if they do not hold top-level questions.
</li>
</ul>

Sometimes what a survey designer really wants is a grouping, rather than an ordering. Consider the two motivating examples. It's clear for the psychology experiment that an ordering is necessary. However, the wage survey does not necessarily need to be ordered; it just needs to keep the questions together. If a survey designer would like to relax the constraints of an ordering, they can prefix the block's id with an underscore at the appropriate level. For a block numbered 1.2.3, if we change this to _1.2.3, then all other members of block 1 will need to have their identifiers modified to _1. If we were instead to change 1.2.3 to 1.2._3, then this block would be able to move around inside block 1.2 under the same randomization scheme as block 1.2's top level questions. Note that you cannot have a randomized block _n and an unrandomized block n. 

Our last column to consider is the <small>BRANCH</small> column. Branching is when different choices for a particular question's answers lead to different paths through the survey. The main guarantee we want to preserve with branching is that the set of nodes in a respondent's path through the survey depends solely on their responses to the questions, not the randomization. 

The contents of the <small>BRANCH</small> column are block ids associated with the branch destination. There can be exactly one branch question per top level block. This ensures the path property mentioned above. The <small>BRANCH</small> column also doubles as a sampling column. We permit one special branch case, where a block contains no subblocks and all of its top level questions are branch questions. In this case, the block is treated as a single question, whose value is selected randomly. If the resulting distributions of answers for the questions in this block are not determined to be drawn from the same underlying distribution, the system flags the block. Since the assumption is that all of the questions are semantically equivalent, the contents of the <small>BRANCH</small> column must be identical for each question. If the user does not want to branch to a particular location, but instead wants the user to see the next question (as with a free, top-level question), then <small>BRANCH</small> can be set to the keyword NULL.

We currently only allow branching to a top-level block, but are considering weakening this requirement. 

<h4>Deprecated and Forthcoming Columns</h4>
The psycholinguistic surveys we've been running often require external files to be played. Non-text media are also often used in other web experiments. We had previously supported a <small>RESOURCE</small> column that would take a URL and display this under the question. Since the question column supports arbitrary HTML, it seemed redundant to include the <small>RESOURCE</small> column. These stimuli are part of the question, so separating them out didn't make much sense. While the current code still supports this column, it will not in the future.

Another column that we are considering deprecating is the <small>RANDOMIZE</small> column. We see two issues with this column : first of all, there has been some confusion about whether it referred to randomizing the question's answer options or if it referred to allowing the question's order to be randomized. It referred to the former, since question order randomization is handled by the <small>BLOCK</small> column. Secondly, we do not see any use-case for not randomizing the question options. It would exist to satisfy a client who has no interest in our quality control techniques.

<a href="http://people.umass.edu/ppizzo/">Presley</a> has suggested adding a <small>CONDITION</small> column, for use in web experiments. This addition is tempting; I would first need to consider whether the information it provides is already captured by the <small>CORRELATED</small> column (or whether we should simply rename the <small>CORRELATED</small> column <small>CONDITION</small> and add some additional behavior to the execution environment).

<h3 id="runtime">Runtime System</h3>
<!--The basic building block of both surveys and experiments is a question-answer pair. Every survey or experiment

address customization -->
Okay, this blog post is too long already...runtime post forthcoming!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>310</wp:post_id>
		<wp:post_date>2014-03-18 02:33:55</wp:post_date>
		<wp:post_date_gmt>2014-03-18 02:33:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>survey-language-essentials</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Creating Sophisticated Online Surveys : A Case Study. </title>
		<link>http://blogs.umass.edu/etosch/2014/03/19/creating-sophisticated-online-surveys-a-case-study/</link>
		<pubDate>Wed, 19 Mar 2014 02:36:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=534</guid>
		<description></description>
		<content:encoded><![CDATA[As discussed in <a href="http://blogs.umass.edu/etosch/2014/03/18/survey-language-essentials/">yesterday's post</a>, the SurveyMan language allows sophisticated survey and experimental design. Since we're about to launch the full version of <a href="https://github.com/sckingsley?tab=activity">Sara Kingsley's</a> wage survey, I thought I'd step through the process of designing the branching and blocking components.

The survey in question is completely flat (i.e. no blocking, no branching) and contains a variety of ordered and unordered radio (exclusive) and checkbox (not exclusive) questions. We would like to be able to run a version that is completely randomized and a version that's totally static and based on conventional survey methodology. Ideally we'd like to be able to run both surveys under the same set of conditions.

We could run both versions of the survey at the same time on AMT. However, we'd run into trouble with people who try to take it twice. To solve this problem, we could run one version, issue "reverse qualifications" a la Automan to the people who answered the first one, and then run the second version. This would entail some extra coding on the side, and while support for longitudinal studies and repeated independent experiments would require keeping a database of past participants and flagging them as uniquely qualified or disqualified for a survey, this feature is not currently supported in SurveyMan. What we'd really like to do, though, is run the two surveys concurrently, in the same HIT such that the process looks something like this : <a href="http://blogs.umass.edu/etosch/files/2014/03/twoVersions2-e1395191847653.png"><img /></a>

Fortunately for us, we can implement both versions of the survey as a single survey using the underlying language. 

Let's start by looking at what the two versions would look like if we were to run them as separate surveys : 
<a href="http://blogs.umass.edu/etosch/files/2014/03/twoVersions21.png"><img src="http://blogs.umass.edu/etosch/files/2014/03/twoVersions21-300x200.png" alt="twoVersions2" width="300" height="200" class="aligncenter size-medium wp-image-568" /></a>
The survey on the left groups all of the questions into one block. The survey on the right places each question into its own block. In order to make the survey on the right truly full static, we would also need to set the <small>RANDOMIZE</small> column to false.* 

As we'll see in my future blog post on the runtime system, the evaluation strategy forces all paths in the survey to join. If there isn't a semantically meaningful way to join, this can be done trivially by creating a final block that contains an instructional "question" (e.g. "Thank you for taking the time to complete our survey") and a submit button. 

Now, in order to represent two separate paths, we will need to place the ordered survey questions inside a container block:
<a href="http://blogs.umass.edu/etosch/files/2014/03/twoVersions3.png"><img src="http://blogs.umass.edu/etosch/files/2014/03/twoVersions3-300x279.png" alt="twoVersions3" width="300" height="279" class="aligncenter size-medium wp-image-578" /></a>

The three orange blocks are top-level: let the fully randomized version be Block 1, the fully static version be Block 2, and the joining "question" Block 3. If we were to simply annotated them according to this scheme, we would end up having every respondent take two versions of the survey. We now need some way of ensuring that the two versions of the survey represent parallel paths through the survey such that <em>the expected number of respondents through each path is 50% of the sample size</em>.

Let's work backwards from Block 3. Assume that the respondent is already on one of the parallel paths. We can ensure that Block 3 follows immediately from both Block 1 and Block 2 by using the <small>BRANCH</small> column. We add branching to Block 3 for every options in the last question of the fully static survey and do the same for that question's equivalent in the fully randomized survey (both of which are not contained in different blocks in our current survey). Clearly we will branch to Block 3 after answer the last question in the fully static version. Because we require that the respondent answer every question in a block before evaluating a branch, the respondent will also answer every question in the randomized version.

<a href="http://blogs.umass.edu/etosch/files/2014/03/twoVersions4.png"><img src="http://blogs.umass.edu/etosch/files/2014/03/twoVersions4-300x279.png" alt="twoVersions4" width="300" height="279" class="aligncenter size-medium wp-image-588" /></a>

Finally, we need to be able to randomly assign respondents to each of the paths. At first, I thought this would be a cute example of randomized blocks, where we could change Block 1 to Block _1. However, this would not give us an even split in the sample population. There are three possible orderings of the blocks : [_1 2 3], [2 _1 3], [2 3 _1]. 2/3 of the population would then see Block 2 first.**

We could instead overwrite the <code>setFirstQuestion</code> method of the SurveyMan Javascript option in  <code>custom.js</code> to randomly assign the first question in the appropriate block. 

We could also just add what we'll call a fork question at the beginning of the survey, asking them to choose between two options:<a href="http://blogs.umass.edu/etosch/files/2014/03/twoVersions5.png"><img src="http://blogs.umass.edu/etosch/files/2014/03/twoVersions5-272x300.png" alt="twoVersions5" width="272" height="300" class="aligncenter size-medium wp-image-606" /></a>

* Looks like I finally found a use-case for that <small>RANDOMIZE</small> column -- the experimental control! Maybe I shouldn't deprecate after all...
** As an aside, I want to point out that this particular example illustrates a case for <a href="http://people.umass.edu/ppizzo/">Presley</a>'s suggestion that we interpret the available indices for randomization over the indices of the blocks marked as randomizable. Here we would have marked both Block 1 and Block 2 as randomizable. I'm still not sure if this would be overly constrained and will discuss further in a future blog post.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>534</wp:post_id>
		<wp:post_date>2014-03-19 02:36:00</wp:post_date>
		<wp:post_date_gmt>2014-03-19 02:36:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>creating-sophisticated-online-surveys-a-case-study</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Simulation and Detecting Bugs : Correlation</title>
		<link>http://blogs.umass.edu/etosch/2014/03/24/simulation-and-detecting-bugs-correlation/</link>
		<pubDate>Mon, 24 Mar 2014 20:25:32 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=684</guid>
		<description></description>
		<content:encoded><![CDATA[We need some way of determining whether the diagnoses of SurveyMan's bugs is correct. It's always possible that a particular technique has a flaw in it, or that a test for a certain feature is not sensitive enough to detect the differences we would like it to detect. We have designed a simulator as a sanity check for our algorithms. 

<h3>Simulator setup</h3>

The first step in our simulator setup is to generate gold-standard data; this is after all the reason for bothering with a simulator in the first place. 

Consider the problem of bot detection. We will need to know the ground truth of who is a bot and who is not. Modeling bots explicitly is easy. We already do this in our static analysis. Modeling human respondents is more challenging. 

We define a <em>profile</em> to be a collection of preferences over a survey. These preferences are the probabilities that an instance of a profile (i.e. a respondent) will choose a particular answer option for a question. For example, uniform adversaries will choose each answer option with equal probability. 

In order to emulate human behavior, we allow the non-bot population of responses to be drawn from some number of clusters. A cluster is generated by randomly assigning a probabilities $$p_1$$ drawn from the interval $$(1/m_i, 1)$$ for each $$q_i$$. We say that a respondent belonging to one of these clusters has a preference for a particular answer, but may choose another answer due to factors we either cannot control or did not account for. These other preferences are assigned uniform probability : $$\frac{1-p_i}{m-1}$$. Sometimes a preference will be very strong (e.g. assigned a probability &gt; 0.8). Sometimes the preference will only be slight, in which case it will be close to $$1/m_i$$.

We can then inject biases into the generated responses and test our bias detection algorithms, testing the robustness of our techniques by varying the impact of bad actors on our results.

<h4 id="correlation">Correlation</h4>
Any measure of correlation between questions in the survey must consider what's called the "level of measurement" of each question. Levels of measurement determine the statistical tools we can use to analyze the data. There are four levels of measurement in total:

<ol>
<li><b>Nominal</b> Data that fall into categories that have no order are said to be nominal. Generally this will correspond to radio button questions such as "What is your gender." This data will be represented by a categorical variable and permutation tests will have to be used to analyze any correlations. Tests on nominal data are sensitive to sparsity; since they are not continuous, we cannot use interpolation to make inferences.
</li>
<li><b>Ordinal</b> Ordered questions fall into this category. This is probably the most common type of survey question. Surveys that ask users about their preferences or to provide rankings for data are using ordinal data. The more common and powerful statistical significance and correlation tests begin at this level.</li>
<li><b>Interval</b> Where ordinal questions required the ability to rank, interval questions require there to be meaningful distances between answer options. Likert scale questions are an attempt to capture interval questions (although they are often analyzed using ordinal tests, since their measurement is imperfect). Interval questions attempt to capture the magnitude of difference between ranked answers.</li>
<li><b>Ratio</b> Ratio questions are "true" numeric questions - that is, individual answers have meaningful magnitude because there is a known underlying zero grounding the measurement. Weight, date of birth, and income are all ratio questions. These questions permit the most powerful statistical tests because data can be interpolated.</li>
</ol>
SurveyMan uses correlation in two ways. The <small>CORRELATED</small> can be used to flag sets of questions that the survey designer expects to have statistical correlation. Flagged questions can be used to validate or reject hypotheses and to help detect bad actors. Alternatively, if a question that is not marked as correlated is found to have statistically significant correlation, then we flag this question. Questions are compared on a pair-wise basis. This information can be used in a variety of ways : 
<ul>
<li> The survey designer could decide to remove one or more of the correlated questions, if their predictive power is strong enough to infer responses from the remaining questions. It is ultimately the responsibility of the survey designer to use good judgement and domain knowledge when deciding to remove questions; note that because we only check pair-wise correlation, we cannot capture the impact of groups on a particular outcome. We do not model interactions between variables.</li>
<li> The survey designer could use discovered correlations to assist in identification of cohorts or bad actors by updating the entries in the <small>CORRELATED</small> column appropriately.</li>
</ul>

We only support automated correlation analysis between exclusive (radio button) questions. These questions may be ordered or unordered.

For two questions such that at least one of them is unordered, we return the $$\chi^2$$ statistic, its p-value, and compute Cramer's $$V$$ to determine correlation. We also use Cramer's $$V$$ when comparing a nominal and an ordinal question. Ordinal questions are compared using Spearman's $$\rho$$. Since in practice we rarely have sufficient data to return confidence intervals on such point estimates, we simply flag the pair and leave the interpretation of the values up to the survey designer.

For non-exclusive (checkbox) ordered questions, we would need a meaningful metric to understand what the relationship between subsets of checkboxes are. For example, in a question of four answer options A, B, C, and D, we would need to know how to compare the answers {A,B}, {B,C}, and {A,C}. If their values are additive and we let their weights correspond to their indices, how far apart are the choices {A,B} and {C}? Any analysis would have to be domain-specific and thus falls outside the scope of SurveyMan.

For non-exclusive (checkbox) unordered questions, we also run into trouble. We don't have to worry about specialized distance functions, but we do have to worry about the fact that our categories are not exclusive. That is, we can no longer use a categorical random variable to represent the question, since a single respondent may belong to multiple categories. This violates the conditions of all known tests. We could use subsets as our events instead and analyze them as we do with exclusive data. However, the contingency table for a question $$q_i$$ having $$m$$ options will have $$2^m - 1$$ as one of its dimensions. While Cramer's $$V$$ reduces the impact of the degrees of freedom on the $$\chi^2$$ test, we still have the problem of sparsity in the table's cells. We observed in simulation that, as sparsity increased, the range of errors increased. While we would still sometimes see the injected correlated questions show up, we also saw many more cases of a question being classified as having a correlation coefficient close to 0 when compared against itself. The misclassification wasn't too bad for three checkbox options, but it was unacceptable at 4. As a result, we do not support correlation on checkbox questions.

If users want to do correlation on checkbox questions anyway, they can enumerate the subsets and display these as exclusive questions. It's true that the very problem we try to avoid with checkbox questions could still be a problem with radio button questions. However, it's unusual in practice to have a large number of nominal choices. We could compute the required number of random respondents needed to have at least 5 entries in each cell of the contingency table and only analyze correlation if this condition is met. This is something to consider for future SurveyMan releases and requires further investigation.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>684</wp:post_id>
		<wp:post_date>2014-03-24 20:25:32</wp:post_date>
		<wp:post_date_gmt>2014-03-24 20:25:32</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>simulation-and-detecting-bugs-correlation</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Regarding Survey Paths</title>
		<link>http://blogs.umass.edu/etosch/2014/03/21/regarding-survey-paths/</link>
		<pubDate>Fri, 21 Mar 2014 04:38:16 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=688</guid>
		<description></description>
		<content:encoded><![CDATA[The key invariant we wish to preserve throughout the survey is that an individual's answers are the same, no matter which version of the survey they see. That is, if the same respondent were to take the survey twice, beginning to end, the total number of questions seen and answers given would remain the same. We'll see in this post how this invariant influences enumerating and computing metrics on paths. In a later post, we'll see how this invariant leads us to analyze survey "bugs."

We use paths through the survey to aid in the debugging process and to automate some parameters to the crowdsourcing backend. Let's take a moment to look at how some of the design choices we've made impact possible paths through the survey:

<h3>Top-level blocks</h3>
At the top level, a survey is composed of blocks. If there are no blocks (i.e. the survey is completely flat), we can say that all questions belong to a single block.

Top-level blocks have the following <a href="http://blogs.umass.edu/etosch/2014/03/19/static-analysis/#lattice">branch requirements</a>:
<ol>
<li>There is only one "true" branch question per block.</li>
<li>Branch question destinations must all be top-level blocks.</li>
</ol>
If there is a branch question, it may appear in any position, subject to any blocking constraints. The branching action isn't evaluated until every question that we plan to execute in the block is executed. 

<h3>Randomizable blocks</h3>
Top-level randomizable blocks are not allowed to have branch questions. They also cannot be branch destinations. Permitting branching to or from randomizable blocks could cause us to have a loop in the program. For example, if block 3 branches to some randomizable block we'll call X, and X happens to appear before block 1, we have a problem, since we will have already executed X. Similarly, if we branch from X to 3 and X appears as the last block in a survey, we will also have a loop.

In addition to violating our DAG guarantee, branching to or from randomizable blocks could cause nondeterminism in the number of questions asked. If block 3 branches to X, but X is the last block in the survey for one person, but immediately follows 3 for another, we will be allowing randomness to have an influence on the answer set. 

Note that if we are using branching for sampling, the above doesn't apply.

On the other hand, randomizable subblocks may be a branch source. If their branch destination is ALL, we have no conflicts, since we will choose the next (randomly determined) sequential question as usual. If their branch paradigm is ONE, then their branch question becomes the one branching question for the topmost enclosing block. This is exactly the same as for non-randomizable subblocks.

<h3>Enumerating paths</h3>
We stated at the beginning of this post that answer sets should be invariant for the version of the survey seen. In the previous post, we described how path data can be used for static analysis. 

When we view a path through the survey as the series of questions seen and answered, the computation of all such paths is intractable. Consider a short survey we recently ran that had 16 top-level randomizable blocks, each having four variants and no branching. We have 16! total block orderings. Then, since we choose from four different questions, we end up with $$2^{16!}$$ unique paths. 

Fortunately, the computations we mentioned in the static analysis post do not require knowing the contents of the path. They only require that we know the size of the path. Since we have required that every appropriate question in a top level block be executed, we can treat top level blocks as nodes in the possible survey DAG. The weight of these nodes (i.e. the size of the block) can be computed statically. Then we only need to consider the branching between these top level blocks. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>688</wp:post_id>
		<wp:post_date>2014-03-21 04:38:16</wp:post_date>
		<wp:post_date_gmt>2014-03-21 04:38:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>regarding-survey-paths</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Runtime</title>
		<link>http://blogs.umass.edu/etosch/2014/03/22/runtime/</link>
		<pubDate>Sat, 22 Mar 2014 20:59:09 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=710</guid>
		<description></description>
		<content:encoded><![CDATA[<h3> Where does the runtime fit in?</h3>

A SurveyMan workflow looks something like this : 
<a href="http://blogs.umass.edu/etosch/files/2014/03/flow.png"><img src="http://blogs.umass.edu/etosch/files/2014/03/flow-300x296.png" alt="flow" width="300" height="296" class="aligncenter size-medium wp-image-744" /></a>
We currently only support csv input; the design of the csv language is such that a human could reasonably write a survey in it. Molly is currently working on a Python library that will streamline some of the more advanced features of SurveyMan and Python proficient users to design surveys in Python and output a JSON equivalent to a csv survey. 

The main SurveyMan program is written in Java. It handles all of the static checks on the input and loads the survey into an internal representation. When it is time to send the program to a crowdsourcing platform, the Java program produces a minimal JSON representation of the survey and sends this wrapped in HTML. When the crowd platform is Mechanical Turk, the HTML is sent inside an XML payload and handled by AMT. When the platform is a local server, it just sends the HTML. We can support any crowdsourcing platform that allows us to send arbitrary HTML. 

We send the minimal JSON representation for two reasons. The first is that for AMT, we have a maximum payload allowed. The second reason is because not all of the survey data is useful to the runtime. We need sufficient information to determine how to display the survey to the user. We don't do any analysis on the crowdsourcing platform, so we don't need to send over information like the <small>CORRELATED</small> column or any user-provided columns. We do allow survey designers to add custom Javascript to their survey, which can be used to perform computations and return the results of those computations. 

<h3>Input</h3>
The input to the SurveyMan program is a survey csv. This csv is parsed and loaded into an internal representation, which is then translated into a JSON payload. The resulting JSON representation of the survey is sent to the crowdsourcing platform. This survey JSON is then interpreted by a Javascript program that controls flow through the survey and how individual questions are displayed.

<h3>The Finite State Machine</h3>
SurveyMan surveys are interpreted in Javascript on a finite state machine that controls the underlying evaluation and display. 

There are two layers to the survey interpreter. The first handles survey logic; it essentially runs in a loop, proffering questions and consuming responses. The second layer handles display information, updating the HTML in response to certain events. 

<h4> User's view of the FSM </h4>
A respondent navigates to the webpage displaying the survey. For AMT, we show a consent form in the HIT preview. When an individual accepts a HIT, they begin a survey. The consent form mechanism is not currently implemented in the local server.

The user then sees the first question* and the answer options. When they select some answer (or type in a text box), the next button and a "Submit Early" button appear. If the question is instructional and is not the final question in the survey, only the next button appears. When the respondent reaches the final question, only a 

Each user sees a different ordering of questions. However, a single user's questions are presented in the same order; the random number generator is seeded with the user's session or "assignment" id. In AMT, this means that if the user navigates away from the page and returns to the HIT, the question order will be the same upon second viewing.

Only one question is displayed at a time. This design decision is not purely aesthetic; it helps us measure breakoff.

<h4>Underlying machinery</h4>
In addition to the functions that implement the state machine, there are three other underlying data structures:

<ol>
<li><b>Block Stack</b> : Since our path through the survey is determined by branching over blocks, and since we may only branch forward, we keep the top-level blocks on a stack.</li>
<li><b>Question Stack</b> : Once we know which block we're to execute, we can fetch the appropriate questions for that block.</li>
<li><b>Branch Reference Cell</b> : The branch target is stored here, since we defer executing any branching at least until all of the questions in the block have been seen.</li>
</ol>

The SurveyMan interpreter is initialized as follows : the survey JSON is parsed into an internal survey representation and then randomized using a seeded random number generator. This randomization preserves necessary invariants (e.g. the partial order over blocks). We then push the top level blocks onto the block stack. Then we pop off the first block and initialize the question stack using <a href="http://blogs.umass.edu/etosch/#allQuestions">getAllQuestions</a>.

The interpreter's execution runs as follows :
<code>
procedure run
begin
&nbsp; while blockStack is not empty
&nbsp; do
&nbsp; &nbsp; question &lt;- getNextQuestion()
&nbsp; &nbsp; display question and answer options
&nbsp; &nbsp; answer &lt;- user-provided answer
&nbsp; &nbsp; store answer
&nbsp; &nbsp; if the question is a branch question
&nbsp; &nbsp; then
&nbsp; &nbsp; &nbsp; branchRef &lt;- answer&#039;s branch destination
&nbsp; &nbsp; fi
&nbsp; &nbsp; if user selects submit
&nbsp; &nbsp; then
&nbsp; &nbsp; &nbsp; return stored answers
&nbsp; &nbsp; fi
&nbsp; done
&nbsp; return stored answers
end
</code>

Answers are stored in the HTML forms in the usual way. We have three main divs in the HTML : one for the displayed question, one for the displayed answers, and one for response input data. The response input elements are kept in a hidden div below the answer display div. Each new response is inserted above the previous one, emulating a stack. 

The function <code>getNextQuestion</code> handles control flow and is the primary point of interaction with the FSM:
<code>
function getNextQuestion
&nbsp; output : question
begin
&nbsp; if questionStack is empty
&nbsp; &nbsp; if branchRef is set
&nbsp; &nbsp; then
&nbsp; &nbsp; &nbsp; while true
&nbsp; &nbsp; &nbsp; do
&nbsp; &nbsp; &nbsp; &nbsp; nextBlock &lt;- peek(blockStack)
&nbsp; &nbsp; &nbsp; &nbsp; if nextBlock = branchRef 
&nbsp; &nbsp; &nbsp; &nbsp; then 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; questionStack &lt;- getAllQuestions(pop(blockStack))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; unset branchRef
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break
&nbsp; &nbsp; &nbsp; &nbsp; else if nextBlock is a floating block
&nbsp; &nbsp; &nbsp; &nbsp; then
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; questionStack &lt;- getAllQuestions(pop(blockStack))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break
&nbsp; &nbsp; &nbsp; &nbsp; else
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; pop(blockStack)
&nbsp; &nbsp; &nbsp; &nbsp; fi
&nbsp; &nbsp; &nbsp; done
&nbsp; &nbsp; else 
&nbsp; &nbsp; &nbsp; questionStack &lt;- getAllQuestions(pop(blockStack))
&nbsp;&nbsp;fi
&nbsp; return pop(questionStack)
end
</code>

<h3>Determinism in the runtime</h3>
There are two main sources of nondeterminism in SurveyMan. The first is our random number generator; the second is human behavior. 

We had previously generated a new static HTML version of the survey for each respondent. Each question was contained in its own div and branch destinations pointed to these divs. None of the blocking structure was visible to the user. The Javascript was minimal; if the question was a branch, it would hide the current question and display the the div with the appropriate id. If the question did not have branching, it would navigate the DOM and just display the next div. 

This prior approach lead to a technical issue that proved to be a nice metaphor for the underlying system. The problem was that, since we had to send over unique HTML for every desired respondent, we ended up with many HITs posted on AMT. AMT groups together HITs that have similar parameters and facilitates workers' acceptance of consecutive HITs in groups. Having many HITs not only attracted bad actors, but it also annoyed honest respondents, who did not always realize that they were not supposed to answer all of the HITs. 

While each respondent sees a different version of the survey, the underlying survey is essentially "the same." It made more sense to move the survey logic into the Javascript and post a single HIT. It was also cleaner to group together user behavior with survey execution, rather than flattening the survey and adding an extra (albeit simpler) evaluator that primarily responded to user nondeterminism.

What's key about these two approaches is that the two types of nondeterminism are quite different from each other. Randomization gives us predictable behavior in the limit and, <a href="http://blogs.umass.edu/etosch/2014/03/19/static-analysis/#minPath">depending upon the task</a> may not even have an impact on the computation. Randomization does not occur in response to user behavior and can be done before executing anything. Once we know which block to execute, the total set of questions is almost deterministic. That is, rather than having to account for nondeterminism and uncertainty at every step in the evaluation, we only have to handle it at known junctures. So long as we can prove that the set of surveys produced at each junction are equivalent, we know that the evaluation is sound.


* The respondent may also see a breakoff notice first. This is just a page stating that they may submit results at any time and that they will be paid bonuses commensurate on the number and quality of questions answered. It can be turned off.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>710</wp:post_id>
		<wp:post_date>2014-03-22 20:59:09</wp:post_date>
		<wp:post_date_gmt>2014-03-22 20:59:09</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>runtime</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Top Level Orderings</title>
		<link>http://blogs.umass.edu/etosch/2014/03/22/top-level-orderings/</link>
		<pubDate>Sat, 22 Mar 2014 03:38:22 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=722</guid>
		<description></description>
		<content:encoded><![CDATA[I just thought I'd post a diagram I made of valid orderings for questions and blocks. We can think about how hierarchical blocks allow the user to impose an ordering on the question and to introduce a guarantee about the distance between two questions. When we permit blocks to be randomizable, we relax the former.

 <a href="http://blogs.umass.edu/etosch/files/2014/03/orderings.png"><img src="http://blogs.umass.edu/etosch/files/2014/03/orderings-271x300.png" alt="orderings" width="271" height="300" class="aligncenter size-medium wp-image-728" /></a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>722</wp:post_id>
		<wp:post_date>2014-03-22 03:38:22</wp:post_date>
		<wp:post_date_gmt>2014-03-22 03:38:22</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>top-level-orderings</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>ANNOUNCEMENT</title>
		<link>http://blogs.umass.edu/etosch/2014/03/22/annoucement/</link>
		<pubDate>Sat, 22 Mar 2014 20:05:01 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=796</guid>
		<description></description>
		<content:encoded><![CDATA[We are now calling "randomizable" blocks "floating" blocks. Kudos to Emery for the increased clarity.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>796</wp:post_id>
		<wp:post_date>2014-03-22 20:05:01</wp:post_date>
		<wp:post_date_gmt>2014-03-22 20:05:01</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>annoucement</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Bugs and their Analyses</title>
		<link>http://blogs.umass.edu/etosch/2014/03/25/bugs-and-their-analyses/</link>
		<pubDate>Tue, 25 Mar 2014 19:21:17 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1044</guid>
		<description></description>
		<content:encoded><![CDATA[[table id=3 /]

<h3 id="correlation">Correlation</h3>
Recall that correlation is typically measured as the degree to which two variables covary. We are generally interested in correlation as a measure of predictive power. Correlation coefficients give the magnitude of a monotone relationship between two variables. Completely random responses for two questions will result in a low correlation coefficient. 

We use two measures of correlation. For ordered questions, we use Spearman's $$\rho$$; this coefficient ranks responses and measures to degree to which the ranked results have a monotone relationship. For unordered questions, we use Cramer's $$V$$. The procedure for Cramer's $$V$$ is based on the $$\chi^2$$ statistic; we take one question and compute the empirical probability for each answer. We then use this estimator to compute the expected values for each of the values in the other question and find the normalized differences between observed and expected values. The sum of these values is the $$\chi^2$$ statistic, which has a known distribution. Cramer's $$V$$ scales the value of the $$\chi^2$$ statistic by the sample size and the minimum degrees of freedom. 

Both of these tests are sensitive to small counts for some categories. We generally do not collect sufficient information to produce meaningful confidence intervals; as a result, we simply flag correlations that may be of interest.

<h3 id="order">Order Bias</h3>
We use the $$\chi^2$$ statistic directly to compute order bias for unordered questions. For any question pair $$q_i, q_j, i\neq j$$, we partition the sample into two sets : $$S_{i&lt;j}$$, the set of questions where $$q_i$$ precedes $$q_j$$ and $$S_{j&lt;i}$$, the set of questions where $$q_i$$ follows $$q_j$$. We assume each set is independent.* We show wolog how to test for bias in $$q_i$$ when $$q_j$$ precedes it:

<ol>
<li>Compute frequencies $$f_{i&lt;j}$$ for the answer options of $$q_i$$ in the set of responses $$S_{i&lt;j}$$. We will use these values to compute the estimator.</li>
<li>Compute frequencies $$f_{j&lt;i}$$ for answer options $$q_i$$ in the set of responses $$S_{j&lt;i}$$. These will be our observations.</li>
<li>Compute the $$\chi^2$$ statistic on the data set. The degrees of freedom will be one less than the number of answer options, squared. If the probability of computing such a number is less than the value at the $$\chi^2$$ distribution with these parameters, there is a significant difference in the ordering</li>
</ol> 

We compute these values for every unique question pair.

<h3 id="wording">Wording</h3>
Wording bias classified in the same was as order bias, except in this case, instead of comparing two sets of responses, we compare $$k$$ sets of responses, corresponding to the number of variants we are interested in. 

<h3 id="breakoff">Breakoff</h3>
We address two kinds of breakoff : ones determined by position, and ones determined by question. For both analyses, we use the nonparametric bootstrap procedure to determine a one-sided 95% confidence intervals and flag indices and questions whose counts exceed the threshhold. 

Breakoff by position is often an indicator that the survey is too long. Breakoff by question may indicate that a question is unclear, offensive, or burdensome to the respondent. There are also some cases where breakoff may indicate a kind of order bias.


<h3 id="adversaries">Adversaries</h3>
We've tried a variety of methods for detecting adversaries. The bests empirical results we've seen so far have been for a method that uses entropy.

We first compute the empirical probabilities for each question's answer options. Then for every response $$r$$, we calculate a score styled after entropy : $$score_{r} = \sum_{i=1}^n p(o_{r,q_i}) * \log_2(p(o_{r,q_i}))$$. We then use the bootstrap method to find a one-sided 95% confidence interval and flag any responses that are above the threshold.


*Independence is based on the assumption that each worker is truly unique and that workers do not collude. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1044</wp:post_id>
		<wp:post_date>2014-03-25 19:21:17</wp:post_date>
		<wp:post_date_gmt>2014-03-25 19:21:17</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>bugs-and-their-analyses</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Pricing, for real this time</title>
		<link>http://blogs.umass.edu/etosch/2014/04/30/pricing-for-real-this-time/</link>
		<pubDate>Wed, 30 Apr 2014 14:53:31 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1104</guid>
		<description></description>
		<content:encoded><![CDATA[Reader be warned : I began this draft several weeks ago, so there might be a lack of coherence...

A few weeks ago I posted <a href="http://blogs.umass.edu/etosch/2014/03/14/the-pricing-problem-in-surveyman/">some musings</a> on pricing. In that post I was mainly concerned with the modeling problem for pricing. Here I'd like to discuss some research questions I've been bandying about with <a href="https://github.com/sckingsley">Sara Kingsley</a>, and outline the experiments we'd like to run. 

<h2>The Problem</h2>
Pricing is intricately tied together with quality control; we use pricing algorithms to help ensure quality control. When we previously outlined our <a href="http://blogs.umass.edu/etosch/2014/03/13/adversaries/">adversaries</a>, we took a traditional approach with the assumption that an actor is either good or bad. There are several key features that this model ignores:

<ol>
<li> Some workers are better at some types of tasks than other types of tasks. </li>
<li> The quality of the task has an impact on the quality of the worker's output. </li>
<li> Some design decisions in services such as AMT sometimes make it difficult to tease out the difference between the above two. </li> 
</ol>

(1) is well known and solved by conditioning the classification on the task. Plenty of work on assessing the quality of AMT workers incorporates task difficulty or task type. Task type discretizes the space, making classification clean and easy. Task difficulty is more difficult to model, since it can be highly subjective. I'm also not sure it's entirely discrete and have not come across a compelling paper on the subject (though I haven't looked thoroughly, so please post any in the comments!). 

(2) seems to be better-known among social scientists than computer scientists. AMT workers have online forums where they post information about HITs. Typically if a HIT is good, they post very little additional information. If a HIT is bad, they will review it.

Poorly designed or unclear HITs incur a high cost for the workers and the requesters. Literature on crowdworkers' behavior suggests that they are aiming for a particular rate. On AMT, a worker can return a HIT at any time. However, if a worker returns a HIT, they will not be compensated for any work whatsoever, and no information about abandonment is returned to the requester. Consequently, as a worker makes their way through a HIT, they must weight the cost of completion against the cost of abandonment. Even workers who are highly skilled at a particular task may perform poorly if a HIT is poorly designed. If workers do not reach out to requesters or if requesters do not search for their own HITs on forums, they may never know that workers are abandoning the work, or if they do, why. 

Quality of the work is clearly tied to quality of the task. In SurveyMan, we address quality of the task in a more principled way than just best practices. It would also stand to reason that quality of the work would be tied to price. One might hypothesize that a higher price for a task would translate to higher quality work. However (according to what Sara's told me), this is not the case. Work quality allegedly does not appear to respond to price. We believe that this result is a direct consequence from the AMT shortcoming detailed above -- prohibiting workers from submitting early enforces a discontinuity in the observed quality/utility function. 

<h2>How to address pricing</h2>
There are two main research questions we would like to address:
<ol>
<li>Does the design of SurveyMan change worker behavior?</li>
<li>Can we find a general function for determining the price/behavior tradeoff, and implement this as part of the SurveyMan runtime system?</li>
</ol>

The impetus for these particular questions was the results we found when running Sara's <a href="http://blogs.umass.edu/etosch/2014/03/19/creating-sophisticated-online-surveys-a-case-study/">wage survey</a>. There were two differences in the deployment of this survey: (1) I did not run this survey with a breakoff notice and (2) this was the first survey launched over a weekend. 

So-called "time of day effects" are a known problem with AMT. Since AMT draws primarily on workers from the US and India, there are spikes in participation during times when these workers are awake and engaged. Many workers perform HITs while employed at another job. It wouldn't be a stretch to claim that sub-populations have activity levels that can be expressed as a function of the day of the week. This could explain some of the behavior we observed with Sara's wage survey. However, the survey ran for almost a week before expiring. I believe that (1) had a strong influence on workers' behavior. 

<h2>Is SurveyMan the Solution?</h2>
Sara had mentioned some work in economics that found that changing the price paid for a HIT on AMT had no impact on the quality of the work. I had read some previous work that discussed the impact of price on attracting workers, but discussed quality control as a function of task design, rather than pricing. I suspect that the observed absence of difference between price points is related to the way the AMT system is designed. 

AMT does not pay for partial work. When a worker accepts a HIT, they can either complete the HIT and submit it for payment (which is not guaranteed), or they return the HIT and receive no payment. Since requester review sites exist, the worker can use the requester's reputation as a proxy for the likelihood that they'll be paid for their work and as a proxy for the quality of the HIT. 

Consider the case where the HIT is designed so that the worker has complete information about the difficulty of the task. In the context of SurveyMan, this would be a survey whose contents are displayed all on the same page. We know that there will be surveys where this approach is simply not feasible - one example that comes to mind is experimental surveys that require measuring the difference in a respondent's responses over two different stimuli. In any case, if the user is able to see the entire survey, they will be able to gauge the amount of effort required to complete the task, and make an informed decision about whether or not to continue with the HIT.

This design has several drawbacks. There's the aforementioned restriction over the types of surveys we can analyze. There's also a problem with our ability to measure breakoff. Since we display one question at a time, in a more or less randomized order, we can tell the difference between questions that cause breakoff and length-related breakoff. When the respondent is allowed to skip around and answer questions in any order, we lose this power. We also lose any inferences we might make about question order, and generally have a more muddied analysis.

Displaying questions one at a time was always part of our design. However, we decided to allow users to submit early as a way of handling this issue with AMT and partial work. Since we couldn't get any information about returned HITs, we decided to discourage users from returning them and instead allow them to submit their work early. Since we figured that we would need to provide users with an incentive to continue answering questions, we displayed a notice at the beginning of a survey that told the user that they would be paid a bonus commensurate with the amount and quality of the work they submitted. We decided against telling the user (a) how the bonus would be calculated and (b) how long the survey would be. 

I initially thought we would court bots by allowing users to submit after answering the first question. This was absolutely not the case for the phonology surveys. Anecdotally it seems that AMT has been cracking down on bots, but I had a really hard time believing that we had no bots. It wasn't until I posed the wage survey that I began to see this behavior. I believe that it is related to the lack of breakoff notice. 

It would be interesting to test some of these hypothesis on a different crowdsourcing platform, especially one that allows tracking for partial work. Even a system that has a different payment system set up would be a good point of comparison.

<h2>Possible Experiments</h2>
We set up the wage survey to run a fully randomized version and a control version at the same time. I really liked this setup, since it meant that any given respondent had a 50/% chance of seeing one of the two, effectively giving us randomized assignments.

<b>Experiment 1</b> To start with, I would like to run another version that randomly displays the breakoff notice on each version. One potentially confounding problem might be the payment of bonuses, since this has been our practice in the past, and may be known to the workers. The purpose of this experiment is to test whether showing the breakoff notice changes the quality of responses.

<b>Experiment 2</b> Another parameter that needs more investigation is the base pay. We recently started using federal minimum wage, an estimated time per question, and the max or average path through the survey (whether to use max or average is still up for debate). I've seen very low base pay, with the promise of bonuses, successfully attract workers. It isn't clear to me how the base pay is related to the number of responses or quality of worker.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1104</wp:post_id>
		<wp:post_date>2014-04-30 10:53:31</wp:post_date>
		<wp:post_date_gmt>2014-04-30 14:53:31</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>pricing-for-real-this-time</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Subblocking Semantics</title>
		<link>http://blogs.umass.edu/etosch/?p=2817</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2817</guid>
		<description></description>
		<content:encoded><![CDATA[$$\forall ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2817</wp:post_id>
		<wp:post_date>2015-06-28 23:46:05</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Differences in Classifiers</title>
		<link>http://blogs.umass.edu/etosch/?p=2841</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2841</guid>
		<description></description>
		<content:encoded><![CDATA[What are the differences between classifiers?Some responses are classified the same way, regardless of classifier, but what do the boundary cases look like?

&nbsp;

&nbsp;]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2841</wp:post_id>
		<wp:post_date>2015-11-19 14:34:18</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Hack the system</title>
		<link>http://blogs.umass.edu/etosch/2014/05/01/hack-the-system/</link>
		<pubDate>Thu, 01 May 2014 18:49:19 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1132</guid>
		<description></description>
		<content:encoded><![CDATA[The Java/Clojure QC of SurveyMan has a RandomRespondent built in. This RandomRespondent class generates answers to the surveys on the basis of some policy. The policies currently available are uniform random, first option, last option, and Gaussian. I've been thinking about some other adversary models I could add to the mix : 

<ol>
	<li>Christmas Tree : This is a variant of the uniform random respondent, where the respondent zigzags down the survey in the form of a "Christmas Tree." <img src="http://www.birdhousestamps.com/wp-content/uploads/2011/06/zigzagxmastree.gif" width="196" height="375" alt="zigzag christmas tree" class="aligncenter" />
</li>
	<li>Memory Bot : This is more like an augmentation to one of the existing policies, where the questions and answers are cached, and for each question, the bot checks whether it has answered something like it before. We know that sometimes researchers repeat questions, or have similarly worded questions with the same answers (e.g. year of birth). The goal of this bot would be to identify similarly worded questions and try to give consistent answers.</li>
	<li>IR Bot : Alternately, we could search through Google for answers and use those answers as solutions.</li>
</ol>

It's fairly trivial to write some Javascript to answer our kind of surveys. Since we now have automated browser testing set up, we should also be able to test collusion in the context of the full pipeline. W00t! ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1132</wp:post_id>
		<wp:post_date>2014-05-01 14:49:19</wp:post_date>
		<wp:post_date_gmt>2014-05-01 18:49:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>hack-the-system</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>On calculating survey entropy</title>
		<link>http://blogs.umass.edu/etosch/2014/05/07/on-calculating-survey-entropy/</link>
		<pubDate>Wed, 07 May 2014 22:11:18 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1168</guid>
		<description></description>
		<content:encoded><![CDATA[I've been spending the past two weeks converting analyses that were implemented in Python and Julia into Clojure. The <a href="http://splashcon.org/2013/cfp/665">OOPSLA Artifact Evaluation</a> deadline is June 1 and moving these into Clojure means that the whole shebang runs on the JVM (and just one jar!). 

One of the changes I really wanted to make to the artifact we submit was a lower upper bound on survey entropy. Upper bounds on entropy can be useful in a variety of ways: in these initial runs we did for the paper, I found them useful for comparing across different surveys. The intuition is that surveys with similar max entropies have similar complexity, similar runtimes, similar costs, and similar tolerance to bad behavior. Furthermore, if the end-user were to use the simulator in a design/debug/test loop, they could use max entropy to guide their survey design.

We've iterated our calculation of the max entropy. Each improvement has lowered the upper bound for some class of surveys. 

<b>Max option cardinality</b> Our first method for calculating maximum entropy of a survey was the one featured in the paper: we find the question with the largest number of options and say that the entropy of the survey must be less than the entropy of a survey having equal number of questions, where every question has this maximum number of answer options. Each option has equal probability of being chosen. For some $$survey$$ having $$n$$ questions, the maximum entropy would then be $$\lceil n \log_2 (\max ( \lbrace \lvert \lbrace o : o \in options(q) \rbrace \rvert : q \in questions(survey) \rbrace ) ) \rceil$$.

The above gives a fairly tight bound on surveys such as the <a href="http://blogs.umass.edu/etosch/2014/03/25/experiment-report-i-breakoff-bot-detection-and-correlation-analysis-for-flat-likert-scale-surveys/">phonology survey</a>. For surveys that have more variance in the number of options proffered to the respondent, it would be better to have a tighter bound. 

<b>Total survey question max entropy</b> We've had a calculation for total survey question max entropy implemented in Clojure for a few weeks now. For any question having at least one answer option, we calculate the entropy of that question, and sum up all those bits. For some $$survey$$ having $$n$$ questions, where each question $$q_i$$ has $$m_i$$ options, the maximum entropy would then be $$\lceil \sum_{i=1}^n \mathbf{1}_{\mathbb{N}^+}(m_i)\log_2(m_i)\rceil$$

While the total survey question max entropy gives a tighter bound on surveys with higher variance, it is still a bit too high for surveys with branching. Consider the wage survey. In Sara's initial formulation of the survey (i.e. not the one we ran), the question with the greatest number of answer options was one asking for the respondent's date of birth. The responses were dates ranging from 1900 to 1996. Most of the remaining questions have about 4 options each:
[table id=4 /]
 Clearly in this case, using max option cardinality would not give much information about the entropy of the survey. The max cardinality maximum entropy calculation gives 258 bits, whereas the total survey question max entropy gives 80 bits.

This lower upper bound still has shortcomings, though -- it doesn't consider surveys with branching. For many surveys, branching is used to ask one additional question, to help refine answers. For these surveys, many respondents answer every question in the survey. However, there are some survey that are designed so that no respondent answers every question in the survey. Branching may be used to re-route respondents along a particular path. We used branching in this way when we actually deployed <a href="http://blogs.umass.edu/etosch/2014/03/19/creating-sophisticated-online-surveys-a-case-study/">Sara's wage survey</a>. The translated version of Sara's survey has two 39-question paths, with a 2-option branch question to start the survey and zero option instructional question to end the survey. This version of the survey has a max cardinality maximum entropy calculation of $$80 * \log_2 97 = 528$$ and a total survey question max entropy of 160 bits (without the ceiling operator, this is approximately equal two two times the entropy of the previous version, plus one bit for the introductory branch question).

The maximum number of bits needed to represent this survey approximately doubled from one version to the next. This isn't quite right -- we know that the maximum path through the survey is 41 questions, not 80. In this case, branching makes a significant difference in the lower bound.

<b>Max path maximum entropy</b> Let's instead compute the maximum entropy over distinct paths through the survey. We've previously discussed <a href="http://blogs.umass.edu/etosch/2014/03/21/regarding-survey-paths/">the computational complexity of computing distinct paths through surveys</a>. In short, randomization significantly increases the number of possible paths through the survey; if we focus on path through blocks instead, we have more tractable results. Rather than thinking about paths through the survey as distinct lists of questions, where equivalent paths have equivalent lengths and orderings, we can instead think about them as unique sets of questions. This perspective aligns nicely with the invariants we preserve.

Our new maximum entropy calculation will compute the entropy over unique sets of questions and select the maximum entropy computed over this set. Some questions to consider are:

<ol>
	<li>Are joined paths the same path?</li>
	<li>If we are computing empirical entropy, should we also consider breakoff? That is, do we need the probability of answering a particular question?</li>
</ol>

We consider paths that join distinct from each other; the probability of answering that question will sum up to one, if we don't consider breakoff. As for breakoff, for now let's ignore it. If we need to compute the empirical entropy over the survey (as opposed to the maximum entropy), then we will use the subset relation to determine which questions belong to which path. That is, if we have a survey with paths $$q_1 \rightarrow q_2 \rightarrow q_4$$ and $$q_1 \rightarrow q_3 \rightarrow q_4$$, then a survey response with only $$q_1$$ answered will be used to compute the path frequencies and answer option frequencies for both paths. The maximum entropy is then computed as $$\lceil max(\lbrace -\sum_{q\in survey} \sum_{o \in ans(q)} \mathbb{P}(o \cap p) \log_2 \mathbb{P}(o \cap p) : p \in paths \rbrace) \rceil$$.

There are two pieces of information we need to calculate before actually computing the maximum entropy path. First, we need the set of paths. Since paths are unique over blocks, we can define a function to return the set of blocks over <a href="https://gist.github.com/etosch/a3d8f3329251152f64fd">the paths</a>. The key insight here is that for blocks that have the <small>NONE</small> or <small>ONE</small> branch paradigm, every question in that block is answered. For the branch <small>ALL</small> paradigm, every question is supposed to be "the same," so they will all have the same number of answer options. Furthermore, since the ordering of floating (randomizable) top level blocks doesn't matter, and since we prohibit branching from or to these blocks, we can compute the DAG on the totally ordered blocks and then just concatenate the floating blocks onto the unique paths through those ordered blocks.

The second thing we need to compute is $$\mathbb{P}(o \cap p)$$. The easiest way to do this is to take a survey response and determine which unique path(s) it belongs to. If we count the number of times we see option $$o$$ on path $$p$$, the probability we're estimating is $$\mathbb{P}(o | p)$$. We can compute $$\mathbb{P}(o \cap p)$$ from $$\mathbb{P}(o | p)$$ by noting that $$\mathbb{P}(o \cap p) = \mathbb{P}(o | p)\mathbb{P}(p)$$. This quantity is computed by $$\frac{\# \text{ of } o \text{ on path } p}{\#\text{ of responses on path } p}\times\frac{\#\text{ of responses on path } p}{\text{total responses}}$$, which we can reduce to $$\frac{\# \text{ of } o \text{ on path } p}{\text{total responses}}$$. It should be clear from this derivation that even if two paths join, the entropy for the joined sub path is equal to the case where we treat paths separately.

The maximum entropy for the max path in the wage survey, computed using the current implementation of SurveyMan's static analyses, is 81 bits -- equivalent to the original version of the survey, plus one extra bit for the branching.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1168</wp:post_id>
		<wp:post_date>2014-05-07 18:11:18</wp:post_date>
		<wp:post_date_gmt>2014-05-07 22:11:18</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-calculating-survey-entropy</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Get money, get paid.</title>
		<link>http://blogs.umass.edu/etosch/2014/05/16/get-money-get-paid/</link>
		<pubDate>Sat, 17 May 2014 02:56:45 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1254</guid>
		<description></description>
		<content:encoded><![CDATA[When we allow breakoff, we typically tell respondents that they will be paid a bonus commensurate with the amount and quality of work they do at the end of the study. We don't typically inform them of when the study will end, since we may need to re-run surveys and we'd like to keep respondents from trying to game our algorithms. The experiments we did in the fall that awarded bonuses used a tiered system. The ones listed here are using respondent's scores to identify the top 95% of scores and award those respondents two cents per question. I'd like to investigate more sophisticated pricing schemata with <a href="http://sarackingsley.net/">Sara</a> in the future.

<h3>Wage Survey</h3>
It was my intention to only have one wage survey running on AMT. However, as I've been porting the Python analyses into Clojure, I found that I actually had three instances running. Given the expiration dates on the latter two, I'm pretty sure they were posted accidentally. I should probably consider asking the user if their sure they want to continue, or have a safe mode that asks a million times "are you sure you want to do X?" so future users don't make this mistake. There's also small possibility that when I extended the original HIT, it somehow spawned two new HITs instead. This isn't documented anywhere, but it's something I probably want to double check on sandbox.

So we had three surveys running. At the time of our OOPSLA submission, we had the wage survey running for about four days and only accrued 69 responses. I extended that HIT twice. It expired Mar 26 2014, 04:42 PM PDT. The other two HITs had expiration dates of Mar 28 2014, 04:44 PM PDT and Mar 30 2014, 07:22 PM PDT. Each HIT requested 150 assignments, and paid $0.10 base wage per survey. Between the three surveys, we collected 154 responses. Under normal circumstances, I wouldn't have three of the same HIT running concurrently -- a feature I might consider adding to SurveyMan is a check for whether a survey with similar parameters has been posted before. If I implement a "safe mode" version of SurveyMan, I could ask then ask the user if they really want to post this survey. 

Anyway, the point is that because I had three versions running, I had repeaters. We had only 132 unique respondents. I typically exclude repeaters from the analyses, since we tell them to return the surveys if they haven't taken them before. After running our new dynamic analyses report, I found that 98 respondents were classified as bad actors. I had a similarly high percentage in the Python analysis and wasn't confident that it was correct. Since we hadn't tested the effects of breakoff on bot classification in simulation, I was hesitant to make any strong assertions about these classifications, without investigating further. Furthermore, since we had so few data points at the time of the OOPSLA submission, we decided to simply report qualitative results. 

Examining the larger data set, we found that the maximum number of questions answered by any one respondent was still 26. I'll leave a more thorough analysis of our quality control to another blog post, but the results were quite interesting and corroborated some of my suspicions about the the original set of data. In any case, the number of questions bad actors answered had a high of 18 and a low of 2 (interestingly, those who only answered one question were confined to repeaters. Inspecting manually, I saw 10 repeaters in total, of whom only one appeared to be a legitimate bad actor.). 

Using the federal minimum wage of $7.25 an hour and an estimated 10 seconds per question, we should award $0.02 per question. Since we already awarded a base pay of $0.10, we subtracted this quantity off the total payment calculated for honest respondents. The static analyses gave us an average path length of 41 questions and an expected payment of $0.825694 for each respondent who answered the survey to completion. Since we requested 150 responses, if every respondent were categorized as honest and answered the survey to completion, it would cost us $123.8541, not counting the AMT commission. AMT charges a 10% commission on both the base pay and the bonuses. This gives us a total expected cost of $136.23951. 

Our actual costs were much lower (although the quality of the data was presumably also lower). Our 154 respondents cost us $22.53 in base pay, including AMT commission. We calculated $7.78 in bonuses to be awarded, costing us $8.558 for the commission. In total, the experiment under these conditions cost us $31.088.

<h3>Prototypicality</h3>
The next survey we'll look at is <a href="http://people.umass.edu/ppizzo/">Presley's</a> prototypicality survey. We had 149 responses total. The survey had an average path length of 17 questions. The estimated base price for honest respondents who answer the survey in full is $0.342361 per survey. Our expected cost for all honest respondents, plus AMT commission is $56.489565. We classified 65 respondents as bad actors, and 84 as valid responses. These results differ from our initial reported results due to how we calculate the frequencies for questions that are variants. In our Python code, we only compared questions that were exactly the same -- that is, we didn't unify the distributions of the variants. In the Clojure implementation, we first remove bots, and assume that there is no statistical difference between the questions, unifying all the variants. We'll leave a discussion of the pros and cons of this approach to a future blog post.

The result of unifying variants is that significantly more responses are classified as bots, but none of the variants are flagged as being drawn from different distributions! I'll have to double check to make sure that the variant code is running as expected, but since my unit test for flagging variants seems to work, I'm going to assume that the differences we detected were due to outliers. We can discuss what might be going on here in a later blog post, since this one is mostly about pricing. The bonuses to be paid amount to $20.32. With AMT commission this is $22.35. In total, this survey costs $38.742.

<h3>Choose Randomly</h3>
I ran a survey that wasn't featured in our OOPSLA paper because it was one I made up entirely. The idea was to post a survey with two floating blocks, where both asked respondents to choose one of the responses randomly, but the one block had identical options, whereas the other had arbitrary categories of things. I wanted to see how random people could actually be. I also wanted to track timing information. One of the things I noticed about this survey, which I posted on a Wednesday morning, was that I was able to collect all of my responses within 90 minutes. The time per question here is clearly less than 10s. If it takes about 2 seconds per question, then the expected cost for a completed survey would be about $0.08. As a result, I decided to not award bonuses. The total cost was $16.50.

<h3>Phonology</h3>
Finally we have our classic phonology survey. With an average path length of 99 questions, we would expect to pay $1.993750 per survey. With a target of 150 responses, our total cost, including commission, would be $328.96875. 

We ran this survey 3 times (not counting our preliminary run). The details of each run can be found in <a href="http://blogs.umass.edu/etosch/2014/03/25/experiment-report-i-breakoff-bot-detection-and-correlation-analysis-for-flat-likert-scale-surveys/">an earlier post</a>. We collected 395 responses total and had 311 unique respondents. 22 respondents accounted for the 84 duplicate responses. This initial run cost us $43.45. 182 responses were classified as valid responses. The total bonuses to be paid were calculated to be $327.96. Factoring in AMT commission, this comes to $360.756. In all, this survey will have cost us $404.206.

<h3>Notes on timing</h3>
Some of the the timing data we returned was flawed, so we couldn't use that information to improve our payment scheme. One possible use of this information would be to use it as a proxy for the difficulty the question and vary the payment in accordance with this information. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1254</wp:post_id>
		<wp:post_date>2014-05-16 22:56:45</wp:post_date>
		<wp:post_date_gmt>2014-05-17 02:56:45</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>get-money-get-paid</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Reading Rainbow</title>
		<link>http://blogs.umass.edu/etosch/2014/05/17/reading-rainbow/</link>
		<pubDate>Sat, 17 May 2014 14:35:09 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1290</guid>
		<description></description>
		<content:encoded><![CDATA[It's only a few short weeks until <a href="http://conferences.inf.ed.ac.uk/pldi2014/">PLDI 2014</a>. Oh, the tedious and expensive travel! Just kidding (well, not really -- it will involve quite a few trains and many, many dollars). 

Inspired by <a href="http://www.ic.unicamp.br/~tachard/">Alex Passos</a>'s yearly <a href="http://atpassos.me/">NIPS reading list</a>, I'm going throw together one of my own. Rather than listing abstracts, I'm going to just post an ordered list of the papers I'm going to read and post on individual papers as I see fit.

<h3>Tier 1 : Authors I know</h3>
Unless the conference is massively multi-tracked, I find having to ask if someone I've actually met and spoken with IRL if they have a paper at the conference a bit tactless. This isn't to say I haven't done it, or that I've done so in a completely shameless way. I do however recognize that refraining from such behavior is A Good Thing.
 

<ol>
	<li><b>Doppio: Breaking the Browser Language Barrier </b>
John Vilk, University of Massachusetts, Amherst; Emery Berger, University of Massachusetts, Amherst.</li>

	<li><b>Expressing and Verifying Probabilistic Assertions</b> 
Adrian Sampson, University of Washington; Pavel Panchekha, University of Washington; Todd Mytkowicz, Microsoft Research; Kathryn S McKinley, Microsoft Research; Dan Grossman, University of Washington; Luis Ceze, University of Washington.</li>

	<li><b>Resugaring: Lifting Evaluation Sequences through Syntactic Sugar </b>
Justin Pombrio, Brown University; Shriram Krishnamurthi, Brown University.</li>

	<li><b>Taming the Parallel Effect Zoo: Extensible Deterministic Parallelism with Lvish</b> 
Lindsey Kuper, Indiana University; Aaron Todd, Indiana University; Sam Tobin-Hochstadt, Indiana University; Ryan R. Newton, Indiana University.</li>

	<li><b>Introspective Analysis: Context-Sensitivity, Across the Board </b>
Yannis Smaragdakis, University of Athens; George Kastrinis, University of Athens; George Balatsouras, University of Athens.</li>

	<li><b>Dynamic Space Limits for Haskell </b>
Edward Z. Yang, Stanford University; David Mazires, Stanford University.</li>


</ol>

<h3>Tier 2 : Authors my advisor knows</h3>
It's a reasonable assumption that my advisor probably knows at least one author on each paper, so we can also call this category "Authors whom I might reasonably expect to be introduced to by My Advisor." These papers include authors whose work I've read before and whose names I know from discussions with my advisor. Reading these papers will help prevent the awkward standing-there thing that happens when someone who is much more comfortable than you are (er, than I am) is deep in a conversation and you (I) have nothing to add. It'll also provide a hook that's socially more acceptable to whatever random thought happens to be passing through your (my) head. Genius, this plan is!

<ol>

	<li><b>Fast: a Transducer-Based Language for Tree Manipulation </b>
Loris D'Antoni, University of Pennsylvania; Margus Veanes, Microsoft Research; Benjamin Livshits, Microsoft Research; David Molnar, Microsoft Research.</li>

	<li><b>Automatic Runtime Error Repair and Containment via Recovery Shepherding </b>
Fan Long, MIT CSAIL; Stelios Sidiroglou-Douskos, MIT CSAIL; Martin Rinard, MIT CSAIL.</li>

		<li><b>Adapton: Composable, Demand-Driven Incremental Computation </b>
Matthew A. Hammer, University of Maryland, College Park; Yit Phang Khoo, University of Maryland, College Park; Michael Hicks, University of Maryland, College Park; Jeffrey S. Foster, University of Maryland, College Park.</li>


	<li><b>FlashExtract : A Framework for Data Extraction by Examples </b>
Vu Le, UC Davis; Sumit Gulwani, Microsoft Research Redmond.</li>

	<li><b>Test-Driven Synthesis </b>
Daniel Perelman, University of Washington; Sumit Gulwani, Microsoft Research Redmond; Dan Grossman, University of Washington; Peter Provost, Microsoft Corporation.</li>

	<li><b>Consolidation of Queries with User Defined Functions </b>
Marcelo Sousa, University of Oxford; Isil Dillig, Microsoft Research; Dimitrios Vytiniotis, Microsoft Research; Thomas Dillig, UCL; Christos Gkantsidis, Microsoft Research.</li>


	<li><b>Atomicity Refinement for Verified Compilation </b>
Suresh Jagannathan, Purdue University; Vincent Laporte, INRIA Rennes; Gustavo Petri, Purdue University; David Pichardie, INRIA Rennes; Jan Vitek, Purdue University.</li>

</ol>

<h3>Tier 3 : The Competition</h3>
The Student Research Competition, that is. Some of those presenting at SRC are also presenting work at the main event. Since we'll presumably have some forced socialization, it's probably a good call to get an idea of what some of their other work is about.
<ol>
	<li><b>A Theory of Changes for Higher-Order Languages - Incrementalizing Lambda-Calculi by Static Differentiation </b>
Yufei Cai, Philipps-Universitt Marburg; Paolo G. Giarrusso, Philipps-Universitt Marburg; Tillmann Rendel, Philipps-Universitt Marburg; Klaus Ostermann, Philipps-Universitt Marburg.</li>

	<li><b>Commutativity Race Detection </b>
Dimitar Dimitrov, ETH Zurich; Veselin Raychev, ETH Zurich; Martin Vechev, ETH Zurich; Eric Koskinen, New York University.</li>

	<li><b>Code Completion with Statistical Language Models </b>
Veselin Raychev, ETH Zurich; Martin Vechev, ETH Zurich; Eran Yahav, Technion.</li>

	<li><b>Verification Modulo Versions: Towards Usable Verification </b>
Francesco Logozzo, Microsoft Research; Manuel Fahndrich, Microsoft Research; Shuvendu Lahiri, Microsoft Research; Sam Blackshear, University of Colorado at Boulder.</li>

	<li><b>Adaptive, Efficient Parallel Execution of Parallel Programs </b>
Srinath Sridharan, University of Wisconsin-Madison; Gagan Gupta, University of Wisconsin-Madison; Gurindar Sohi, University of Wisconsin-Madison.</li>


<li><b>Globally Precise-restartable Execution of Parallel Programs </b>
Gagan Gupta, University of Wisconsin-Madison; Srinath Sridharan, University of Wisconsin-Madison; Gurindar S. Sohi, University of Wisconsin-Madison.</li>
</ol>

There are 13 participants in the SRC total. Five are presenting at the conference proper (One is on a paper in another tier).

<h3>Tier 4 : Pure Interest</h3>
No motivation, except that the papers look interesting.
<ol>

	<li><b>Improving JavaScript Performance by Deconstructing the Type System </b>
Wonsun Ahn, University of Illinois at Urbana Champaign; Jiho Choi, University of Illinois at Urbana Champaign; Thomas Shull, University of Illinois at Urbana Champaign; Maria Garzaran, University of Illinois at Urbana Champaign; Josep Torrellas, University of Illinois at Urbana Champaign.
</li>

	<li><b>Automating Formal Proofs for Reactive Systems </b>
Daniel Ricketts, UC San Diego; Valentin Robert, UC San Diego; Dongseok Jang, UC San Diego; Zachary Tatlock, University of Washington; Sorin Lerner, UC San Diego.</li>


	<li><b>Tracelet-Based Code Search in Executables </b>
Yaniv David, Technion; Eran Yahav, Technion.</li>

	<li><b>Getting F-Bounded Polymorphism into Shape </b>
Benjamin Lee Greenman, Cornell University; Fabian Muehlboeck, Cornell University; Ross Tate, Cornell University.</li>

	<li><b>Compositional Solution Space Quantification for Probabilistic Software Analysis </b>
Mateus Borges, Federal University of Pernambuco; Antonio Filieri, University of Stuttgart; Marcelo D'Amorim, Federal University of Pernambuco; Corina S. Pasareanu, Carnegie Mellon Silicon Valley, NASA Ames; Willem Visser, Stellenbosch University.</li>

	<li><b>Test-Driven Repair of Data Races in Structured Parallel Programs </b>
Rishi Surendran, Rice University; Raghavan Raman, Oracle Labs; Swarat Chaudhuri, Rice University; John Mellor-Crummey, Rice University; Vivek Sarkar, Rice University.</li>

	<li><b>VeriCon: Towards Verifying Controller Programs in Software-Defined Networks </b>
Thomas Ball, Microsoft Research; Nikolaj Bjorner, Microsoft Research; Aaron Gember, University of Wisconsin-Madison; Shachar Itzhaky, Tel Aviv University; Aleksandr Karbyshev, Technical University of Munich; Mooly Sagiv, Tel Aviv University; Michael Schapira, Hebrew University of Jerusalem; Asaf Valadarsky, Hebrew University of Jerusalem.</li>

<li><b>AEminium: A permission based concurrent-by-default programming language approach</b> 
Sven Stork, Carnegie Mellon University; Karl Naden, Carnegie Mellon University; Joshua Sunshine, Carnegie Mellon University; Manuel Mohr, Karlsruhe Institute of Technology; Alcides Fonseca, University of Coimbra; Paulo Marques, University of Coimbra; Jonathan Aldrich, Carnegie Mellon University</li>
	<li><b>First-class Runtime Generation of High-performance Types using Exotypes</b> 
Zachary DeVito, Stanford University; Daniel Ritchie, Stanford University; Matt Fisher, Stanford University; Alex Aiken, Stanford University; Pat Hanrahan, Stanford University.</li>

</ol>
<h4>Why is pure interest ranked last?</h4>
People say that a talk should be an advertisement for the paper. If I don't get through the papers in tier 4 before PLDI, I'll at least know which talks I want to go to and perhaps prune that list accordingly. Since a conference is actually a social event, it seems like a better use of time to target papers that I would expect to come up in conversation. I haven't tried this tactic before, so we'll see how things go!

Finally, I'd like to thank the NSF, the ACM, and President Obama for help on my upcoming travel.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1290</wp:post_id>
		<wp:post_date>2014-05-17 10:35:09</wp:post_date>
		<wp:post_date_gmt>2014-05-17 14:35:09</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>reading-rainbow</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="pldi2014"><![CDATA[#pldi2014]]></category>
		<category domain="category" nicename="conferences"><![CDATA[Conferences]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Celebrities : they&#039;re just like us! </title>
		<link>http://blogs.umass.edu/etosch/2014/06/16/celebrities-theyre-just-like-us/</link>
		<pubDate>Mon, 16 Jun 2014 04:21:18 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1376</guid>
		<description></description>
		<content:encoded><![CDATA[Hopefully this post will be short and sweet, since I'm trying to get back on EST. 

Highlights from this year's PLDI:
<ul>
<li><b>Year of PLASMA!</b> <a href="http://people.cs.umass.edu/~jvilk/">John Vilk's</a>  <a href="http://doppiojvm.org/">DoppioJVM</a> won Best Artifact. His talk was great, and he event got a mid-talk round of applause for a meta-circular evaluator joke. Nothing like Scheme to whet the appetites of PL nerds! (I admit it, I clapped and laughed, too.)</li>
<li><b>Year of PLASMA!</b> In a surprising turn of events, my work on <a href="http://surveyman.org">SurveyMan</a> won top prize in the Graduate category of the <a href="http://src.acm.org/">ACM Student Research Competition</a>. This means I'll submit a short paper in a few months to compete in the Grand Finals! Exciting!</li>
<li>The <a href="http://approx2014.cs.umass.edu/">APPROX</a> was awesome! It was very exciting to see current work presented across approximate computing and probabilistic programming. Emery was chair of the event. Given the amount of discussion it engendered, I would say it was a resounding success.</li>
<li>I met a bunch of new people, and connected with those I haven't seen in a while. Shoutouts to <a href="http://homes.cs.washington.edu/~asampson/">Adrian Sampson</a> and <a href="http://people.csail.mit.edu/mcarbin/">Michael Carbin</a>. I'll be following <a href="http://homes.cs.washington.edu/~asampson/blog/">Adrian's blog</a> now, and pestering Michael about formalizing the SurveyMan semantics (using his work on <a href="http://people.csail.mit.edu/rinard/paper/pldi12.relaxed.pdf">reasoning about relaxed programs</a> as a guide).</li>
<li>A cheeky dig at the New York Times lead to <a href="http://homepages.inf.ed.ac.uk/wadler/">Phil Wadler</a> telling me that I had the best teaser! Famous professors : they're just like us!</li>
<li><a href="http://cs.brown.edu/~sk/">Shriram Krishnamurthi</a> declared he'd read this blog.</li>
</ul>

In other news, I need to try uploading my VM for the OOPSLA artifact evaluation, now that I have reasonable internet again. But first, I need to sleep (though I did set aside time to watch GoT -- OMG, the ending was awesome! Arya's face! That exchange! WTF just happened?!?!? Also, shit's finally starting to get real, north of the wall! You know nothing, Jon Snow...)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1376</wp:post_id>
		<wp:post_date>2014-06-16 00:21:18</wp:post_date>
		<wp:post_date_gmt>2014-06-16 04:21:18</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>celebrities-theyre-just-like-us</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="pldi2014"><![CDATA[#pldi2014]]></category>
		<category domain="category" nicename="conferences"><![CDATA[Conferences]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<category domain="category" nicename="surveyman"><![CDATA[Television]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>36</wp:comment_id>
			<wp:comment_author><![CDATA[Adrian]]></wp:comment_author>
			<wp:comment_author_email>asampson@cs.washington.edu</wp:comment_author_email>
			<wp:comment_author_url>http://homes.cs.washington.edu/~asampson/</wp:comment_author_url>
			<wp:comment_author_IP>64.134.172.79</wp:comment_author_IP>
			<wp:comment_date>2014-06-16 14:41:38</wp:comment_date>
			<wp:comment_date_gmt>2014-06-16 18:41:38</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi! I totally subscribed to this blog too after you plugged it in the AEC session. But it looks like I'll have to stay on my toes to avoid Game of Thrones spoilers... :)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[true]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1402944098.0984599590301513671875;s:7:"message";s:35:"Akismet caught this comment as spam";s:5:"event";s:10:"check-spam";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1403049908.69441890716552734375;s:7:"message";s:40:"etosch reported this comment as not spam";s:5:"event";s:10:"report-ham";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_user_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_user</wp:meta_key>
				<wp:meta_value><![CDATA[etosch]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1403054120.8458690643310546875;s:7:"message";s:45:"etosch changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>38</wp:comment_id>
			<wp:comment_author><![CDATA[Emma Tosch]]></wp:comment_author>
			<wp:comment_author_email>etosch@cns.umass.edu</wp:comment_author_email>
			<wp:comment_author_url>https://cs.umass.edu/~etosch</wp:comment_author_url>
			<wp:comment_author_IP>128.119.40.193</wp:comment_author_IP>
			<wp:comment_date>2014-06-19 16:47:44</wp:comment_date>
			<wp:comment_date_gmt>2014-06-19 20:47:44</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This was my first television mention; next time I'll make sure to issue a warning. :)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>36</wp:comment_parent>
			<wp:comment_user_id>20775</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1403210864.320703983306884765625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>What does it mean for a survey to be &quot;correct&quot;? -- Part I.</title>
		<link>http://blogs.umass.edu/etosch/2014/06/19/what-does-it-mean-for-a-survey-to-be-correct-a-first-stab-at-formalizing-surveyman/</link>
		<pubDate>Thu, 19 Jun 2014 19:05:06 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1392</guid>
		<description></description>
		<content:encoded><![CDATA[One of the arguments we keep making about SurveyMan is that our approach allows us to debug surveys. We talk about the specific biases (order, wording, sampling) that skew the results, but one thing that we don't really emphasize in the tech report is what it really means for a survey to be correct. Here I'd like to tease out various notions of correctness in the context of survey design.

<h3>Response Set Invariant</h3>
Over the past few months, I've been explaining the central concept of correctness as one where we're trying to preserve an invariant: the set of answers returned by a particular respondent. The idea is that if this universe were to split and you were not permitted to break off, the you in Universe A would return the same set of answers as the you in Universe B. That is, your set of answers should be invariant with respect to ordering, branching, and sampling. 

<blockquote>
<img src="http://starsmedia.ign.com/stars/image/article/837/837864/capt-kirk_yellowCU-001_1196284873.jpg" width="50%" height="50%" align="left" class /><img src="http://cmsimg.freep.com/apps/pbcsi.dll/bilde?Site=C4&amp;Date=20130203&amp;Category=ENT01&amp;ArtNo=130203012&amp;Ref=AR&amp;Border=0" width="50%" height="50%" align="right" class />
These guys should give the same responses to a survey, up to the whole timeline split thing.
</blockquote>

<b>Invalidation via biases</b> Ordering and sampling can be flawed and when they are, they lead to our order bias and wording bias bugs. Since we randomize order, we want to be able to say that your answer to some question is invariant with respect to the questions that precede it. Since we sample possible wordings for variants, we want to be able to say that each question in an <code>ALL</code> block is essentially "the same." We denote "sameness" by treating each question wording variant as exchangeable and aligning the answer options. The prototypicality survey best illustrates variants, both in the question wording and the option wording:

[table id=5 /]

<b>Invalidation via runtime implementation</b> Clearly ordering and sampling are typical, well-known biases that we are framing as survey bugs. These bugs violate our invariant. We have also put restrictions on the survey language in order to preserve this invariant. I <a href="http://blogs.umass.edu/etosch/2014/03/21/regarding-survey-paths/">discussed</a> these restrictions <a href="http://blogs.umass.edu/etosch/2014/03/19/static-analysis/">previously</a>, but didn't delve into their relationship with correctness. In order to understand the correctness of a survey program, we will have to talk a little about semantics.

Generally when we talk about language design in the PL community, we have two sets of rules for understanding a language: the syntax and the semantics. As with natural language, the syntax describes the rules for how components may be combined, whereas the semantics describes what they "really mean". The more information we have encoded in the syntax, the more we can check at compile time -- that is, before actually executing the program. At its loosest, the syntax describes a valid surface string in the language, whereas the semantics describes the results of an execution. 

When we want "safer" languages, we generally try to encode more and more information in the syntax. A common example of this is type annotation. We can think of this extra information baked into the language's syntax as some additional structure to the program. Taking the example of types, if we say some variable <code>foo</code> has type <code>string</code> and then try to use it in a function that takes type <code>number</code>, we should fail before we actually load data into <code>foo</code> -- typically before we begin executing the program.

In SurveyMan, these rules are encoded in the permitted values of the boolean columns, the syntax of the block notation, and the permitted branch destinations. Let's first look at the syntax rules for SurveyMan.

<h6><a href="#syntax">SurveyMan Syntax : Preliminaries</a></h6>
<br />
Before we define the syntax formally, let us define two functions we'll need to express our relaxed orderings over questions, blocks, and column headers:

$$ \sigma_n : \forall (n : int), \text{ list } A \times n \rightarrow \text{ list } A \times n$$

This is our random permutation. Actually, it isn't *really* a random permutation, because we are not using the randomness as a resource. Instead, we will be using it more like a nondeterministic permutation (hence the subscript "n"). We will eventually want to say something along the lines of "this operator returns the appropriate permutation to match your input, so long as your input is a member of the set of valid outputs." $$\sigma_n$$ takes a list and its length as arguments and returns another list of the same length. We can do some hand-waving and say that it's equivalent to just having a variable length argument whose result has the same number of arguments, so that

$$ \sigma_n([\text{apples ; oranges ; pears}], 3) \equiv \sigma_n(\text{apples, oranges, pears})$$

and the set of all valid outputs for this function would be

$$ \lbrace \text{[apples ; oranges ; pears], [apples ; pears ; oranges], [oranges ; pears ; apples],}$$ 
$$\quad \text{[oranges ; apples ; pears], [pears ; apples ; oranges], [pears ; oranges ; apples]} \rbrace$$.

We are going to abuse notation a little bit more and say that in our context, the output is actually a string where each element is separated by a newline. So, if we have for example 

$$\sigma_n(\text{apples, oranges, pears}) \mapsto [\text{oranges ; pears ; apples}]$$, 

then we can rewrite this as 

$$\sigma_n(\text{apples, oranges, pears}) \mapsto \text{oranges}\langle newline \rangle \text{pears}\langle newline \rangle \text{apples}$$, 

where $$\langle newline \rangle$$ is the appropriate newline symbol for the program/operating system.

Why not encode this directly in the typing of $$\sigma_n$$? The initial definition makes the syntax confined and well-understood for the (PL-oriented) reader. Since we're defining a specific kind of permutation operator, we want its type to be something easy to reason about. The "true" syntax is just some sugar, so we have some easy-to-understand rewrite rules to operate over, rather than having to define the function as taking variable arguments and returning a string whose value is restricted to some type. Note that I haven't formalized anything like this before, so there might be a "right way" to express this kind of permutation operator in a syntax rule. If someone's formalized spreadsheets or databases, I'm sure the answer lies in that literature.

The above permutation operator handles the case where we have a permutation where we can use some kind of equality operator to match up inputs and outputs that are actually equal. Consider this example: suppose there are two rules in our syntax:

$$\langle grocery\_list \rangle ::= \langle fruit\rangle \mid \sigma_n(\langle fruit\rangle,...,\langle fruit\rangle)$$ 
$$\langle fruit \rangle ::= \text{ apple } \mid \text{ orange } \mid \text{ pear } \mid \text{ banana } \mid \text{ grape }$$

Suppose store our grocery list in a spreadsheet, which we export as a simple csv:
<code>
apple
banana
orange
</code>
Then we just want to be able to match this with the $$grocery\_list$$ rule. 

Next we need to introduce a projection function: a function to select one element from an ordered, fixed-sized list (i.e. a tuple). This will actually be a family of functions, parameterized by the natural numbers. The traditional projection function selects elements from a tuple and is denoted by $$\pi$$. If we have some tuple $$(3,2,5)$$, then we define a family of projection functions $$\pi_1$$, $$\pi_2$$, and $$\pi_3$$, where $$\pi_1((3,2,5))=3$$, $$\pi_2((3,2,5))=2$$, and $$\pi_3((3,2,5))=5$$. Our projection function will have  to operator over list, rather than a tuple, since it will be used to extract values from columns and since we will not know the total number of columns until we read the first line of the input. The indices correspond to the column indices, but since columns may appear in any order, we must use the traditional permutation operator to denote the correct index:

$$\pi_{\sigma(\langle column \rangle)}(\langle col\_data \rangle_{(\sigma^{-1}(1))}, \langle col\_data \rangle_{(\sigma^{-1}(2))},...,\langle col\_data \rangle_{(\sigma^{-1}(n))})$$

Now we have some set of projection functions, $$\pi_{\sigma(\langle column \rangle)}$$, the size and values of which we do not know until we try to parse the first row of the csv. However, once we parse this row, the number of columns ($$n$$ above) and their values are fixed. Furthermore, we will also have to define a list of rules for the valid data in each column, but for column headers recognized by the SurveyMan parser, these are known ahead of time. All other columns are treated as string-valued. We parameterize the column data rules, using the subscripts to find the appropriate column name/rule for parsing.


<h6><a href="#syntax">SurveyMan Syntax : Proper</a></h6>
<br />

$$\langle survey \rangle ::= \langle block \rangle \mid \sigma_n(\langle block \rangle, ...,\langle block \rangle)$$

$$\langle block \rangle ::= \langle question \rangle \mid \sigma_n(\langle block \rangle, ..., \langle block \rangle)$$ 
$$\quad\quad\mid \; \sigma_n(\langle question \rangle,...,\langle question \rangle) \mid \sigma_n(\langle question \rangle, ... ,\langle block \rangle,...)$$

$$\langle row \rangle ::= \langle col\_data \rangle_{(\sigma^{-1}(1))},...,\langle col\_data \rangle_{(\sigma^{-1}(n))} $$

$$\langle question \rangle ::= \langle other\_columns \rangle\; \pi_{\sigma(\mathbf{QUESTION})}(\langle row \rangle)$$
$$\quad\quad\quad\quad\quad\;\langle other\_columns\rangle\;\pi_{\sigma(\mathbf{OPTIONS})}(\langle row \rangle) \;\langle other\_columns \rangle$$
$$\quad\quad\mid\;\langle question \rangle \langle newline\rangle \langle option \rangle$$

$$\langle option \rangle ::= \langle empty\_question\rangle \; \langle other\_columns\rangle \; \pi_{\sigma(\mathbf{OPTIONS})}(\langle row \rangle) \; \langle other\_columns \rangle$$
$$\quad\quad\mid\;\langle other\_columns\rangle \;\langle empty\_question\rangle \;  \pi_{\sigma(\mathbf{OPTIONS})}(\langle row \rangle) \; \langle other\_columns \rangle$$
$$\quad\quad\mid\;\langle other\_columns\rangle \; \pi_{\sigma(\mathbf{OPTIONS})}(\langle row \rangle) \; \langle empty\_question\rangle \;\langle other\_columns \rangle$$
$$\quad\quad\mid\;\langle other\_columns\rangle \; \pi_{\sigma(\mathbf{OPTIONS})}(\langle row \rangle) \; \langle other\_columns\; \rangle\langle empty\_question\rangle \;$$

$$\langle empty\_question \rangle ::= \langle null \rangle$$

$$\langle other\_columns \rangle ::= \langle null \rangle \mid \langle other\_column \rangle \; \langle other\_columns \rangle$$

$$\langle other\_column \rangle ::=  \langle repeatable\_column \rangle$$
$$\quad\quad \mid\; \langle nonrepeatable\_column \rangle $$
$$\quad\quad \mid \;\langle changable\_column\rangle$$

$$\langle nonrepeatable\_column \rangle ::= \;, \;\pi_{\sigma(\mathbf{FREETEXT})}(\langle row \rangle) \mid \pi_{\sigma(\mathbf{FREETEXT})}(\langle row \rangle)$$

$$\langle repeatable\_column \rangle ::= \; , \;\pi_{\sigma(\mathbf{BLOCK})}(\langle row \rangle) \mid \pi_{\sigma(\mathbf{BLOCK})}(\langle row \rangle)$$
$$\quad\quad\mid\; , \;\pi_{\sigma(\mathbf{EXCLUSIVE})}(\langle row \rangle) \mid \pi_{\sigma(\mathbf{EXCLUSIVE})}(\langle row \rangle)$$
$$\quad\quad\mid\; , \;\pi_{\sigma(\mathbf{ORDERED})}(\langle row \rangle) \mid \pi_{\sigma(\mathbf{ORDERED})}(\langle row \rangle)$$
$$\quad\quad\mid\; , \;\pi_{\sigma(\mathbf{RANDOMIZE})}(\langle row \rangle) \mid \pi_{\sigma(\mathbf{RANDOMIZE})}(\langle row \rangle)$$
$$\quad\quad\mid\; , \;\pi_{\sigma(\mathbf{CORRELATION})}(\langle row \rangle) \mid \pi_{\sigma(\mathbf{CORRELATION})}(\langle row \rangle)$$

$$\langle changable\_column \rangle ::= , \;\pi_{\sigma(\mathbf{BRANCH})}(\langle row \rangle) \mid \pi_{\sigma(\mathbf{BRANCH})}(\langle row \rangle)$$
$$\quad\quad\mid\; , \;\pi_{\sigma(\langle user\_defined \rangle)}(\langle row \rangle) \mid \pi_{\sigma(\langle user\_defined \rangle)}(\langle row \rangle)$$


$$\langle col\_data \rangle_{(\mathbf{FREETEXT})} ::= \mathbf{TRUE} \mid \mathbf{FALSE} \mid \mathbf{YES} \mid \mathbf{NO} \mid \langle string \rangle \mid \#\;\lbrace\; \langle string \rangle \; \rbrace$$
$$\langle col\_data \rangle_{(\mathbf{BLOCK})} ::= (\_|[a-z])?[1-9][0-9]*(\setminus .\_?[1-9][0-9]*)*$$
$$\langle col\_data \rangle_{(\mathbf{BRANCH})} ::= \mathbf{NEXT} \mid [1-9][0-9]*$$
$$\langle col\_data \rangle_{(\mathbf{EXCLUSIVE})} ::= \mathbf{TRUE} \mid \mathbf{FALSE} \mid \mathbf{YES} \mid \mathbf{NO}$$
$$\langle col\_data \rangle_{(\mathbf{ORDERED})} ::= \mathbf{TRUE} \mid \mathbf{FALSE} \mid \mathbf{YES} \mid \mathbf{NO}$$
$$\langle col\_data \rangle_{(\mathbf{RANDOMIZE})} ::= \mathbf{TRUE} \mid \mathbf{FALSE} \mid \mathbf{YES} \mid \mathbf{NO}$$


<small>Notes : <ul>
<li>The last match for the $$\langle block\rangle$$ rule should really be four rules -- one for one question and one block, one for one question and many blocks, one for one block and many questions, and one for many blocks and many questions, but I thought that would be cumbersome to read, so I shortened it.</li>
<li> All $$\langle col\_data \rangle$$ rules not explicitly listed are unconstrained strings.</li>
<li> Remember, the subscripted $$n$$ in $$\sigma_n$$ is for nondeterminism; the $$n$$ indexing the column data denotes the number of input columns.</li>
<li> We also accept json as input; the json parser assigns a canonical ordering to the columns, so generated ids and such are equivalent.</li>
</small>

What's worth noting here are the things that we intend to check on the first pass of the parser and compare them against those that require another iteration over the structure we've built. We would like to be able to push as much into the syntax as possible, since failing earlier is better than failing later. On the other hand, we want the syntax of the language to be clear to the survey writer; if the survey writer struggles to just get past the parser, we have a usability problem on our hands.

A good example of the limitations of what can be encoded where is the <code>BRANCH</code> column. Consider the following rules for branching:
<ol>
<li> Branch only to top-level blocks.</li>
<li> Branch only to static (i.e. non-floating) blocks.</li>
<li> Only branch forward.</li>
<li> Either branch to a specific block, or go to the next block (may be floating).</li>
</ol>

(1) and (2) are encoded in the grammar -- note that the regular expression does not include the subblock notation used in the <code>BLOCK</code> column's regular expression. Furthermore, the <code>BRANCH</code> rule will not match against floating blocks. Any survey that violates these rules will fail early.

(3) is not encoded directly in the grammar because we must build the question and top-level containing block first. Since it cannot be verified in the first pass, we perform this check after having parsed the survey into its internal representation. 

(4) cannot be determined until runtime. This is an example of where our syntax and static checks reach their limits. $$\mathbf{NEXT}$$ is typically used when we want to select exactly one question from a block and continue as usual. A user could conceivably use $$\mathbf{NEXT}$$ as a branch destination in the usual branching context; the only effect this could have would be the usual introduction of constraints imposed by having that block contain a "true" branch question.

This leads into my next post, which will (finally!) be about the survey program semantics.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1392</wp:post_id>
		<wp:post_date>2014-06-19 15:05:06</wp:post_date>
		<wp:post_date_gmt>2014-06-19 19:05:06</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>what-does-it-mean-for-a-survey-to-be-correct-a-first-stab-at-formalizing-surveyman</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Defining Equality</title>
		<link>http://blogs.umass.edu/etosch/2014/07/31/defining-equality/</link>
		<pubDate>Fri, 01 Aug 2014 03:29:19 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1758</guid>
		<description></description>
		<content:encoded><![CDATA[This post follows directly from my <a href="http://blogs.umass.edu/etosch/2014/06/19/what-does-it-mean-for-a-survey-to-be-correct-a-first-stab-at-formalizing-surveyman/">previous post</a> on correctness. Rather than dive directly into the details of correctness for surveys, I wanted to spend some time thinking about some of the things we might want to prove, show or derive first.

We've stated that one way to define what we mean by survey correctness is to say that the same survey (modulo randomization) administered to the same person, under the same set of conditions, will yield the same set of results. Let's start by assuming that it is possible to repeat the survey for a respondent under the exact same conditions. Then there is is some underlying gold standard for the survey. While we cannot repeat the survey for the same set of respondents, under the exact same conditions in practice, we can emulate this scenario with our <a href="http://blogs.umass.edu/etosch/2014/03/24/simulation-and-detecting-bugs-correlation/">simulator</a>. We choose a set of answers, according to a particular profile and let this be the gold standard. The simulated respondent then chooses a response according to the map from questions to answers. Once the survey is complete, we compare the results returned by the respondent with our generated gold standard. The question-answer sets for the gold standard and the returned values should always be equal.


At first, it may seem that this definition of equality is trivial. We have unique identifiers for each question and answer option that are created directly from their data's location in the input csv. The response form uses these ids. One question is displayed at a time and no questions ever repeat (we'll prove this later). We simply check the id of the question currently displayed, look up its answer in our response map, and select the option(s) that has/have matching identifier(s) to our gold standard's responses. So long as the HTML is correct, we will get back the same id, right?


Wrong. Well, a little wrong. If your survey does not use variants, you're fine. If it does, here's the problem: the variant is chosen at runtime i.e. when the respondent is actually executing the program (taking the survey). Since we cannot divine which question will be shown, we need a different way of deciding the appropriate answer.

<h3>Solution 1: Specify the random choice at compile-time, rather than run-time.</h3> 
This is certainly a compelling argument, and it solves our problem of not knowing what question will be displayed at runtime. When we began work on SurveyMan, we generated all of our surveys statically. All of the randomization was done in Java. We produced the full HTML to be displayed and sent this over as the payload. We didn't have much in the way of an interpreter; we just had some jQuery running in <code>document.ready</code> that hid all question divs and then showed the first question. We were not playing with breakoff at this point, so a next and a previous button were always displayed (except for the first and last questions). We simply swapped which divs were visible and which weren't.

<b>Practical issues</b> We found considerable issues with this approach, enumerated below:
<ul>
	<li><b>AMT limitations</b> While this may change at any time, historically there haven't been many degrees of freedom when deciding assignments of workers to tasks on AMT. Since our primary source of respondents was AMT, we had to keep this in mind. Since each survey was static HTML, we had to send each over in its own HIT. In the past, this meant that we would post some number of HITs at a time. The easiest way to do this was to post a survey, wait for a response, validate that response, issue "reverse" qualifications, and then post some more. The "reverse" qualification thing is an Automan technique to disqualify people who had already responded to that HIT under a different randomization.

There are some logistical issues with this approach. Posting one HIT that has one assignment at a time means that we incur significant latency in our response period. We cannot take advantage of the massive parallelism of the crowd. If we instead post the HITs in batches, we have to hedge against workers who will answer some number of HITs in a row before the reverse qualification kicks in. We end up paying for redundant collection, as well as incurring additional latency (due to the workers who completed the HIT more than once). Repeaters are a major threat to this approach -- AMT groups HITs in such a way that similar HITs appear together. The researcher can get around this automated grouping by creating a new group id and putting each HIT in its own group. Now we are back to the original latency problem, because we have one HIT wonders floating around in the MTURK ether.

Note that the AMT system is designed as follows: Requesters may either specify one HIT that has many assignments (redundancy), or they may specify many HITs that will be grouped together <em>with the express purpose of making it easier for one person to perform many HITs</em>. We would like to be able to take advantage of the former, and posting static HTML does not allow this.</li>

	<li><b>Page Load Time</b> The prototypicality survey had almost 100 questions. There was an obvious lag in loading on several browsers; the contents flashed before hiding the appropriate divs. We could have denoted these divs as hidden, but we would still have the lag of loading in a large file. The problem was even worse when we loaded in media content. It was clear from this alone that some of the content would have to be loaded on an as-needed basis.</li>

	<li><b>Payload size</b> AMT has a maximum payload size. I can look it up, but needless to say, we hit it with one of the linguistics surveys. I believe that survey had approximately 200 questions -- they were similar in nature to the phonology questions. The true payload would have been smaller, since that survey was designed to be sampled from. However, there will be cases where survey writers are going to want to simulate an infinite stream of survey questions. We would like to be able to send over as large a survey as possible, and this is just not going to happen if we send over a static webpage. Now, one could argue that the cost of the payload was really in the redundant HTML we were sending over -- we could easily have swapped out the question and option text, while still specifying the full set of questions to be seen a priori. This is a fairly compelling argument that adequately addresses both the page load time, as well as the payload size.</li>

	<li><b>Opportunities for exploitation</b> It's very easy for someone to write a script that, for instance, selects every radio button answer and hits submit. In fact, if we have a submit button sitting around anywhere, we have a problem. We could dynamically generate all buttons as needed, while still sending over the statically chosen questions. The solution at this point would be to send over minimal HTML and instead swap in the data that we need, as needed. All buttons and control flow through the survey would be controlled by a small state machine. This would prevent respondents from writing a script to click on all input of a given type and submit. It's certainly still possible to write a "bot" to submit a form when inputs are generated dynamically, but it will now require the script to simulate the survey, which is more effort. Furthermore, the user has no guarantee that the script will terminate (although we know it will).</li>

	<li><b>Opportunities to "cheat."</b> Besides automated "cheating," it's also possible for users to see the set of questions being asked, simply by looking at the source code. If we store the questions and option text in a lookup table, they become significantly more difficult for a human to interpret -- while they can still see what the questions are asking, human adversaries will not have the full picture. Even though there is still room for adversarial action, we hope that the increased indirection of dynamically generating the questions increases the complexity of the problem and deters adversaries from gaming the system.</li>
</ul>

The above arguments make a strong case for generating the survey dynamically. The first point is the strongest practical reason for performing randomization at runtime; for the latter points, randomizing at runtime gives us a nice defense against some adversarial behavior, but it isn't particularly compelling.

A major advantage to randomizing at runtime is that it is fundamentally more principled than randomizing a priori. Our randomization mechanism can also be framed as a an issue of nondeterministic choice -- in the case of variants, we have choose one of a set of these variants. In the case of the total set of questions, it is one of the set of all possible permutations. In principle, we can say that the behavior of the machine at runtime is the same for all permutations of the survey, since the machine itself is selecting which permutation. All equivalent surveys are statically equivalent, with only the runtime behavior differing. This will make certain properties easier to prove later on.

<h3>Solution 2: Mark every question in a variant block with equivalent answers</h3>

Recall that the problem we're trying to solve is how to define equivalence between questions. The idea behind fixing the random order at compile time was so that our two parallel universe survey takers would end up with the exact same answers. Even if you don't buy the argument to randomize at runtime, rather than randomizing at compile-time, there's still the problem of comparing across different respondents who see different variants. 

A new pass solution might be to answer every question in a variant's block. However, if we do this, we will have answers for every question in that block sitting around in our answer set. The resulting set of question-answer pairs will not be equal to the gold standard set.

We could try to get around this shortcoming by removing the questions that weren't seen. This doesn't quite work either. One of the things we're trying to show is that, not only are the answers consistent (this is the part of the correctness argument that addresses bugs that are unique to surveys), but also that the same set of questions are seen (this is the part of the correctness argument that will address the more traditional programming languages issues in our design). If we just remove the other questions, we run into a dangerous situation where we are adding and removing data in the course of our correctness argument, which will make some things harder to prove. Furthermore, we will need to wait until runtime to decide whether two answer sets are equivalent. 

<h3>Solution 3: Define a set of equality relations for our objects of interest</h3>

Thus far our solutions have been computational in nature -- their correctness is defined by their implementation. We'd really like a more general solution that will allow us to reason about a survey program from first principles. We would also like a solution that can be applied to a broader swatch of implementations. We'd like our definitions to be the gold standard of correctness for future survey language writers. Let's start with some definitions. 

A <b>survey</b> is a set of blocks. A <b>block</b> is a heterogenous set of questions and blocks. A <b>question</b> is a record with a survey-scoped unique identifier and a (possibly empty) set of answer options. An <b>option</b> is a record with a survey-scoped unique identifier. These all follow from our definition of the syntax. 

<b>Answer Set</b> An answer set is a set of 3-tuples of question identifiers, option identifiers, and indices.

<h4>Equality of Answer Sets</h4> 
Two sets of answers are "equal" (denoted as $$\equiv_{resp}$$, called "response-equivalent") if and only if two conditions are satisfied:
<ol>
	<li>There exists a bijective mapping between two answer sets.</li>
	<li>The tuples in the mapping satisfy an equivalence relation, which we will define.</li>

</ol>

<h5>Begin Algebra Tangent</h5>

I asked <a href="http://people.cs.umass.edu/~dstubbs/">Dan Stubbs</a> if there was an algebraic structure where these properties would make sense. He suggested that perhaps I was defining a quotient group. After a nap and some perusing of Wikipedia (and a dense Algebra text), I'm not entirely sure this is what I want. Here are my unstructured thoughts on this:

<ul>
	<li>The answer set is a subset of all possible answers. All possible answers for each question type are defined in the language as follows:
<ul>
<li><b>Exclusive (Radio Button) questions</b> The set of individual response options.</li>
<li><b>Non-exclusive (Checkbox) questions</b> The power set of response options minus one (you cannot have the empty set -- that is, the respondent must check at least one box).</li>
<li><b>Regex questions</b> Any string satisfying the regex, up to the size allowable in the response field (would have to look this up).</li>
<li><b>Freetext questions</b> The subset of all strings up to the size allowable in the response field.</li>
</ul>
</li>

	<li>Since we have a clear mapping between questions and answers, we can talk about questions instead. Really, when we say that the response set is invariant under randomization, we're saying that the question-answer pairs should be invariant under randomization.</li>

	<li>According to <a href="http://en.wikipedia.org/wiki/Quotient_group">Wikipedia</a>


<blockquote>In mathematics, specifically group theory, a quotient group (or factor group) is a group obtained by aggregating similar elements of a larger group using an equivalence relation that preserves the group structure. </blockquote>

What's the larger group here? Presumably it would be the set of all possible answers. This doesn't seem right, thought -- we don't want the set of all answers, because this would mean that a valid response could include answering, say, two questions from the same variant block. Instead, I think it ought to be the set of unique answer sets. We're not worrying about breakoff for now, so we don't need to define this as the power set of some set of answers. If we have $$n$$ questions, each with $$m$$ radio-button (exclusive) options and no variants and no breakoff, then we have $$m^n$$ possible answer sets. To account for variants, we multiply by the cardinality of each variant set.

   What are the smaller groups? They're the set of question-answer pairs we've collapsed. Now what's the group structure? To be honest, I'm not entirely sure an answer set satisfies the definition of a group. For starters, what's our group operator? I suppose a tempting first choice might be a set addition operator. We could say that if $$\lbrace (q_i, a_i, i) \rbrace$$ and $$\lbrace (q_j, a_j, j) \rbrace$$ are each in the set of possible question-answer pairs, then  $$\lbrace (q_i, a_i, i), (q_j, a_j, j)\rbrace$$ is also in the set of possible question-answer pairs. Of course, for $$\lbrace (q_i, a_i, i) \rbrace$$ and $$\lbrace (q_j, a_j, j) \rbrace$$ to be floating around, we would need to allow breakoff. Now, what would happen if we tried to combine two complete answer sets? If the answer sets are equivalent, we get back the same answer set. If the answer sets are equivalent, modulo variant questions, how do we decide what's returned? If the answer sets are radically different, then what?

I think at this point, we need to be talking about questions, rather than answer sets. Two answer sets can differ either because the questions seen are different (due to breakoff or variants) or because the answers chosen are different, or both. If we only consider questions, we can define a question-combining operator (rather than a response-combining operator) and get the properties we want. We would need to define some unified variable to represent the set of variants, so any time a variant is combined, we represent the variant with that variable.  

And this is about where I'm stuck. It's not clear to me that this kind of algebraic structure is what we want. Wait -- why would we want it in the first place? Well, I like challenging myself to think about these problems from a variety of angles, so it's intellectually satisfying as an exercise in its own right. But it could also prove fruitful later down the line. It might be the case that by having these algebraic properties, we get some proofs of correctness or other properties for free. Reducing a known and well-studied problem to a new domain for the win!

<h5>End Algebra Tangent</h5>

So basically, we need to define this bijective mapping. Here's the short version (since this post has been sitting, incomplete and open in a tab for well over a month):

Two questions are eqivalent if either:
<ul>
	<li>The questions are identical. That is, they have the same ids (and should belong to the same block, have the same flags, etc. Questions are immutable entries in a spreadsheet, so you could only get two questions with equal ids but unequal other stuff if your lexer/parser/compiler screwed something up).</li>
	<li>The questions are both contained in a block having type <small>ALL</small>. In this case, the number of options will be equivalent. The order of their entry denotes equivalence. That is, the $$i^{th}$$ option for each question variant is assumed to be equal in its meaning.</li>
</ul>

Now here's the punchline: since we use different paths through the blocks (and thus different containing block ids) to denote an experimental control (really, a "control path") and to denote experimental designs such as Latin squares, we currently have no way in either our static nor our dynamic analyses to understand equivalence across these paths. Although we can express experimental surveys using SurveyMan, our system currently cannot analyse these surveys as experiments. 
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1758</wp:post_id>
		<wp:post_date>2014-07-31 23:29:19</wp:post_date>
		<wp:post_date_gmt>2014-08-01 03:29:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>defining-equality</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>On Experiments vs Surveys (Ramble Time!)</title>
		<link>http://blogs.umass.edu/etosch/2014/07/09/on-experiments-vs-surveys-ramble-time/</link>
		<pubDate>Wed, 09 Jul 2014 12:21:44 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1778</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
Big data hubris is the often implicit assumption that big data are a substitute for, rather than a supplement to, traditional data collection and analysis.
...
The core challenge is that most big data that have received popular attention are not the output of instruments designed to produce valid and reliable data amenable for scientific analysis. 
 <a href="http://www.lazerlab.net/sites/default/files/publications/The%20Parable%20of%20Google%20Flu%20%28WP-Final%29.pdf">Lazer et al.</a></blockquote>



I wrote <a href="http://blogs.umass.edu/etosch/2014/03/11/observational-studies-surveys-quasi-experiments-and-experiments/">previously</a> about the range of experimental paradigms available to us on the web. In that post, we looked at observational studies, surveys, quasi-experiments, and experiments on axes that captured the range of control available to us. We might to consider other axes as well:

<h3>Exploratory vs. Confirmatory</h3> 

Observational studies and surveys can be considered more exploratory, whereas quasi- and true experiments are more confirmatory. This follows from the level of control we expect to have. Observational studies, as we've said before, are a lot like data mining. The environment that we are observing is rich with potential features, but may also be sparse with respect to observations on those features. Suppose we were interested in improving a recommendation system. We might start by looking for correlations between products. That is, we might observe that people who watched <em>The Wizard of Oz</em> also watched <em>It's a Wonderful Life</em> on Netflix, or people who bought Bishop's Machine Learning book also bought Hastie et al's <em>Elements of Statistical Learning</em> on Amazon. We would quickly find that while there are a few strongly correlations, we would have a long tail of single observations. Furthermore, our observations do not differentiate between a lack of information and negative training data. That is, we do not know if someone hasn't watched <em>2 Fast 2 Furious</em> because they didn't know it existed or if they didn't watch it because they already know they won't like it. Many of these recommender systems have user-provided ratings to help differentiate these two categories, but they cannot cover all the cases, since ratings are still opt-in. 

While I am personally not particularly interested in recommender systems, they do provide a nice example of how different instruments correspond to exploratory vs. confirmatory data analysis. We could start by extracting interesting features from our user and product base. By "interesting" we mean features that have sufficient observations, that are as close to orthogonal as possible. The researcher should use domain knowledge to enumerate features. 

<b>Slight tangent</b>: One thing I want to point out at this point is that enumerating features is the first step in generating a hypothesis we might want to test. In a lot of ways, features are the source of the hypotheses we're testing. Features are the basic building blocks of hypotheses. Emery and I discussed recently the idea that hypotheses are like a probabilistic grammar over these features. Typically in machine learning research, the model is the subject of interest. The model is a representation of the variables of interest. What is the relationship between the features and the variables? Features are either directly observed or computed from the data. Personally, I would say that "computed from the data" here should have kind of bound on it. Clearly in the case of text classification, surface string suffixes are valid features. But what about parts of speech? If you are performing co-reference resolution or doing some kind of entity identification, then parts of speech could be a useful feature. But what if your task *is* part of speech tagging? Would it be fair to use the tags from one part of speech tagger as input to another? What if you did a first pass with your tagger, and then used that information as input on another run (that is, your tagger can take its own output as input)? Now what are the differences between variables in your model and features?  /endrant

We would begin our exploratory analyses by looking at the features and the raw data and seeing what we find there. We can use this information or our own domain knowledge to construct a survey. The purpose of this survey is to collect data we might not otherwise have access to. For example, we might explicitly ask respondents who use Netflix about their viewing preferences. As discussed in my earlier post, this helps us fill in some data we might not have already. 

A difference we generally see between traditional surveys and what we'll call experimental questionnaires is the presence of a hypothesis to test. We'll get into this in a later blog post, but generally in surveys we do not have the notion of a null hypothesis, nor an experimental control. I would say that surveys have their origins in much more qualitative goals. While they are used to guide decision-making, they are also widely used to help tell a narrative. This isn't to say that they can't or aren't being used  They can be manipulated easily, if they are not designed well, and it can be difficult to differentiate between well-designed surveys and poorly designed surveys. This susceptibility to manipulation strongly motivated our work on SurveyMan. 

We noticed that there were considerable differences in the objectives and designs of the surveys our collaborators were using. For example, our collaborators in linguistics used surveys in a more experimental context -- the questions in their surveys bore strong similarity to each other and their differences could be subtle to detect. Many of the questions belonged to discrete categories (what we might call "factors" in experimental design). There wasn't much variability between questions. Those surveys were very focused on acquiring enough information to answer a particular question, or test a particular hypotheses. 

The wage survey we ran was quite different. The questions were structurally heterogenous. None of the questions were "redundant" (that is, they did not appear to represent categories of questions that are equivalent in some way). The nature of survey design will always inject some bias for generating hypotheses -- for example, the wage survey did not ask questions about favorite foods or whether the respondents had a family history of breast cancer. The authors of the wage survey were clearly interested in the relationships between education, employment, attitudes toward the AMT marketplace, and willingness to negotiate wages. However, the survey was not encoded as an experiment. It was more exploratory. We observed some interesting behavior in the context of randomization and breakoff, and would like to use this information in a future study.  

<h3>Missing Data</h3>
In a survey, if we have missing data, it's due to breakoff. We are faced with some choices: if breakoff occurs more frequently at a particular question, we ought to look at the question and see what's going on -- is it worded poorly? Are there insufficient options to cover potential responses? Is the question potentially offensive? If breakoff occurs most frequently at a position, this might indicate that the survey is too long, or that there's a jarring shift between blocks in the survey. We would generally use this information to help debug the survey.

In an experiment, it's not quite clear that the researcher would use the same information in the same way. Since we expect experiments to have some redundancy built in, breakoff at a particular question might tell us less than breakoff at a question type. That is, we might suspect there to be a latent variable influencing breakoff. Our analysis will have more hypotheses to consider when diagnosing the cause of breakoff.

<h3>Confounding variables</h3>
A major threat to validity in experiments that is not typically addressed (so far as I can tell) is the failure to model confounding variables. This is where we might use surveys, or a survey-like section of an experiment, to help identify these variables. We've done this sort of thing before -- in the linguistics surveys, we had a demographics section. We could expand this and ask more questions in an attempt to capture these other variables. In any case, there is something qualitatively different about the demographic questions, when compared with the core questions of interest. In both surveys and experiments, we can view demographic questions as a path to stratifying the population. However, there seems to me to be a clearer divide between how these questions are used in surveys versus experiments.

<h3>Correlation vs. Causation</h3>
With surveys, we are looking primarily for two things: the raw data (e.g. 35% of Netflix subscribers have viewed <em>The Wizard of Oz</em>) and correlations (viewing <em>It's a Wonderful Life</em> is positively correlated with having viewed <em>The Wizard of Oz</em> more than once). In experiments, we are also interested in the raw data and correlations, but we are also looking for causal relationships and patterns in the latent variables. If we are lucky, we might be able to find causal relationships in a dataset, but the permutations required to infer a causal relationship may be too spare for us to discover them through data analysis alone. Instead, we will need to design experiments to ensure coverage of the space. This will require some changes to the specification of the survey language, to capture the abstractions we need. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1778</wp:post_id>
		<wp:post_date>2014-07-09 08:21:44</wp:post_date>
		<wp:post_date_gmt>2014-07-09 12:21:44</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-experiments-vs-surveys-ramble-time</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Reproducibility and Privacy</title>
		<link>http://blogs.umass.edu/etosch/2014/07/09/reproducibility-and-privacy/</link>
		<pubDate>Wed, 09 Jul 2014 13:16:35 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1812</guid>
		<description></description>
		<content:encoded><![CDATA[What would it take to have an open database for various scientific experiments? An increasing number of researchers are posting data online, and many are willing to (and required) to share their data if you ask for it. This is fine for a single experiment, but what if you'd like to reuse data from two different studies? 

There is a core group of AMT respondents who are very active. Sometimes, AMT respondent contact requesters, at which point they are no longer anonymous. My colleague, <a href="http://cs.umass.edu/~dbarowy">Dan Barowy</a> received an email from a respondent, thanking him for the quality of the HIT. I asked him the respondents name and as it turned out, they had contacted me when I was running my experiments as well.

So if we have the general case of trying to pair similar pieces of data into a unit (i.e. person) and the specific case of AMT workers who are definitely the same people (they have unique identifiers), how can we combine this information in a way that's meaningful? In the case of the AMT workers, we will need to obfuscate some information for the sake of privacy. For other sources of data, could we take specific data, infer something about the population, and build a statistical "profile" of that population to use as input to another test? Clearly we can use standard techniques to learn summary information about a population, but could we take pieces of data and unify them into a single entity and say with high probability these measurements are within some epsilon of a "true" respondent? How would we use the uncertainty inherent in this unification to ensure privacy?

Is it possible to unify data in such a way that an experimenter could execute a query asking for samples of some observation, and get a statistically valid Frankenstein version of that sample? I'm sure there's literature out there on this. Might be worth checking into...]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1812</wp:post_id>
		<wp:post_date>2014-07-09 09:16:35</wp:post_date>
		<wp:post_date_gmt>2014-07-09 13:16:35</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>reproducibility-and-privacy</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="experiman"><![CDATA[experiman]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>SocialSci</title>
		<link>http://blogs.umass.edu/etosch/2014/07/31/socialsci/</link>
		<pubDate>Thu, 31 Jul 2014 23:52:02 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1900</guid>
		<description></description>
		<content:encoded><![CDATA[There's this website I came across while investigating what other people are doing for online experiments. It's called SocialSci.com. They have a platform for writing experimental surveys. They also boast of a well-curated participant pool. Here's the claim on their front page:



<blockquote>We take a three-tiered approach to our participant pool. We first authenticate users to make sure they are human and not creating multiple accounts. We then send them through our vetting process, which ensures that our participants are honest by tracking every demographic question they answer across studies. If a participant claims to be 18-years-old one week and 55-years-old the next, our platform will notify you and deliver another quality participant free-of-charge. Finally, we compensate participants via a secure online transaction where personally identifiable information is never revealed.</blockquote>

Ummmmm, okay. That's not game-able at all. 

If you were thinking about just using their survey design tools and instead posting your instrument on AMT, be warned! They <a href="https://research.socialsci.com/v2/pool_requests/open">point out</a> that AMT is full of scammers! I'm not sure this is true anymore; my own research has lead me to believe that traditional survey issues such as fatigue and attention issues are a bigger threat to validity. In any case, I looked up quotes for using their participant pool. The costs for 150 respondents (assured to be valid, via some common sense heuristics and their more complete knowledge of the participant pool) and no restrictions on the demographics (e.g. can include non-native English speakers) are:

[table id=6 /]

<a href="http://blogs.umass.edu/etosch/2014/05/16/get-money-get-paid/">Even with bad actors, I'm pretty sure AMT is cheaper.</a> The results were the same when I submitted a request for country of origin==USA. There were also options for UK and Australia, but these are not yet available (buttons disabled). I'll leave any analysis of the pay to Sara.

Their filters include Country, Language, Age, Sex, Gender, Sexual Orientation, Relationship Status, Ethnicity, Income, Employment Status, Education, Occupation, Lifestyle, and Ailments. What I find potentially useful for survey writers here is the opportunity to target low-frequency groups. There's our now infamous <a href="http://www.npr.org/blogs/ed/2014/05/22/313166161/mischievous-responders-confound-research-on-teens">NPR story</a> on how teens respond mischievously to survey questions. The set of responses from these teens have a high number of low-frequency responses, which amplifies false correlations when researchers analyze low-frequency populations. A service like SocialSci could provide useful, curated pools that rely less on one-time self reporting. However, it doesn't look like they're there yet, since there are the options available (shown, but currently disabled options are listed in brackets):

Country: USA, [UK], [Australia]
Language: English, [Spanish]
Age: 13-17, 18-40, 41-59, [51+], [60+]
Sex: Male, Female, [Transgender], [Intersex]
Gender: Cis Male, Cis Female, [Trans Male], [Trans Female]
Sexual Orientation: Heterosexual, Homosexual, BiSexual (sic), Other
Relationship Status: Married, Single, Co-Habitating, Dating
Ethnicity: Caucasian, [Asian], [Hispanic], [Black], [Native American], [Multiracial]
Income: less than 25K, 25K-50K, 50K-75K, 75K-100K, [100K-125K], [125K or more]
Employment Status: Full Time, Unemployed, [Part Time],  [Temporary Employee Or Independent Contractor]
Education: Some College, Associate's Degree, High School Diploma, Bachelors Degree, Some High School, [Masters Degree],  [Doctoral Degree]
Occupation: Student, Professional, Technical, Teacher, Sales, Corporate
Lifestyle: Smoke, Used to Smoke, Have Children, Have Cell Phone
Ailments: Chronic Pain, Addiction (Smoking)

** sorry for the lack of formatting!! Also, what do you think of these demographic characteristics? Please leave comments!**

I want to note that the front page advertises "a global pool." In keeping with the spirit of the times, I've included a screenshot (things I've learned from shaming bigoted celebrities who express themselves a little too freely on twitter!)

<a href="http://blogs.umass.edu/etosch/files/2014/07/Screen-Shot-2014-07-31-at-7.04.57-PM.png"><img src="http://blogs.umass.edu/etosch/files/2014/07/Screen-Shot-2014-07-31-at-7.04.57-PM-300x187.png" alt="Screen Shot 2014-07-31 at 7.04.57 PM" width="300" height="187" class="aligncenter size-medium wp-image-1920" /></a>

Now, it's quite possible that they have at least one person from every demographic listed, but not 150. Until they can compete with AMT in size (where 150 respondents ain't nothin), I think they should be careful about what they advertise to researchers.

It's not clear to me how SocialSci recruits their participants. Here's the <a href="https://research.socialsci.com/participants">page</a>. I had never heard of them until I went looking for resources used in other experiments (via <a href="http://psych.hanover.edu/research/exponnet.html">this page Emery recommended</a>) I just did a search of craigslist to see if there's anything there. No dice. $10 says it's just their team plus friends:

[youtube]https://www.youtube.com/watch?v=N9qYF9DZPdw[/youtube] 

Snark aside, I'd like to see what a well-curated pool looks like. Not sure these folks are up to the task, but just in case, I did them a solid and <a href="http://detroit.craigslist.org/wyn/cpg/4597116325.html">posted</a> <a href="http://bham.craigslist.org/cpg/4597133924.html">their</a> <a href="http://baltimore.craigslist.org/cpg/4597134751.html">signup</a> <a href="http://memphis.craigslist.org/cpg/4597135930.html">page</a> <a href="http://neworleans.craigslist.org/cpg/4597141348.html">on</a> <a href="http://flint.craigslist.org/cpg/4597142147.html">craigslist</a>. I'm not holding out on it being as effective as the Colbert bump, but a girl can dream.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1900</wp:post_id>
		<wp:post_date>2014-07-31 19:52:02</wp:post_date>
		<wp:post_date_gmt>2014-07-31 23:52:02</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>socialsci</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="experiman"><![CDATA[experiman]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Metrics to answer our research questions</title>
		<link>http://blogs.umass.edu/etosch/2014/07/31/metrics-to-answer-our-research-questions/</link>
		<pubDate>Fri, 01 Aug 2014 03:00:00 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=1902</guid>
		<description></description>
		<content:encoded><![CDATA[We had a number of excellent questions and comments from our reviewers. I've been working on a few entries to address them. 

One of our reviewers pointed out that we didn't provide metrics for our research questions. Another pointed out that our results were qualitative, but that this was okay because we provided case studies, rather than user studies. Since we'd like to be able to do user studies in the future (hopefully the exposure we'll get at OOPSLA will lead to more collaborations!), it's certainly worth thinking about.

<h3>Research Question 1: Is SurveyMan usable by survey authors and sufficiently expressive to describe their surveys?</h3>

<b>Usable by survey authors?</b> We think so. Certainly our colleagues in linguistics didn't have any problems with the CSV structure and the PL-ness of it. We were warned a bit more by our colleagues in econ. The argument I've heard is that (a) people are accustomed to flashy interfaces and would put off by using something text-based and (2) the formatting of the data, both the input and output, is counter-intuitive. I'm not entirely surethis is true. First of all, people have been using SAS and SPSS for years. When I took my first stats course in Spring 2004, we used one of those tools (too long ago to remember which). I remember the painstaking data entry involved. The summaries spit out, full of text tables (no graphics) weren't the easiest to interpret. This was the standard tool for statistical analyses in the social sciences at the time (I was an econ major then).

This project has been a challenge \emph{because} less-powerful, flashy alternative exist. Our colleagues in linguistics liked the csv idea because they were already storing their data in csvs (since they're easy to load into R and since they can -- and already were -- programmatically generate the data). It's unclear whether people in fields that are used to generating these things by hand will be as receptive. There's no reason I can see why someone wouldn't be able to write in our csv language, but drag and drop doesn't require reading a spec, and it's what most commercial services are using.

While I think the language is usable as-is, it would be more usable if we had a spreadsheet plugin to statically check the thing. How would we measure the usability of the tool, empirically?

I think one way of evaluating its usability would be to gather a bunch of surveys and recruit people for a user study. A traditional, offline version of this might go as follows:

<ol>
	<li>Generate a list of available survey tools.</li>
	<li>Recruit a representative sample of participants across the social sciences (probably other graduate students at UMass).</li>
	<li>Administer a survey to those graduate students to learn which survey technologies they'd used before, whether they were familiar with any programming languages, and if they have any experience with spreadsheets and/or databases.</li>
	<li>Ask the participants to take a survey specification and design a series of surveys with various features on one of the competing survey software packages (that they had not used before) and have them do the same tasks with SurveyMan.</li>
	<li>Perform a post-treatment survey to ascertain results.</li>
</ol>



We would measure the time it took to complete the task, the number of clarifying questions the participants ask, and the perceived ease of the task. It might be a good idea to have the post-study follow-up survey about a week later.

Some design features we might want to consider are whether we assign the SurveyMan task first, how much prior experience in either programming languages or survey software served as prior training, and how we present the survey tasks. If we give the participants csvs of questions and just ask them to manipulate the data, we will be biasing them to a more favorable view of SurveyMan. We might instead give them a variety of inputs, such as a csv, a pdf, an English description of a survey we might want to run, and a followup survey based on that English description.  

<b>Expressive enough to describe the target population's surveys?</b> Yes. We can certainly express all of the surveys that our clients wanted to be able to express, and in some cases we were able to express more. One interesting aspect of our collaborations is that we've discovered that some things we've been calling surveys are actually experiments. While we can describe them using our language, we currently lack the support in our runtime system to perform the kinds of analyses we'd like to see. This has caused us to pursue work in a new direction.

I think the best way to address this question would be with a list of features describing what it means to be a survey. I've been thinking about the difference between experiments and surveys a lot recently and a checklist like this would be illuminating.

<h3>Research Question 2: Is SurveyMan able to identify survey errors?</h3> 

We think so. We use standard statistical methods to identify wording bias and question order bias. 

For breakoff, we report the distribution of breakoff over questions and positions. Diagnosing whether we have an unusual amount of breakoff requires a prior over comparable surveys, so we don't attempt to define a threshold for whether the amount of breakoff is statistically significant. 

We could model a small amount of noise that says, "if this survey is well-formed, every respondent will have equal probability of breaking off at any point before the final question." We could say that there is some small $$\epsilon$$ representing this probability, which is independent of any features of the survey. Let $$r$$ represent our sample size. Then the expected number of respondents who broke off from this survey would be $$r\epsilon$$. If the total number of respondents who broke off before the final question is unusually high, then the survey author would need to debug the survey to figure out why so many respondents were leaving early. 

We're clearly be interested in where respondents were breaking off, and not just that they were breaking off. It's possible that the survey is uniformly poorly designed; it's also possible that it's too long or that there is some point in the survey (such as a transition from one block to another) that causes an unusual amount of breakoff. For a flat survey having $$n$$ questions, every position is identical insofar as the set of questions that may be seen there. In such a case, we might want to model breakoff as a series of Bernoulli trials -- at each point, a respondent may break off with probability $$\frac{\epsilon}{n-1}$$. We would then expect there to be $$\frac{\epsilon}{n - 1} \times r$$ people breaking off at each position. 

The problem with this model is that although the probability of breaking off at any given point is the same, the number of participants is not \emph{because a single person cannot break off twice}. First, let's model this as a recurrence. Let $$B_i$$ be the random variable denoting expected breakoff at position $$i$$. Then we get the recurrence:

$$\mathbb{E}(B_1) = \frac{r\epsilon}{n-1}$$
$$\mathbb{E}(B_{i&lt;n}) = \frac{\epsilon}{n-1}(r - \sum_{k=1}^{i-1}\mathbb{E}(B_k))$$

Let's see if we can do better substituting in values:

$$\mathbb{E}(B_2) = \frac{\epsilon}{n-1}\biggl(r - \frac{r \epsilon}{n-1}\biggr) = \frac{r\epsilon}{n-1}\biggl(1 - \frac{\epsilon}{n-1}\biggr)$$
$$\mathbb{E}(B_3) = \frac{\epsilon}{n-1}\biggl(r - \bigl(\frac{r\epsilon}{n-1}\bigl(1 - \frac{\epsilon}{n-1}\bigr)\biggr) = \frac{r\epsilon}{n-1}\biggl(1 - \frac{\epsilon}{n-1} + \bigl(\frac{\epsilon}{n-1}\bigr)^2\biggr)$$
$$\mathbb{E}(B_4) = \frac{\epsilon}{n-1}\biggl(r - \frac{r\epsilon}{n-1}\bigl(1 - \frac{\epsilon}{n-1} + \bigl(\frac{\epsilon}{n-1}\bigr)^2\bigr)\biggr) = \frac{r\epsilon}{n-1}\biggl(1 - \frac{\epsilon}{n-1} + \bigl(\frac{\epsilon}{n-1}\bigl)^2 - \bigl(\frac{\epsilon}{n-1}\bigr)^3\biggr)$$
$$\mathbb{E}(B_{i&lt;n}) = \frac{r\epsilon}{n-1}\sum_{k=0}^{i-1}\bigl(-\frac{\epsilon}{n-1}\bigr)^k$$

Recall the closed-form formula for a geometric series, shamelessly ripped off <a href="http://en.wikipedia.org/wiki/Geometric_series">Wikipedia</a>: 

$$\sum_{k=0}^{n-1}ar^k = a\frac{1-r^n}{1-r}$$. 

Since we're overloading variable names, let's be clear about what represents what: $$a_{wiki} \equiv \frac{r\epsilon}{n-1}$$, $$r_{wiki}\equiv \frac{-\epsilon}{n-1}$$, $$n_{wiki}\equiv i$$. That means our closed form for $$\mathbb{E}(B_{i&lt;n})$$ is $$\frac{r\epsilon}{n-1}\times\frac{1 - \bigl(\frac{-\epsilon}{n-1}\bigr)^i}{1 - \frac{-\epsilon}{n-1}}$$, which we can make prettier as $$\frac{r\epsilon}{n-1}\times\biggl(1 - \bigl(\frac{-\epsilon}{n-1}\bigr)^i\biggr)\times\frac{n-1}{n - 1 + \epsilon}$$ and reduce to a still hideous $$\frac{r\epsilon}{n-1+\epsilon}\biggl(1 - \bigl(\frac{-\epsilon}{n-1}\bigr)^i\biggr)$$. No lie, I remember it being less formidable looking in my notes, but they're at my desk, and I'm pretty sloppy with the algebraic manipulation, so I welcome any spot-checking.

So what does this mean? We have a formula for the expected number of respondents who break off at each index. Now we can at least use the Markov inequality to flag really egregious positional breakoff. Since the Markov inequality is very coarse, I have little doubt that researchers would be able to spot the cases it would find. That is, Markov doesn't give us much value for small data sets (note that it's still useful as a feature in an automated tool). 

In order to get tighter bounds, we are going to need to know more about the distribution. We can use Chebyshev's inequality if we know the variance. We can get a Chernoff-like bound by substituting into Chebyshev's Inequality the expectation for the moment generating function, if we have it. 

I think this process is similar to a Martingale, but I will have to spend more time thinking about this. Clearly the total expected breakoff is the same. The number of participants in the survey is expected to decay. It would be great to get some tight bounds on this. Once we can characterize breakoff for flat surveys, we have a prior on the role position plays and can use this information for diagnosing breakoff in more structured surveys. I haven't considered an equivalent analysis for question-based breakoff yet.

For now, in the analyses, we just stick with reporting the top locations and questions for abandonment/breakoff.

<h3>Research Question 3: Is SURVEYMAN able to identify random or inattentive respondents?</h3>

I am working on a more in-depth analysis to help answer this question. It's going to need a whole blog post to itself. We have some gold-standard data from Joe and company for the phonology survey, and there are some heuristics I can use for the prototypicality survey. Graphs forthcoming!

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1902</wp:post_id>
		<wp:post_date>2014-07-31 23:00:00</wp:post_date>
		<wp:post_date_gmt>2014-08-01 03:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>metrics-to-answer-our-research-questions</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Finally, a post devoted entirely to television: Scandal needs some Sandra Oh!</title>
		<link>http://blogs.umass.edu/etosch/2014/08/13/finally-a-post-devoted-entirely-to-television-scandal-needs-some-sandra-oh/</link>
		<pubDate>Thu, 14 Aug 2014 03:53:30 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2112</guid>
		<description></description>
		<content:encoded><![CDATA[I have pages of notes for a blog post about correlation, but since it's 11pm, let's put that off for now and have a little summer fun!

I had a chat today with <a href="http://people.cs.umass.edu/~mhjang/">Myung-ha Jang</a> about Grey's Anatomy and Scandal. Myung-ha's watched more Grey's, but hasn't tried Scandal yet. I unfortunately dove into Grey's at the height of its ridiculousness (<a href="https://www.youtube.com/watch?v=FyePUT2pbIY">the icicle that stabbed Cristina</a>, the romance of <a href="https://www.youtube.com/watch?v=ZJqAYUAJbTk">The Lady of the Lake</a> and  <a href="https://www.youtube.com/watch?v=qRnv36k8gis">poor homely Sonya</a>, and Izzie's hallucinations that turned out to be cancer). If it was about to get worse, I didn't want to see it.
 
Okay, you know what? That clip of the icicle stabbing Cristina begins with a great moment between friends:
[youtube]https://www.youtube.com/watch?v=FyePUT2pbIY[/youtube]
Yeah, it was a little judgey. Cristina isn't known for sugar-coating what she thinks. Even if it's not the smoothest move in real life, it's nice to see a character say what you're thinking. This is television fantasy land, where all your male coworkers are McSexy and you're surrounded by awesome, interesting women who speak their minds and are really f***ing good at their jobs. 

Now, I told Myung-ha I preferred Scandal because the story lines were eerily believable. However, the thing that's been nagging me has been that Olivia Pope has no friends.
<figure align="middle"> 
<img src="http://cdn.thedailybeast.com/content/dailybeast/articles/2013/10/04/kerry-washington-s-hair-evolution-as-olivia-pope-on-scandal/jcr:content/body/inlineimage_0.img.800.jpg/1380909813368.cached.jpg">
<figcaption>Sad Olivia</figcaption>
</figure>

While I think Scandal's plot lines are more believable, I find the relationships between characters sad and emotionally stunted. Of course, these characters are in a morally bankrupt universe of bad actors, whereas the Grey's folks are trying to do good in the world. In any case, it would be great if Sandra Oh guest starred as Cristina and had a long-distance besties relationship with Olivia. Apparently Cristina is in Switzerland now, and I think Olivia might have been sent there for her <strike>internal spy training</strike> secondary schooling. I wish they would ship in someone like her, rather than yet another emotionally crippled assassin/torture artist.

<figure align="middle">
<img src="http://3.bp.blogspot.com/-ZsR4EFfZQYY/UjHF8J8bQ6I/AAAAAAAAD58/lU5Y6TCNlFg/s400/Battle+buddy.gif">
<figcaption><a href="http://www.slate.com/blogs/browbeat/2013/08/15/sandra_oh_leaves_grey_s_anatomy_cristina_yang_her_character_changed_tv.html">Christina Yang: A+ Surgeon, A+ Friend</a></figcaption>
</figure>




]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2112</wp:post_id>
		<wp:post_date>2014-08-13 23:53:30</wp:post_date>
		<wp:post_date_gmt>2014-08-14 03:53:30</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>finally-a-post-devoted-entirely-to-television-scandal-needs-some-sandra-oh</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="greys-anatomy"><![CDATA[grey's anatomy]]></category>
		<category domain="post_tag" nicename="scandal"><![CDATA[scandal]]></category>
		<category domain="category" nicename="surveyman"><![CDATA[Television]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>When the needs of the few outweigh the needs of the many</title>
		<link>http://blogs.umass.edu/etosch/2014/09/17/when-the-needs-of-the-few-outweigh-the-needs-of-the-many/</link>
		<pubDate>Wed, 17 Sep 2014 12:55:16 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2150</guid>
		<description></description>
		<content:encoded><![CDATA[I'm not posting much these days, since I'm currently in California on an internship. I have some posts I'd like to make on notes from the summer, but posts will likely being sparse for the next few months.

However, I'm up thinking about something and thought I'd blog about it.

<h4>The facts</h4>

A few days ago I received an email from a TA at Prestigious Institution X, asking me to take down my <a href="http://www.cis.upenn.edu/~bcpierce/sf/current/index.html">Software Foundations</a> github repository because students were allegedly taking too much "inspiration" from it. My repository contains solutions to a chunk of the textbook, painstakingly solved mostly by me, with some input from Cibele Freire, logician. Posting solutions to a textbook released under the MIT license is, I believe, quite legal. Using those solutions to cheat in a class, however, is typically against an academic institution's honor code. Since there is no course even remotely similar to this at UMass (and in fact, few other resources exist to bootstrap a self-study plan for learning Coq and formal verification), I do not believe I am in violation of UMass' code of student conduct. The email implied none of these things, but I wanted to make sure I wasn't doing anything illegal or malicious in ignorance.

The academic world is a small one, so I consulted with my advisor. He suggested that I email the folks teaching the class, explain my reasons for having a public repo, and offer to make the repository private. The response to my email was indeed a request to make the repository private. I'm putting the previous link to the repository in this blog post, just in case someone comes searching for it and Google happens to index this post:

http://github.com/etosch/software_foundations

If you would like access, please email me directly. I am happy to discuss problems and give people access when it is appropriate to do so. There are plenty of exercises I have not completed, but would like to. I expect anyone who is interested enough to email me to abide by their school's honor code, if applicable.

<h4>The principle of the thing</h4>

Me taking down my repository does not dissuade cheating. It just buries cheating a little deeper. Dishonesty and plagiarism are pandemic in computer science. This shocked me when I started working in CS. In the humanities, the standards for attribution are quite explicit and stringently applied. Students must routinely attend lectures on academic honesty. I have never seen seen anything even remotely similar in computer science. 

Removing the repository from public view shifts the blame from the student plagiarist to the provider of information. At one point in my life, when I thought I would go to grad school for English Literature, I made writing samples publicly available. Would I be culpable for someone plagiarizing my work simply because I wished to promote my work? No one would ask Harold Bloom to stop publishing if a student plagiarized him in a Shakespeare class. Obviously I'm no Harold Bloom. 

The perpetrator (plagiarist) here is the one in the wrong.

It's a bit difficult for me to not be personally a little upset/frustrated by this. Not only does it devalue work in our field, but there's something maddening about students at Prestigious University X ruining it for the rest of us. I doubt they think that way, and I doubt anyone would ever put it to them that way. One of the earliest courses I took in computer science was SICP. It was not taught well. Coming from the humanities, where you actually do the reading you're assigned, I read the text. I also did nearly all of the exercises up to the chapter on metacircular evaluation. This self-study was extremely important to my education. When I got stuck on a problem I couldn't solve, I searched for the answer on the internet. Seeing solutions from people who were better programmers than I was made me a better programmer. 

Eventually someone else will post solutions and it won't matter much that I took mine down. My solutions are still cached out there and I know some rando forked my repository. <a href="https://github.com/search?l=Coq&amp;q=software+foundations&amp;type=Repositories&amp;utf8=%E2%9C%93">A quick search of github for "software foundations" and Coq returns about 80 results</a> and at least <a href="https://github.com/jwiegley/software-foundations">one</a> looks fairly complete.

<img src="http://media-cache-ak0.pinimg.com/736x/83/34/6b/83346bd1eb7bd2ef89706420cdec6080.jpg" alt="sad spock" />

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2150</wp:post_id>
		<wp:post_date>2014-09-17 08:55:16</wp:post_date>
		<wp:post_date_gmt>2014-09-17 12:55:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>when-the-needs-of-the-few-outweigh-the-needs-of-the-many</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="coq"><![CDATA[Coq]]></category>
		<category domain="category" nicename="formal-methods"><![CDATA[Formal Methods]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_06b13be003bfdf4c126ba67090c02c59</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Formal Methods in Practice</title>
		<link>http://blogs.umass.edu/etosch/2014/09/28/formal-methods-in-practice/</link>
		<pubDate>Mon, 29 Sep 2014 03:39:32 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2184</guid>
		<description></description>
		<content:encoded><![CDATA[Emery recently sent me a link to <a href="http://research.microsoft.com/en-us/um/people/lamport/tla/formal-methods-amazon.pdf">an experience report of a collection of engineers at Amazon who used formal methods to debug the designs of AWS systems</a>. It's pretty light on specifics, but they do report some bugs that model checking found and discuss their use of tools. They describe why they turned to formal methods in the first place (hint: it wasn't a love of abstract algebra and proof by induction) and how they pitched formal methods as their solution to a variety of problems at Amazon.

<h3>SAT solvers vs...Something Else</h3>
The report begins by noting that AWS is massive. As a result, statistically rare events occur with much higher frequency than the [human] users' gut notion of rare events. The first anecdote describes how one of the authors found some issues in "several" of the distributed systems he either designed or reviewed for Amazon. Although they all had extensive testing to ensure that the executed code would comply with the design, the author realized that these tests were inadequate if the design was wrong. Since he was aware that there were problems with edge cases in the design, he turned to formal methods to ensure that the system was actually exhibiting the behaviors he expected.

His first attempt at proving correctness of these systems was to use the <a href="http://alloy.mit.edu/alloy/">Alloy specification language</a>. Alloy's model checking is powered by a SAT solver. At a high level, SAT solvers model many states of the world (i.e., a program or system) as boolean propositions. If we wanted to talk about a distributed system, we might have propositions like <code>MasterServerLive</code> or <code>PacketSent</code>. These propositions have true or false values. Nothing that isn't explicitly defined can be described. 

If you are familiar with propositional and predicated logic, this should all be trivial! However, if you're not, you might be wondering what we mean by "nothing that isn't explicitly defined can be described." Consider this: I have 10 computers from 1995 in my basement. I split them into two clusters. Each has one queen and four drones (<a href="#master_slave_queen_drone">1</a>). I'd like to state some property <code>foo</code> about each queen. Since I have two queens, I would have two propositions: <code>FooQueen1</code> and <code>FooQueen2</code>. If I never need to use either of these propositions separately (that is, if they always appear in expressions looking like this: $$FooQueen1 \wedge FooQueen2$$), then I can just define one proposition, $$FooQueen$$. If, however, I want to say something like "<code>foo</code> is true for at least one of the queens," the I would need each of <code>FooQueen1</code> and <code>FooQueen2</code>.

Every time I want to assert some property of these two queens, I will have to create two propositions to express them. Furthermore, if I want to say something we'll call about a relationship <code>bar</code> between a queen and each of its drones, I will have create variables <code>BarQueen1Drone1</code>, <code>BarQueen1Drone2</code>, ..., <code>BarQueen2Drone4</code>. Any relationship or property that isn't described in this way cannot be reasoned about.

Anyone who has experience with first order logic might point out that there is a much more elegant solution to propositional logic -- we could always just describe these properties as predicates over a universe. Instead of defining two propositions <code>FooQueen1</code> and <code>FooQueen2</code>, we could say $$\forall x \in \lbrace computers \rbrace: Queen(x) \rightarrow Foo(x)$$, which states that if one of my basement computers is a queen, then <code>foo</code> holds. This expression gives us the possibility of adding another queen to the system without having to explicitly add more propositions. 

So far so good, right? Well, there's a another problem with using SAT solvers for distributed systems -- timing information matters. Some processes are redundant, and we need a way to describe concurrency. We could try to define predicates like <code>Before</code> and <code>Concurrently</code>, but again our model explodes in the number of "things" we need to specify. Tools like Alloy have no notion of timing information -- we have to tell it what temporal ordering means. 

The author found that Alloy simply was not expressive enough to describe the kinds of concurrency invariants he needed. He instead started using TLA+, a modelling language based on temporal logic, with logical "actions" (I think I learned about this in a class I took on logic in NLP, but I don't recall much about it and the Wikipedia page is empty). This language was expressive enough to describe the systems he wanted to model. Others at Amazon independently came to the same conclusion. Some found a language on top of TLA+, called PlusCal more helpful. 

<h3>Prejudice</h3>

There seems to be a lot of suspicion (if not prejudice) in the "real world" of formal methods. The authors decided to market their approaches as "debugging designs," rather than "proving correctness." This part was the real hook for me. I'll admit, formally proving things can be quite satisfying. Proving correctness or invariance can feel worthy in their own right. Unfortunately, not all problems may be worthy of this effort.

A major problem in the adoption of these tools is that they aren't marketed to the right people in the right way. Pitching formal methods as a way to "debug designs" has a lot of appeal to pragmatists -- it emphasizes the idea that designs themselves may be flawed, and these flaws may be difficult to find. The authors cite medical devices and avionics as fields that value correctness, because the cost of bugs is very high. However, in most software systems, the cost is much lower. A bug that hasn't been exposed in testing is probably quite infrequent. Its infrequency means that the cost of paying someone to ensure a program's correctness outweighs the cost of the consumer's exposure to the bug. What the authors realized, and what they pitched in this paper, was the idea that an event that only occurs 1 out of a billion executions is no longer a rare event in real software systems. There is a mismatch between the scale at which we test and develop and the scale at which we run production code. We may not be able to extrapolate that a program is correct or a system robust to failure on the basis of testing that are orders of magnitude below actual load. 

<h3>How this relates to me</h3>
Our pitch for SurveyMan and related tools is similar to the authors' pitch for using formal methods for large systems. We're starting from the assumption that the design of the survey (or experiment) may be flawed in some way. Rather than deploying costly studies, only to find out months or even years later that there were critical flaws, use formal methods instead. Our tools don't look like formal methods, but they are. We construct a model for the survey on the basis of its structure and a few invariants. We use this data to statically analyze and help debug the results of a survey. The author's model checking and our random respondent simulations are both used to check assumptions about the behavior of the system. 

<h3>Footnotes</h3>
<ol>
<li id="#master_slave_queen_drone">Technically, the "proper" terminology here is master/slave, but that has an inhumane, insensitive ick factor to it, so I've decided to start calling it a queen/drone architecture instead.</li>
</ol>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2184</wp:post_id>
		<wp:post_date>2014-09-28 23:39:32</wp:post_date>
		<wp:post_date_gmt>2014-09-29 03:39:32</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>formal-methods-in-practice</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="formal-methods"><![CDATA[Formal Methods]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>What happened to Mark Brendanawicz?</title>
		<link>http://blogs.umass.edu/etosch/2014/09/30/what-happened-to-mark-brendanawicz/</link>
		<pubDate>Tue, 30 Sep 2014 05:48:02 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2224</guid>
		<description></description>
		<content:encoded><![CDATA[Mark Brendanawicz was a character on NBC's Parks and Recreation for seasons 1 and 2. Before I get into any detail, I wanted to settle something...

<h3>What is up with that surname?</h3>
I don't know a ton about Polish names (Leslie refers to Mark as Polish in season one), but "Brendanawicz" sounds like a made-up name, like someone was making fun of a guy named Brendan at a party, forgot his last name, and ended up referring to him as "Brendan Brendanawicz." I found a <a href="http://www.pbs.org/pov/thesweetestsound/popularityindex.php">website</a> where you can check whether a name is in the top 150,000 names in the 2000 census. Here's what I get back on Brendanawicz:

<a href="http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-9.46.33-PM.png"><img src="http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-9.46.33-PM-300x116.png" alt="Screen Shot 2014-09-29 at 9.46.33 PM" width="300" height="116" class="aligncenter size-medium wp-image-2228" /></a>
 
As a point of comparison, I queried the surname of a long lost friend from high school:

<a href="http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-9.58.17-PM.png"><img src="http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-9.58.17-PM-300x103.png" alt="Screen Shot 2014-09-29 at 9.58.17 PM" width="300" height="103" class="aligncenter size-medium wp-image-2232" /></a>

That's only about 30,000 from the bottom! And now someone I knew at Brandeis:

<a href="http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-10.10.01-PM.png"><img src="http://blogs.umass.edu/etosch/files/2014/09/Screen-Shot-2014-09-29-at-10.10.01-PM-300x113.png" alt="Screen Shot 2014-09-29 at 10.10.01 PM" width="300" height="113" class="aligncenter size-medium wp-image-2238" /></a>

This is clearly not scientific, nor is it even pretending to be conclusive. Anyway, I guess appearing in the top 150,000 surnames is a bit of a wash. I still think it doesn't sound real.

<h3>The real point of this post</h3>
Mark begins the show as kind of a playboy and a bit of an ass. When he begins dating Ann, he starts to act less selfish and actually comes across as a good guy. In fact, he just keeps hitting home runs in the good guy department over the course of season two. Here are some reasons why Mark actually turns out to be a good guy:
<ul>
<li> Rather than behaving territorially, he shows compassion toward Andy (Ann's ex), who still hangs around, expecting Ann to take him back.</li>
<li> Unlike Andy, Mark understands that Ann is not a prize, but a person. He doesn't ever try to assert control over her, and he never stoops to the sitcom stereotype.</li>
<li> Rather than purchasing something useless and typical for Ann's birthday, he buys her a computer case. She had mentioned many weeks before her birthday that she needed one. He <em>listened to what she was saying,</em> wrote it down, and waited to surprise her with one. </li>
<li> He makes fun of himself. On Valentine's day, he dresses up and enacts all of the cliches, giving Ann a Teddy bear, chocolates, roses, etc., because he's never been in a relationship with someone on Valentine's day. Ann is not dressed up, she's hanging out on the couch, and she's just enjoying his company.</li>
<li> When Ann fails to introduce Mark as her boyfriend to a childhood friend she's always had a crush on, Mark doesn't get jealous, confront her with it, or otherwise act super dramatic. He has ever right to be hurt -- it was a glaring omission, and her feelings for this friend are obvious. However, he deals with it with grace, even going so far as to talk to Andy about it. </li>
</ul>

What I appreciated about Mark was that he was a good guy who, despite never having been in a serious relationship before, knew enough about human interaction to treat Ann like a person, and not just TV Girlfriend Stereotype. They had one of the healthiest, most respectful relationships I've ever seen in t.v. Sure, the way the writers began their relationship was kind of shitty, but somehow Ann and Mark worked. 

<img src="http://basementrejects.com/wp-content/uploads/2013/04/parks-and-recreation-season-1-4-the-banquet-leslie-knope-ann-perkins-mark-brendanawicz-paul-schneider-amy-poehler-rashida-jones-300x167.jpg" width="300" height="167" class="aligncenter" />

The way they portrayed Mark contrasted with Tom Haverford, who thinks he's starring in Entourage (a show I've never seen), or Ron Swanson, the manliest man or something. Tom hits on women so aggressively, but with such little tact and appeal, that he's turned himself into a joke (a joke who should also be disciplined for sexual harassment on a regular basis). Yeah, his character is funny, but what he does is NOT OKAY. Ron Swanson is Teddy Roosevelt, constantly performing masculinity. He's also quite entertaining, but definitely a caricature. 

<img src="http://media-cache-ak0.pinimg.com/736x/0f/84/0c/0f840c7c58db0e9c1b43642fc96783e3.jpg" width="500" height="334" class="aligncenter" />

I very much enjoy watching both Tom and Ron, but having Mark balance them out is a rare treat. You just don't see men behave with such grace, maturity, and self-respect on t.v., without being...well, Captain Adama. 

[caption width="500" align="alignnone"]<img src="http://media-cache-ak0.pinimg.com/736x/31/eb/a6/31eba6ad317a4c4fb6d41f0b849a29bf.jpg" width="500" height="332" class /> Are all the admirable men on T.V. admirals?[/caption]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2224</wp:post_id>
		<wp:post_date>2014-09-30 01:48:02</wp:post_date>
		<wp:post_date_gmt>2014-09-30 05:48:02</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>what-happened-to-mark-brendanawicz</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="surveyman"><![CDATA[Television]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>5158</wp:comment_id>
			<wp:comment_author><![CDATA[Maureen Coffey]]></wp:comment_author>
			<wp:comment_author_email>maureen.coffey@ratheryes.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.ratheryes.com</wp:comment_author_url>
			<wp:comment_author_IP>82.127.25.238</wp:comment_author_IP>
			<wp:comment_date>2014-10-30 09:49:43</wp:comment_date>
			<wp:comment_date_gmt>2014-10-30 13:49:43</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Well, Brendanawicz might be constructed as many Slav names are "...icz" being "the son of" and hence that younger Brendan might have had a (fore-) father with the same first name which then got appended with the filial postscript.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1414676983.3611319065093994140625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1414679552.6233561038970947265625;s:7:"message";s:45:"etosch changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>30633</wp:comment_id>
			<wp:comment_author><![CDATA[What ever happened to Emma Tosch?]]></wp:comment_author>
			<wp:comment_author_email>catjed@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>74.92.94.177</wp:comment_author_IP>
			<wp:comment_date>2015-08-13 17:20:49</wp:comment_date>
			<wp:comment_date_gmt>2015-08-13 21:20:49</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[heyyyyyyy!!!!!!!!!!!!!!!!!! I'm #122844! I miss you! Exclamation point!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1444399979.179871082305908203125;s:5:"event";s:15:"status-approved";s:4:"user";s:6:"etosch";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1439500849.929152965545654296875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>The one where I&#039;m a Debbie Downer</title>
		<link>http://blogs.umass.edu/etosch/2014/11/03/the-one-where-im-a-debbie-downer/</link>
		<pubDate>Mon, 03 Nov 2014 05:52:06 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2260</guid>
		<description></description>
		<content:encoded><![CDATA[I've spent the month of October ironing out some nasty kinks in my internship and prepping for my OOPSLA talk, so I'm pretty behind on blog posts (what else is new). This post isn't me finishing up anything I'd started earlier in the season. Instead, it's a quick look at a study I saw in my Google Scholar recommendations. 

The title of the work is 
<h4>
<a href="http://www.cs.dartmouth.edu/farid/downloads/publications/pus14.pdf">Does the Sun Revolve Around the Earth? A Comparison between the General Public and On-line Survey Respondents in Basic Scientific Knowledge</a></h4>

The conclusions are heartening -- the authors find (a) AMT workers are significantly more scientifically literate than their general population peers and (b) even with post-stratification, the observed respondents fare better. The point of the survey was an assessment of the generalizability of conclusions drawn from AMT samples. The authors note that past replications of published psychological studies were based on convenience samples anyway, so the bar for replication may have been lower. Polling that requires probability sampling is another animal.*

The authors describe their method; I'm writing my comments inline:

<blockquote>
Using a standard Mechanical Turk survey template, we published a HIT (Human Intelligence Task) titled "Thirteen Question Quiz" with the short description "Answer thirteen short multiple choice questions." The HIT was limited to respondents in the United States over the age of 18, to those that have a HIT Approval Rate greater than or equal to 95%, and to those that
have 50 or more previously approved HITs.
</blockquote>

Here is a screenshot of the standard AMT survey template:

<a href="http://blogs.umass.edu/etosch/files/2014/11/Screen-Shot-2014-11-02-at-9.08.27-PM.png"><img src="http://blogs.umass.edu/etosch/files/2014/11/Screen-Shot-2014-11-02-at-9.08.27-PM-300x215.png" alt="Screen Shot 2014-11-02 at 9.08.27 PM" width="300" height="215" class="aligncenter size-medium wp-image-2270" /></a>

We can safely assume that (a) the entire contents of the survey was displayed at once and (b) there was no randomization.

With Turker filters, the quality of the respondents was probably pretty good. The title conveys that the survey is easy. If they didn't use the keyword survey, they probably attracted a broader cohort than typical surveys.

I did a quick search with the query "mechanical turk thirteen question quiz" and came up with <a href="http://www.reddit.com/r/HITsWorthTurkingFor/comments/1y2ks3/us_thirteen_question_quiz_hany_farid_501min_95/">these</a> <a href="http://www.reddit.com/r/HITsWorthTurkingFor/comments/1y0vx1/us_thirteen_question_quiz_hany_farid_05040/">two</a> hits, which basically just describe the quiz as taking &lt;2mins to complete and being super easy fast cash (didn&#039;t see anything on TurkerNation or other forums). Speaking of fast cash, the methods section states:

<blockquote>
Respondents were paid $0.50 regardless of how they performed on the survey.
</blockquote>

Well, there you go.

The authors continue:
<blockquote>
Shown in Table 1 are the thirteen questions asked of each AMT respondent. The questions numbered 1-7 relate to scientific literacy (Miller, 1983, 1998). The questions numbered 8-10 provide basic demographic information (gender, age, and education). Interlaced within these ten questions are three simple control questions, which are used to ensure that the respondent reads each question. We published a total of 1037 HITs each of which were completed. 
</blockquote>

I'm going to interject here and say I think they meant that they posted 1 HIT having 1,037 assignments. Two things make me say this: (1) the authors did not mention anything about repeaters. If they posted 1,037 HITs, they should have problems with repeaters (unless they used the newly implemented <a href="http://mechanicalturk.typepad.com/blog/2014/07/new-qualification-comparators-add-greater-flexibility-to-qualifications-.html">NotIn</a> qualification, which essentially implement's AutoMan's exclusion algorithm) and (2) since the authors say they used a default template, that means they constructed the survey using AMT's web interface, rather than constructing it programmatically. It would be difficult to construct 1,037 HITs manually. 

<blockquote>
The total sample size was decided upon before publishing the HITs, and determined as a number large enough to warrant comparison with the GSS sample.
</blockquote>

That sounds fair.

<blockquote> 
Of the completed HITs, 23 (2.2%) were excluded because the respondent either failed to answer all of the questions, or incorrectly answered one or more of the simple control questions.
</blockquote>

There were three control questions in total. The probability of a random respondent answering all three correctly is 0.125, which is not small. Conversely, the control questions may not have been as clear as the authors thought (more on this later).

Continuing:

<blockquote> 
In half of the HITs, Question 3 was accompanied by a simple illustration of each option (Earth around Sun
and Sun around Earth), to ensure that any incorrect responses were not due to confusion caused by the wording of the question, which was identical to the GSS ballot wordings.
</blockquote>

First off, the "half of the HITs" not makes me think that they had two hits, each assigned approximately one half of the 1,037 responents. 

Secondly, it's good to know that the wording of the experimental questions was identical to the comparison survey. I am assuming that the comparison survey did not have control questions, since this would be far less of a concern for a telephone survey. However, I don't know and am postponing verifying that in an attempt to actually finish this blog post. :)


<blockquote>
However, the illustration did not affect the response accuracy, so we report combined results for both survey versions throughout. A spreadsheet of the survey results are included as Supplemental Material.
</blockquote>

There is not a supplemental material section on this version of the paper (dated Oct. 9, 2014 and currently in press for the "Public Understanding of Science" journal).  I checked both author's webpages and there was not a supplemental material link anywhere obvious. I suppose we'll have to wait for the article (or email them directly). 

Now for the survey questions themselves and the heart of my criticism:

 [table id=7 /]

You can see essentially this table in the paper. I added the last column, since I couldn't get the formatting in the table to work. I have two main observations about the questions:

<ol>
	<li>The wording of the control questions isn't super clear. Strawberries are green for most of their "lives" and only turn red upon maturing. Also, a five hour day could mean five hours of sunlight, a five hour work-day, or some other ambiguous unit of time.</li>
	<li>More importantly though, the design contains ONE WEIRD FLAW -- A respondent could "Christmas tree" the response and get all three control questions correct! (Although "only" four out of the seven real questions would be correct.) This makes me wonder what effect randomization would have on the total survey. </li>
</ol>


While I'd love to believe that Americans are more scientifically literate than what's generally reported (or even believe that AMT workers are more scientifically literate), I do see some flaws in the study and would like to see a follow-up study for verification.




 

<small>
*While traditionally true, the domain of non-representative polls is currently hot stuff. I've only seen Gelman &amp; Microsoft's work on XBox polls, which used post-stratification and hierarchical models to estimate missing data. As far as I understand it (and I'm not claiming to understand it well), this work relies on two major domains of existing knowledge about the unobserved variables (1) the proportion of the unobserved strata and (2) past behavior/opinions/some kind of prior on that cohort. So, if you're gathering data via XBox for election polling and as a consequence have very little data on what elderly women think, you can use your knowledge about their representation in the current population, along with past knowledge about how they respond to certain variables, to predict how they will respond in this poll. If you are collecting data on a totally new domain, this method won't work (unless you have a reliable proxy). I suspect it also only works in cases where you suspect there to be reasonable stability in a population's responses -- if there is a totally new question in your poll that is unlike past questions and could cause your respondents to radically shift their preferences, you probably couldn't use this technique. I think a chunk of AMT work falls into this category, so representative sampling is a legitimate concern.</small>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2260</wp:post_id>
		<wp:post_date>2014-11-03 00:52:06</wp:post_date>
		<wp:post_date_gmt>2014-11-03 05:52:06</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-one-where-im-a-debbie-downer</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Repost: Tries for typeclass lookup</title>
		<link>http://blogs.umass.edu/etosch/2014/11/30/repost-tries-for-typeclass-lookup/</link>
		<pubDate>Sun, 30 Nov 2014 21:03:29 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2298</guid>
		<description></description>
		<content:encoded><![CDATA[Over at <a href="http://ezyang.tumblr.com/" title="ezyang's research log">ezyang's research log</a>, Edward reports on SPJ's question of <a href="http://ezyang.tumblr.com/post/102771565197/type-class-lookup-with-tries">whether tries would improve typeclass lookup</a>. There are no comments yet, but it would be nice to see some discussion!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2298</wp:post_id>
		<wp:post_date>2014-11-30 16:03:29</wp:post_date>
		<wp:post_date_gmt>2014-11-30 21:03:29</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>repost-tries-for-typeclass-lookup</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Alternate Backends for PLASMA Crowdsourcing Tools</title>
		<link>http://blogs.umass.edu/etosch/2014/12/20/alternate-backends-for-plasma-crowdsourcing-tools/</link>
		<pubDate>Sun, 21 Dec 2014 04:59:23 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2304</guid>
		<description></description>
		<content:encoded><![CDATA[Although in practice AutoMan and SurveyMan were both designed to make their backends pluggable, we have yet to implement an alternate backend for either because there simply aren't any AMT competitors out there. There are plenty of crowdsourcing websites, but none are as programmable as AMT and few are as general. That is to say, all competitors appear to offer specialized labor markets and/or be designed for specialized work.  

A known problem with the labor market on Amazon is that, even if you pay your workers minimum wage based on time actually spent on a task, they spend a significant amount of time searching for tasks. There are websites set up to facilitate this process, but it's still time spent searching for work, instead of actually working. A major subfield of alternate approaches involves extracting work either voluntarily, or in contexts where non-monetary compensations make sense. <a href="http://www.ipeirotis.com/wp-content/uploads/2014/01/fp267-ipeirotis.pdf">Quizz</a> uses Google's advertising system to embed knowledge-mining quizzes in with its usual ads. Other approaches substitute consumer marketing tasks or questions for paywalls. In both cases, users are motivated by something other than payment.

I've been wondering for a while whether <a href="http://en.wikipedia.org/wiki/File:Thefacebook.png"><code>thefacebook</code></a> would be a good platform for our software. Although the general understanding is that <a href="http://journal.sjdm.org/10/10630a/jdm10630a.html">respondents are anonymized</a>, <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2228728">but we know this is not true</a>. Researchers have assumed that workers are independent. Recent work <a href="http://research.microsoft.com/en-us/people/mlg/">out of MSR</a> has found that some Indian workers actually collaborate on tasks. For these reasons, I think Facebook would be a perfectly reasonable alternate platform for crowd sourcing. In fact, I believe that Facebook is a <em>better</em> platform for crowdsourcing, since it overcomes ones of the major shortcomings of AMT -- people are already hanging out there*. Rather than appeal to a populace that is explicitly looking for work, sometimes as <a href="http://www.behind-the-enemy-lines.com/2010/03/new-demographics-of-mechanical-turk.html">a primary source of income</a>, we would like to instead use a Facebook to tap into people's excess capacity**.

Since Facebook doesn't currently have a crowdsourcing interface, could we mock up a substitute using what's already available? Amazon currently handles listing, pool management, payment, presentation, and offers a sandboxed version of AMT for testing. A minimal implementation would involve hosting our own *Man servers and just using Facebook advertising to recruit workers. However, this diverts the user away from the Facebook ecosystem, which defeats the purpose of using Facebook in the first place (for example, we could just as easily use Google AdWords instead). 

To keep users in the ecosystem, we could write a SurveyMan app***. I looked into this briefly, and while it isn't as integrated into the main Facebook experience as I'd want, it's closer than routing users to an outside website. We could use Facebook advertising to do the initial recruitment and then use wall updates to bootstrap that process. If Facebook advertising provided a way to target ads to particular demographics, we would have a better time with bias in our sample. 

<code>
* Admittedly, I am not a regular user of <code>thefacebook</code>. I've heard the "so and so spends their whole day on facebook" complaint, but I really don't know how common this is. Consequently, this post is predicated on the idea that <code>thefacebook</code> is a place where people spend a lot of time, not doing work. I have heard that this is less the case since mobile became ubiquitous.

** TBH, I think the cult of excess capacity is unethical, but for the sake of capitalism and this blog post, let's assume it isn't. I will save a discuss of ethics and excess capacity later.

** Word on the street is that no one actually uses these things anyway...
</code>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2304</wp:post_id>
		<wp:post_date>2014-12-20 23:59:23</wp:post_date>
		<wp:post_date_gmt>2014-12-21 04:59:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>alternate-backends-for-plasma-crowdsourcing-tools</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="amt"><![CDATA[amt]]></category>
		<category domain="post_tag" nicename="crowdsourcing"><![CDATA[crowdsourcing]]></category>
		<category domain="post_tag" nicename="experiman"><![CDATA[experiman]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>What code review should actually be like (and related problems with AI)</title>
		<link>http://blogs.umass.edu/etosch/2014/12/18/what-code-review-should-actually-be-like-and-related-problems-with-ai/</link>
		<pubDate>Thu, 18 Dec 2014 17:18:24 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2310</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://cs.umass.edu/~jfoley">John</a> sent me a blog post about a month ago that was already old when he saw it. I thought I'd post about it because I see a larger issue with how (lay)people understand AI in it.

The post is <a href="http://stevehanov.ca/blog/index.php?id=145">here</a>. It contains some musings about how to measure sortedness of a list.

I'm going to walk through the problems I see with this post:

<blockquote>
How do you measure the "sortedness" of a list? There are several ways. In the literature this measure is called the "distance to monotonicity" or the "measure of disorder" depending on who you read. It is still an active area of research when items are presented to the algorithm one at a time. In this article, I consider the simpler case where you can look at all of the items at once
</blockquote>

What literature? Links? My CLRS algorithms book is currently on loan to a labmate, so I can't search its index, but I'm pretty sure the average computer science student would be introduced to it through that route. A quick google search of "distance to monotonicity" gives the following results in Google:

<a href="http://blogs.umass.edu/etosch/files/2014/12/Screen-Shot-2014-12-18-at-10.56.58-AM.png"><img src="http://blogs.umass.edu/etosch/files/2014/12/Screen-Shot-2014-12-18-at-10.56.58-AM-300x193.png" alt="Screen Shot 2014-12-18 at 10.56.58 AM" width="300" height="193" class="aligncenter size-medium wp-image-2312" /></a>

and the following in Bing:
<a href="http://blogs.umass.edu/etosch/files/2014/12/Screen-Shot-2014-12-18-at-11.00.34-AM.png"><img src="http://blogs.umass.edu/etosch/files/2014/12/Screen-Shot-2014-12-18-at-11.00.34-AM-300x263.png" alt="Screen Shot 2014-12-18 at 11.00.34 AM" width="300" height="263" class="aligncenter size-medium wp-image-2316" /></a>

A quick review of the links verifies what the author says (note that I didn't get anything meaningful for "measure of disorder," so links there would have been good). 

The next paragraph mention's Kendall's distance. I thought I'd never heard of it, so I searched for it. It appears that the author was basically talking about Kendall's tau, which I have heard of/used. Kendall's tau is used widely for inter-annotator agreement and relevance judgements. These measures are widely used in linguistics, NLP, information retrieval and extraction, etc. Its application to sorting lists isn't quite right -- tau was designed for ordinal data, where the distance between ranks is not something we can really quantify. Consider its use in search: it's clear that the Wikipedia page when searching with the query "Kendall's tau" should be more relevant than a domain-specific paper that uses Kendall's tau, but it's not clear whether there is a meaningful measure of how much more relevant Wikipedia is. Furthermore, there is no notion of inherent distance between the Wikipedia hit and the paper. If these are the only two documents on the web, then Wikipedia should be ranked first and the paper should be ranked second. However, if we introduce a document from a statistics class taught at University of X, it should probably be ranked second, and the paper third. Conversely, if we're sorting a list of integers, we know that 2 will always following 1 and that we cannot introduce any numbers that ought to be ranked between 1 and 2 without breaking out of the set of integers. This brings us to the next problem with the post.

The problem states it is about the sortedness of lists, but what it's really about is the sortedness of compact lists. I think this is what the author is trying to say when he critiques edit distance and longest increasing subsequence: "A drawback of this method is its large granularity. For a list of ten elements, the measure can only take the distinct values 0 through 9." I don't recall LIS having a problem with duplicates, and I also don't remember it requiring compactness. In fact, I'm pretty sure there is a O(n log n) algorithm for any sequences of items that has a comparator relation defined on it. Conversely, the metric the author proposes does not have these properties. Let's take a closer look:

<blockquote>
Here, I propose another measure for sortedness. The procedure is to sum the difference between the position of each element in the sorted list, x, and where it ends up in the unsorted list, f(x). We divide by the square of the length of the list and multiply by two, because this gives us a nice number between 0 and 1. Subtracting from 1 makes it range from 0, for completely unsorted, to 1, for completely sorted.

$$1 - \frac{2}{N^2}\sum_{i=1}^N \lvert f(x_i) - x_i \rvert $$
</blockquote>

The $$\lvert f(x_i) - f(x_1) \rvert$$ part looks okay (for now -- it' really not, but I'll get to that below). The distance between two indices (where the item is and where it should be) is in one dimension, and direction doesn't matter. Presumably $$N$$ is the size of the list. The sum of all these distances would be the total distance moved <em>if we were copying to another location</em>. Why isn't this the distance if we perform the operations in place? Because we are going to need to put the displaced value somewhere. Consider the following two lists: [2,1,4,3,5,6] and [6,2,3,4,5,1]. Using the above formula, we see that the first is considered more sorted than the second. However, the first requires more swaps to sort than the second. One could argue that we are simply not interested in swaps as a cost unit in this context. However, in a post about sortedness, this seems like a foolish argument to make (and if the author wanted to make the argument in terms of entropy and argue that the distance of the swap matters more, then he should have focused on this instead).

In any case, $$\lvert f(x_i) - f(x_1) \rvert$$ makes some sense in a particular context. The sum also makes sense in a particular context (copying). However, it is not clear at all why we are normalizing by $$\frac{2}{N^2}$$. When computing the cost of comparison operations that are not symmetric, it's common to use a double-nested loop to compare each item with each of the others. If we were to compare every item we, we would have $$N^2$$ operations. If comparing against oneself is a waste of time, then every item only compares against every other item, and we have $$N(N-1)$$ operations. If we only need to compare in one direction (e.g., if we use a symmetric operation, like absolute value), then we can cut these comparisons in half: $$\frac{N^2 - N}{2}$$. I could use see someone using the total number of operations as a normalizing factor. Furthermore, someone could argue that the exact number of comparisons are not needed, only an upper bound, so $$\frac{N^2}{2}$$ is good enough. This is where I believe the normalizing factor of $$\frac{2}{N^2}$$ comes from. I do not however think it is justified, since it is still based on the number of comparison and there are better metrics out there. 

Finally, we subtract this quantity from one, since lower is better (but we humans like to think more is better). 

Now for an analysis of the code. 

First of all, the code does not actually match the formula. $$x_i$$ in the formula is the target location of the item. However, the $$x_i$$ in the code (see where enumerate is called: $$x_i$$ is called <code>element</code>) is the value of the item in the list itself. This means that the author is expecting us to only be sorting numbers. Furthermore, subtracting the value from the index in which it appears confirms my suspicion that this approach only works for a very small subset of lists. In fact, the code restricts the lists that this technique sorts even further by assuming that they start at 1. That is, a sorted list of [2,3,4,5,6] cannot have a score of 1!

My second problem with this approach is that the solution to the problem is encoded directly in the fitness function! You could just copy the list out to the appropriate order. The whole point of search-based AI techniques like GAs is that you "know it when you see it," but you cannot generate the appropriate solution yourself (or doing so would be costly).

<h3>The Meta Part</h3>
So first of all, why am I picking on this guy? I am a PhD student in computer science and he is putting himself out there thinking about these problems. My worry is that people's eyes tend to glaze over when it comes to math. I admit, when I first saw this blog post, I didn't read carefully and just assumed it was a neat insight. I think it's important to play around with these ideas, but it's also important to not accept what someone else has reasoned about without questioning. The blog post begins with a summary of the problem and even notes that a particular formulation of the problem is an active area of research. I worry this has the effect of setting up the reader to believe the author is an authority on the subject.

As for the parenthetical part of the title of my post, this guy's post doesn't discuss AI at all, but it does present a short implementation of a genetic algorithm for sorting a list using the metric the author defines. The reason why I'm framing this post as a problem with AI is because I think this post illuminates major mistakes people when applying AI.

Now, I don't meant to take the author of the post to task. It's just a one-off blog post that was probably fun to write. However, I do think it's illustrative of how techniques from AI and statistics are presented as if they can be used off the shelf as a black box, when in fact they are only appropriate for specific problems. Certainly there are things we could be doing to ameliorate this mismatch. The fact that people are "doing it wrong" is not entirely their fault and we should be doing more to make these techniques "safer." We did this in programming languages, with static typing and static analysis tools. It's time to start doing it in AI.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2310</wp:post_id>
		<wp:post_date>2014-12-18 12:18:24</wp:post_date>
		<wp:post_date_gmt>2014-12-18 17:18:24</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>what-code-review-should-actually-be-like-and-related-problems-with-ai</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Logic and Probability</title>
		<link>http://blogs.umass.edu/etosch/2014/12/21/logic-and-probability/</link>
		<pubDate>Sun, 21 Dec 2014 20:52:09 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2354</guid>
		<description></description>
		<content:encoded><![CDATA[When I first started at UMass, I had effectively no background in statistics or probability. So, when I was taking the first course in the graduate stats sequence, I tried to frame what I was learning in terms of things I already understood. When I saw the conditional probability $$\mathbb{P}(Y\;\vert\; X)$$, I couldn't help but think:

$$\begin{array}{|l} X \\ \hline \vdots \\ Y \\ \hline \end{array}\\ X \rightarrow Y$$

Assumption seems to be a close analogy of observation, and if we analyze each construct operationally, they both have a strict order (i.e., observe/assume $$X$$, then derive/calcuate the probability of $$Y$$). Both hold $$X$$ fixed in some way for part of the calculation. Suppose we then say that $$X$$ implies $$Y$$ with some probability $$p$$. If we denote this as $$X \overset{p}{\rightarrow} Y$$, then we have some equivalence relation where $$X \overset{p}{\rightarrow} Y \equiv \mathbb{P}(X\rightarrow Y) = p \equiv \mathbb{P}(Y\;\vert\;X) = p$$. 

Since $$X \overset{p}{\rightarrow} Y$$ is just normal logical implication, with a probability attached, we should be able to use the usual rewrite rules and identities (after all, what's the point of modeling this as a logic if we don't get our usual identities, axioms, and theorems for free?). In classical logic, implication is short for a particular instance of disjunction: $$X \rightarrow Y \hookrightarrow \neg X \vee Y$$. We can then rewrite our probabilistic implication as $$\neg X \overset{p}{\vee} Y$$ and say $$\mathbb{P}(\neg X \vee Y) = p \equiv \mathbb{P}(\neg X \cup Y) = p$$.

Similarly, we want to have the usual rules of probability at our disposal, so by the definition of conditional probabilities, $$\mathbb{P}(Y\;\vert\; X) = \frac{\mathbb{P}(Y\;\cap\;X)}{\mathbb{P}(X)}$$. We can apply the above rewrite rule for implication to say $$\mathbb{P}(\neg X \cup Y) = p \equiv \frac{\mathbb{P}(Y\;\cap\;X)}{\mathbb{P}(X)} = p$$. This statement must be true for all events/propositions $$X$$ and $$Y$$. 

Let's take a closer look at a subset of events: those where $$X$$ is independent of $$Y$$, denoted $$X \bot Y$$. Independence is defined by the property $$\mathbb{P}(Y\;\vert\; X)=\mathbb{P}(Y)$$. From this definition, we can also derive the identities $$\mathbb{P}(X\cap Y) = \mathbb{P}(X)\mathbb{P}(Y)$$ and $$\mathbb{P}(X\cup Y) = \mathbb{P}(X) + \mathbb{P}(Y)$$. Now we can rewrite $$\mathbb{P}(\neg X \cup Y) = p \equiv \frac{\mathbb{P}(Y\;\cap\;X)}{\mathbb{P}(X)} = p$$ as $$\mathbb{P}(\neg X) + \mathbb{P}(Y) = p \equiv \mathbb{P}(Y) = p$$. Since the relations on either side are equivalent, we can then substitute the right into the left and obtain $$\mathbb{P}(\neg X) = 0 \equiv \mathbb{P}(Y) = p$$. Although this looks a little weird, it's still consistent with our rules: we're just saying that when the events are independent (a notion that has no correspondence in our logical framework), the probability of the implication (i.e., the conditional probability) is wholly determined by $$Y$$ -- if $$X$$ happens (which it will, almost surely) then $$Y$$'s marginal is $$p$$. If $$X$$ never happens (which it won't), then $$Y$$ is 0, and the probability of the whole implication is 0. 

Now let's consider how this works over events that are not independent. For this example, let's gin up some numbers:

$$\mathbb{P}(X) = 0.1 \quad\quad \mathbb{P}(Y) = 0.4 \quad\quad \mathbb{P}(X \cap Y) = 0.09$$.

Note that $$X\not\bot\; Y$$ because $$\mathbb{P}(X\cap Y) \not = 0.04$$. Recall that because either $$X$$ or $$Y$$ are supersets of $$X\cap Y$$, their marginals cannot have a lower probability than their intersections. 

Now let's compute values for either side of the equivalence $$\mathbb{P}(\neg X \cup Y) = p \equiv \mathbb{P}(Y\;\vert\; X) = p$$. First, the conditional probability:

$$\mathbb{P}(Y\;|\; X) = \frac{\mathbb{P}(Y\cap X)}{\mathbb{P}(X)} = \frac{0.09}{0.1} = 0.9 = p$$

Now for the left side of the equivalence, recall the definition of union: 
$$\mathbb{P}(\neg X \cup Y) = \mathbb{P}(\neg X) + \mathbb{P}(Y) - \mathbb{P}(\neg X \cap Y)$$. 

Since we don't have $$\mathbb{P}(\neg X \cap Y)$$ on hand, we will need to invoke the law of total probability to compute it: $$\mathbb{P}(\neg X \cap Y) = \mathbb{P}(Y) - \mathbb{P}(X\cap Y) = 0.4 - 0.09 = 0.31$$.

We can now substitute values in: 
$$\mathbb{P}(\neg X \cup Y) = 0.9 + 0.4 - 0.31 = 0.99 = p$$. 

Now our equivalence looks like this:
$$\mathbb{P}(\neg X \cup Y) = 0.99 \equiv \mathbb{P}(Y\;\vert\; X) = 0.9$$,
which isn't really much of an equivalence at all.

So what went wrong? Clearly things are different when our random variables are independent. Throughout the above reasoning, we assumed there was a correspondence between propositions and sets. This correspondence is flawed. Logical propositions are atomic, but sets are not. The intersection of non-independent sets illustrates this. We could have identified the source of this problem earlier, had we properly defined the support of the random variables. Instead, we proceeded with an ill-defined notion that propositions and sets are equivalent in some way.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2354</wp:post_id>
		<wp:post_date>2014-12-21 15:52:09</wp:post_date>
		<wp:post_date_gmt>2014-12-21 20:52:09</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>logic-and-probability</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="formal-methods"><![CDATA[Formal Methods]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Branch Paradigm Propagation Rules</title>
		<link>http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=640</link>
		<pubDate>Wed, 19 Mar 2014 04:47:46 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=640</guid>
		<description></description>
		<content:encoded><![CDATA[[["My Branch Paradigm(s)","Parent's Branch Paradigm(s)","Sibling's Branch Paradigm(s)","Children's Branch Paradigm(s)","Additional Sibling Constraint"],["NONE","NONE, ONE","NONE, ONE, SAMPLE","NONE, SAMPLE","If my parent is ONE, then only one of my siblings is ONE."],["SAMPLE","NONE, ONE","NONE, ONE, SAMPLE","--","If my parent is ONE, then only one of my siblings is ONE.\n\nI cannot have children."],["ONE","ONE","NONE, SAMPLE","ONE, SAMPLE, NONE","If I immediately contain the branch question, then my children cannot be ONE. Otherwise, exactly one of my descendants must be ONE. "]]]]></content:encoded>
		<excerpt:encoded><![CDATA[LATTICE?]]></excerpt:encoded>
		<wp:post_id>640</wp:post_id>
		<wp:post_date>2014-03-19 04:47:46</wp:post_date>
		<wp:post_date_gmt>2014-03-19 04:47:46</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>closed</wp:ping_status>
		<wp:post_name>branch-paradigm-propagation-rules</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>tablepress_table</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>_tablepress_export_table_id</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_options</wp:meta_key>
			<wp:meta_value><![CDATA[{"last_editor":20775,"table_head":true,"table_foot":false,"alternating_row_colors":true,"row_hover":true,"print_name":false,"print_name_position":"above","print_description":false,"print_description_position":"below","extra_css_classes":"","use_datatables":true,"datatables_sort":false,"datatables_filter":false,"datatables_paginate":false,"datatables_lengthchange":true,"datatables_paginate_entries":10,"datatables_info":true,"datatables_scrollx":false,"datatables_custom_commands":""}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_visibility</wp:meta_key>
			<wp:meta_value><![CDATA[{"rows":[1,1,1,1],"columns":[1,1,1,1,1]}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=854</link>
		<pubDate>Sun, 23 Mar 2014 21:47:54 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=854</guid>
		<description></description>
		<content:encoded><![CDATA[[["","","","",""],["","","","",""],["","","","",""],["","","","",""],["","","","",""]]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>854</wp:post_id>
		<wp:post_date>2014-03-23 21:47:54</wp:post_date>
		<wp:post_date_gmt>2014-03-23 21:47:54</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>closed</wp:ping_status>
		<wp:post_name>854</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>tablepress_table</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>_tablepress_export_table_id</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_options</wp:meta_key>
			<wp:meta_value><![CDATA[{"last_editor":20775,"table_head":true,"table_foot":false,"alternating_row_colors":true,"row_hover":true,"print_name":false,"print_name_position":"above","print_description":false,"print_description_position":"below","extra_css_classes":"","use_datatables":true,"datatables_sort":true,"datatables_filter":true,"datatables_paginate":true,"datatables_lengthchange":true,"datatables_paginate_entries":10,"datatables_info":true,"datatables_scrollx":false,"datatables_custom_commands":""}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_visibility</wp:meta_key>
			<wp:meta_value><![CDATA[{"rows":[1,1,1,1,1],"columns":[1,1,1,1,1]}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Analyses</title>
		<link>http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=1046</link>
		<pubDate>Tue, 25 Mar 2014 13:23:05 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=1046</guid>
		<description></description>
		<content:encoded><![CDATA[[["","Ordered","Unordered","Ordered-Unordered\n",""],["<a href=\"#correlation\">Correlation<\/a>","Spearman's rho","Cramer's V","Cramer's V",""],["<a href=\"#order\">Question Order Bias<\/a>","Mann-Whitney U-Test","Chi-Squared Test","N\/A",""],["<a href=\"#wording\">Question Wording Bias<\/a>","Mann-Whitney U-Test","Chi-Squared Test","N\/A",""],["<a href=\"#breakoff\">Breakoff<\/a>","Nonparametric Bootstrap","Nonparametric Bootstrap","N\/A",""],["<a href=\"#adversaries\">Bad Actors<\/a>","Nonparametric Bootstrap on Empirical Entropy","Nonparametric Bootstrap on Empirical Entropy","N\/A",""]]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1046</wp:post_id>
		<wp:post_date>2014-03-25 13:23:05</wp:post_date>
		<wp:post_date_gmt>2014-03-25 13:23:05</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>closed</wp:ping_status>
		<wp:post_name>analyses</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>tablepress_table</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>_tablepress_export_table_id</wp:meta_key>
			<wp:meta_value><![CDATA[3]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_options</wp:meta_key>
			<wp:meta_value><![CDATA[{"last_editor":20775,"table_head":true,"table_foot":false,"alternating_row_colors":true,"row_hover":true,"print_name":false,"print_name_position":"above","print_description":false,"print_description_position":"below","extra_css_classes":"","use_datatables":true,"datatables_sort":true,"datatables_filter":true,"datatables_paginate":true,"datatables_lengthchange":true,"datatables_paginate_entries":10,"datatables_info":true,"datatables_scrollx":false,"datatables_custom_commands":""}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_visibility</wp:meta_key>
			<wp:meta_value><![CDATA[{"rows":[1,1,1,1,1,1],"columns":[1,1,1,1,1]}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Frequency of the number options per question in Sara&#039;s wage survey.</title>
		<link>http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=1196</link>
		<pubDate>Wed, 07 May 2014 20:02:40 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=1196</guid>
		<description></description>
		<content:encoded><![CDATA[[["#\/Options","#\/Questions \n","#\/Options","#\/Questions "],["2\n","8","7","2"],["3","5","8","1"],["4","16","9","1"],["5","2","10","1"],["6","2","97","1"]]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1196</wp:post_id>
		<wp:post_date>2014-05-07 16:02:40</wp:post_date>
		<wp:post_date_gmt>2014-05-07 20:02:40</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>closed</wp:ping_status>
		<wp:post_name>frequency-of-the-number-options-per-question-in-saras-wage-survey</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>tablepress_table</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>_tablepress_export_table_id</wp:meta_key>
			<wp:meta_value><![CDATA[4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_options</wp:meta_key>
			<wp:meta_value><![CDATA[{"last_editor":20775,"table_head":true,"table_foot":false,"alternating_row_colors":true,"row_hover":true,"print_name":false,"print_name_position":"above","print_description":false,"print_description_position":"below","extra_css_classes":"","use_datatables":true,"datatables_sort":false,"datatables_filter":false,"datatables_paginate":false,"datatables_lengthchange":true,"datatables_paginate_entries":10,"datatables_info":false,"datatables_scrollx":false,"datatables_custom_commands":""}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_visibility</wp:meta_key>
			<wp:meta_value><![CDATA[{"rows":[1,1,1,1,1,1],"columns":[1,1,1,1]}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>prototypicality.csv</title>
		<link>http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=1438</link>
		<pubDate>Wed, 18 Jun 2014 13:01:57 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=1438</guid>
		<description></description>
		<content:encoded><![CDATA[[["BLOCK","QUESTION","OPTIONS","BRANCH","EXCLUSIVE","ORDERED","RANDOMIZE","parity","prototypicality","tag","FREETEXT","correlation"],["_1._1","How good an example of an odd number is the number 3?","Somewhat bad","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Good","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["_1._1","How well does the number 3 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Well","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["_1._1","How odd is the number 3?","Not very odd","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["_1._1","How odd is the number 3?","Not at all odd","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["_1._2","How good an example of an odd number is the number 241?","Bad","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Good","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["_1._2","How well does the number 241 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Well","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["_1._2","How odd is the number 241?","Not very odd","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["_1._2","How odd is the number 241?","Not at all odd","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","odd","low","twofortyone","","oddlow"],["_1._3","How good an example of an odd number is the number 2?","Bad","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Good","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["_1._3","How well does the number 2 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Well","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["_1._3","How odd is the number 2?","Not very odd","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["_1._3","How odd is the number 2?","Not at all odd","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","even","high","two","","evenhigh"],["_1._4","How good an example of an odd number is the number 158?","Bad","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Good","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["_1._4","How well does the number 158 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Well","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["_1._4","How odd is the number 158?","Not very odd","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["_1._4","How odd is the number 158?","Not at all odd","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","even","low","onehundredfiftyeight","","evenlow"],["_1._5","How good an example of an odd number is the number 7?","Bad","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Good","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["_1._5","How well does the number 7 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Well","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["_1._5","How odd is the number 7?","Not very odd","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["_1._5","How odd is the number 7?","Not at all odd","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","odd","high","seven","","oddhigh"],["_1._6","How good an example of an odd number is the number 4?","Bad","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Good","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["_1._6","How well does the number 4 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Well","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["_1._6","How odd is the number 4?","Not very odd","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["_1._6","How odd is the number 4?","Not at all odd","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","even","high","four","","evenhigh"],["_1._7","How good an example of an odd number is the number 465?","Bad","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Good","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["_1._7","How well does the number 465 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Well","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["_1._7","How odd is the number 465?","Not very odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["_1._7","How odd is the number 465?","Not at all odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtyfive","","oddlow"],["_1._8","How good an example of an odd number is the number 396?","Bad","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Good","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["_1._8","How well does the number 396 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Well","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["_1._8","How odd is the number 396?","Not very odd","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["_1._8","How odd is the number 396?","Not at all odd","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","even","low","threehundredninetysix","","evenlow"],["_1._9","How good an example of an odd number is the number 1?","Bad","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["_1._1","How good an example of an odd number is the number 3?","Bad","NEXT","TRUE","TRUE","TRUE","odd","high","three","","oddhigh"],["","","Good","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["_1._9","How well does the number 1 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["","","Well","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["_1._9","How odd is the number 1?","Not very odd","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["_1._9","How odd is the number 1?","Not at all odd","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","odd","high","one","","oddhigh"],["_1._10","How good an example of an odd number is the number 463?","Bad","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Good","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["_1._10","How well does the number 463 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Well","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["_1._10","How odd is the number 463?","Not very odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["_1._10","How odd is the number 463?","Not at all odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","odd","low","fourhundredsixtythree","","oddlow"],["_1._11","How good an example of an odd number is the number 6?","Bad","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Good","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["_1._11","How well does the number 6 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Well","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["_1._11","How odd is the number 6?","Not very odd","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["_1._11","How odd is the number 6?","Not at all odd","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","even","high","six","","evenhigh"],["_1._12","How good an example of an odd number is the number 730?","Bad","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Good","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["_1._12","How well does the number 730 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Well","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["_1._12","How odd is the number 730?","Not very odd","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["_1._12","How odd is the number 730?","Not at all odd","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","even","low","sevenhundredthirty","","evenlow"],["_1._13","How good an example of an odd number is the number 9?","Bad","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Good","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["_1._13","How well does the number 9 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Well","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["_1._13","How odd is the number 9?","Not very odd","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["_1._13","How odd is the number 9?","Not at all odd","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","odd","high","nine","","oddhigh"],["_1._14","How good an example of an odd number is the number 8?","Bad","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Good","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["_1._14","How well does the number 8 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Well","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["_1._14","How odd is the number 8?","Not very odd","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["_1._14","How odd is the number 8?","Not at all odd","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","even","high","eight","","evenhigh"],["_1._15","How good an example of an odd number is the number 827?","Bad","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Good","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["_1._15","How well does the number 827 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Well","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["_1._15","How odd is the number 827?","Not very odd","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["_1._15","How odd is the number 827?","Not at all odd","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","odd","low","eighthundredtwentyseven","","oddlow"],["_1._16","How good an example of an odd number is the number 532?","Bad","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Somewhat bad","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Somewhat good","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Good","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["_1._16","How well does the number 532 represent the category of odd numbers?","Poorly","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Somewhat poorly","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Somewhat well","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Well","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["_1._16","How odd is the number 532?","Not very odd","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Very odd","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["_1._16","How odd is the number 532?","Not at all odd","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Somewhat not odd","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Somewhat odd","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["","","Completely odd","NEXT","TRUE","TRUE","TRUE","even","low","fivehundredthirtytwo","","evenlow"],["2","What is your native tongue?","American English","","TRUE","FALSE","TRUE","","","","",""],["","","British English","","","","","","","","",""],["","","Indian English","","","","","","","","",""],["","","Other","","","","","","","","",""]]]]></content:encoded>
		<excerpt:encoded><![CDATA[prototypicality.csv]]></excerpt:encoded>
		<wp:post_id>1438</wp:post_id>
		<wp:post_date>2014-06-18 09:01:57</wp:post_date>
		<wp:post_date_gmt>2014-06-18 13:01:57</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>closed</wp:ping_status>
		<wp:post_name>prototypicality-csv</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>tablepress_table</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>_tablepress_export_table_id</wp:meta_key>
			<wp:meta_value><![CDATA[5]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_options</wp:meta_key>
			<wp:meta_value><![CDATA[{"last_editor":20775,"table_head":true,"table_foot":false,"alternating_row_colors":true,"row_hover":true,"print_name":false,"print_name_position":"above","print_description":false,"print_description_position":"below","extra_css_classes":"","use_datatables":true,"datatables_sort":true,"datatables_filter":true,"datatables_paginate":true,"datatables_lengthchange":true,"datatables_paginate_entries":16,"datatables_info":true,"datatables_scrollx":true,"datatables_custom_commands":""}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_visibility</wp:meta_key>
			<wp:meta_value><![CDATA[{"rows":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"columns":[1,1,1,0,0,0,0,0,0,0,0,0]}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>How to interpret entropy</title>
		<link>http://blogs.umass.edu/etosch/2015/05/20/how-to-interpret-entropy/</link>
		<pubDate>Wed, 20 May 2015 15:56:28 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2220</guid>
		<description></description>
		<content:encoded><![CDATA[<i>Note: This is a post that I started some time ago and have had in my todo list to finish for...maybe a year now? Apologies for the delay!</i>

We've argued that more entropy in a survey is better for detecting bad actors. The argument goes like this: A survey of 5 yes/no questions has (ignoring breakoff) 32 possible unique answers. The maximum entropy of this survey is 

$$-5\bigl(\frac{1}{2} \log_2(\frac{1}{2}) + \frac{1}{2} \log_2(\frac{1}{2}) \bigr) = -5\bigl(-\frac{1}{2} - \frac{1}{2}) = 5$$. 

This seems rather low. Clearly if we were to ask our usual 150 respondents to answer this survey, we could easily run into problems being able to tell the difference between good and bad actors. We've argued that as the length of the survey and the "width" (i.e., number of options a question has) increase, it's easier to catch random actors. However, we also know that especially long surveys can cause fatigue, making good respondents behave badly. 

<h3>How much entropy is enough?</h3>
We ran a couple of simulations to find out what the relationship was between entropy and accuracy. <a href="http://cs.umass.edu/~cibelemf">Cibele</a> and I ran a bunch of baseline analyses under idealized circumstances for our <a href="http://blogs.umass.edu/etosch/files/2015/05/main.pdf">machine learning final project</a>. In the project, we used our strongest adversary (the lexicographic respondent, <a href="http://blogs.umass.edu/etosch/2014/03/13/adversaries/">mentioned in a previous post</a>) as our model for honest respondents. We did this because we could then use an already-written module for a population of respondents who always gave fixed response set. Most of the analyses did well.

However, if we really want to know whether we can debug real surveys, we have to consider doing so under non-ideal circumstances. We consider a situation like the non-random respondent described in our <a href="http://blogs.umass.edu/etosch/2014/03/24/simulation-and-detecting-bugs-correlation/">simulator</a>, where these responses are mixed in with some random respondents. Extracting the non-random respondents from the random respondents is our goal, which is significantly easier when we use something like the lexicographic respondent, rather than the simulator's respondent. The goal of the machine learning project was in part to investigate the different approaches' robustness to bad actors. This post is about what actions a user might take in response to a survey that does not lend itself to detection of bad actors.

When we've asserted that more entropy is better, we weren't actually saying what we mean -- what we mean is that the <em>potential</em> for more entropy is better. That is, a larger space of possible options is better. Let's take a look at the empirical entropy of the survey plotted against the accuracy for surveys having 20%(red), 40%(purple), 60%(orange), and 80%(blue) bad actors. Each graph has 100 points, corresponding to 100 different surveys. The surveys were generated programatically, starting with 5 questions, each having 4 answer options, and increasing the number of questions and answer options incrementally. The dotted line is a baseline guess and corresponds to the dominant class.

<a href="http://blogs.umass.edu/etosch/files/2015/05/accuracy_entropy.png"><img src="http://blogs.umass.edu/etosch/files/2015/05/accuracy_entropy-300x129.png" alt="accuracy_entropy" class="aligncenter size-large" /></a>

Over all, we see the loess regression line correlate higher empirical entropy due to to survey structure (rather than the increase in the percentage of bots, which is not informative) with higher accuracy. However, this trend isn't as informative as it might appear: the upper bound of 100% and the lower bound of a naive classifier make the apparent trend less compelling. Note that when we have 20% bad actors, we observe much higher variance. Let's take a closer look at the data to see if there isn't something else going with this data.

<a href="http://blogs.umass.edu/etosch/files/2015/05/roc.png"><img src="http://blogs.umass.edu/etosch/files/2015/05/roc-300x129.png" alt="roc" width="300" height="129" class="aligncenter size-medium wp-image-2765" /></a>

The graphs flow down and to the right. The colors of the dots correspond to the empirical entropy; lighter is higher. All graphs use a scale from 0 to 400 bits. 

The first thing that jumps out with these graphs is that the false positive rate is very low. The classifiers are very conservative -- they rarely classify a bad actor as an honest respondent. However, there doesn't appear to be a clear trend between total entropy and either the false positive rate or the true positive rate.

Since our main argument is about the utilization of the search space, let's take a look at the relationship between the maximum possible entropy and the empirical entropy. Maximum possible entropy is a kind of resource we want to conserve. We prefer to have a large reservoir of it, but only use a small amount. Let's plot the ROC "curve" as before, but this time use the ratio of empirical entropy to the maximum possible entropy to color the points we observe:

<a href="http://blogs.umass.edu/etosch/files/2015/05/roc2.png"><img src="http://blogs.umass.edu/etosch/files/2015/05/roc2-300x129.png" alt="roc2" width="300" height="129" class="aligncenter size-medium wp-image-2769" /></a>

This looks a little better, but is hard to read and/or reason about. <a href="http://cs.umass.edu/~dbarowy">Dan</a> suggested plotting the entropy ratio against the accuracy, much as we plotted the entropy against the accuracy:

<a href="http://blogs.umass.edu/etosch/files/2015/05/accuracy-entropy-ratio.png"><img src="http://blogs.umass.edu/etosch/files/2015/05/accuracy-entropy-ratio-300x129.png" alt="accuracy-entropy-ratio" width="300" height="129" class="aligncenter size-medium wp-image-2721" /></a>

At the same entropy level (e.g., 0.85), our accuracy is lower for the scenario with 20% bots than for the scenario with 40% bots. Since the false positive rate is very low, this boost isn't coming from from a better classifier, since our greater number of bad actors is increasing the accuracy. Let's now take a look at just the entropy ratios plotted against precision:

<a href="http://blogs.umass.edu/etosch/files/2015/05/entropy_ratio_precision.png"><img src="http://blogs.umass.edu/etosch/files/2015/05/entropy_ratio_precision-300x129.png" alt="entropy_ratio_precision" width="300" height="129" class="aligncenter size-medium wp-image-2779" /></a>

<h3>Debugging</h3>
So what does this all mean? The end user should try to minimize the ratio of empirical entropy to maximum possible entropy. This can be done by adding more control questions, or padding existing questions with more options. Since we know most of the quality control techniques we use are robust to false positives, we focus on trying to detect true positives. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2220</wp:post_id>
		<wp:post_date>2015-05-20 11:56:28</wp:post_date>
		<wp:post_date_gmt>2015-05-20 15:56:28</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>how-to-interpret-entropy</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="crowdsourcing"><![CDATA[crowdsourcing]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Breakfast at Tiffany&#039;s is the Second Worst Film I have ever finished watching.</title>
		<link>http://blogs.umass.edu/etosch/2014/12/22/breakfast-at-tiffanys-is-the-second-worst-film-i-have-ever-finished-watching/</link>
		<pubDate>Mon, 22 Dec 2014 16:59:18 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2460</guid>
		<description></description>
		<content:encoded><![CDATA[Someday I will post a longer essay on everything that is wrong with this film and how it's hurting America and how everything is going to hell because all the kids internet all day. I watched it two days ago and am still irritated by the characters, the plot, and how everyone should STFU and get a real job and get off my lawn. The only cinematic sequence to more effectively evoke that strange combination of disgust and boredom was the interminable torture of Theon Greyjoy. 

On a score from "Changed my life" to "<a href="http://the-toast.net/2013/11/20/yes-you-can-even/">I've lost my ability to can</a>," Cibele was more forgiving than I and rated it a "I can't even." 

Also, for those who are wondering, the worst is a tie between <a href="http://en.wikipedia.org/wiki/Nazis_at_the_Center_of_the_Earth">Nazis at the Center of the Earth</a>* and <a href="http://en.wikipedia.org/wiki/Wanted_%282008_film%29">Wanted</a>. 

<small>
* I may have "fast-forwarded" through chunks of this film, so it isn't clear it's actually eligible for first place.
</small>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2460</wp:post_id>
		<wp:post_date>2014-12-22 11:59:18</wp:post_date>
		<wp:post_date_gmt>2014-12-22 16:59:18</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>breakfast-at-tiffanys-is-the-second-worst-film-i-have-ever-finished-watching</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="film"><![CDATA[Film]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Stats Review: Infinite sets</title>
		<link>http://blogs.umass.edu/etosch/2015/01/05/stats-review-infinite-sets/</link>
		<pubDate>Mon, 05 Jan 2015 16:53:16 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2480</guid>
		<description></description>
		<content:encoded><![CDATA[I'm looking over Casella and Berger's <a href="http://www.amazon.com/dp/0534243126/?tag=mh0b-20&amp;hvadid=3486843850&amp;ref=pd_sl_88k3cgtfa6_b">Statistical Inference</a> and reviewing some of the concepts. For the record, this book is exactly what you want if you need to take a statistics qualifying exam as a graduate student and not at all what a generalist will want. I actually enjoyed the material for the relatively clean overview it gave. There's depth, but not so much as to deter someone without a degree in mathematics. That said, I would not recommend this book for beginners. If you do want to slog through, get a supplementary book.

I'm currently looking at a section on page 4 of the second edition. There is a section that begins:

<blockquote>
The operations of union and intersection can be extended to infinite collections of sets as well If $$A_1, A_2, A_3, ...$$ is a collection of sets, all defined on a sample space $$S$$, then

$$\quad\bigcup_{i=1}^\infty A_i = \lbrace x \in S : x \in A_i \text{ for some } i\rbrace$$
$$\quad\bigcap_{i=1}^\infty A_i = \lbrace x \in S : x \in A_i \text{ for all } i \rbrace$$

For example, let $$S = (0, 1]$$ and define $$A_i = [(1/i), 1]$$. Then 

$$\bigcup_{i=1}^\infty A_i = \bigcap_{i=1}^\infty [(1/i), 1] =  \lbrace x \in (0,1] : x \in[(1/i), 1] \text{ for some } i \rbrace$$
$$\quad\quad\quad = \lbrace x \in (0, 1]\rbrace = (0,1];$$

$$\bigcap_{i=1}^\infty A_i = \bigcap_{i=1}^\infty[(1/i), 1] = \lbrace x \in (0, 1] : x \in [(1/i), 1] \text{ for all } i \rbrace$$
$$\quad\quad\quad = \lbrace x \in (0, 1] : x \in [1, 1]\rbrace = \lbrace 1 \rbrace$$    (the point 1)
</blockquote>

The above occurred after a discussion about countable and uncountable sets and proving theorems about sets from first principles (rather than Venn diagrams). If your eyes kind of glazed over while reading the above, no worries -- mine did, too. Actually, my reaction was worse than glazing over: I skimmed and thought I understood. However, as I started to tease apart what was written, I realized that there was much more going on here than I realized.

First of all, the preceding section describes reasoning about events that can be represented as finite sets, over finite sample spaces. This section builds on what we know to discuss the infinite case.

In our example, the sample space is the interval $$(0, 1]$$, which is defined over the reals. We are defining a <em>countably infinite</em> set of intervals, each denoted by some $$A_i$$. How do we know it's countably infinite? This is implied by the notation: we start at 1 and go to infinity. Therefore, there is a 1-1 correspondence with the natural numbers and thus the set of intervals is infinite. Let's take a look at some of the intervals (note that I can't draw lines, so imagine that the dashed line is actually connecting the endpoints and they're actually aligned):

$$A_1 : \underset{0}{\circ} \quad \quad \quad \quad \quad \quad \underset{1}{\bullet}$$
$$A_2 : \underset{0}{\circ} \quad \quad \quad \underset{\frac{1}{2}}{\bullet} \mbox{-------}\underset{1}{\bullet}$$
$$A_3 : \underset{0}{\circ} \quad \quad \underset{\frac{1}{3}}{\bullet} \mbox{-----------}\underset{1}{\bullet}$$
$$\vdots$$
$$A_\infty : \underset{0}{\bullet}\mbox{--------------------}\underset{1}{\bullet}$$

Now, the first statement says that the union of infinite subsets of a sample space should be equal to the sample space: $$\lbrace x \in (0,1] : x \in[(1/i), 1] \text{ for some } i \rbrace$$. The statement on the far left of the set notation, $$x\in (0,1]$$, gives us $$x$$'s domain: it is defined over $$S$$, which we have defined to be $$(0,1]$$. The right side of the "such that" states that every x in the domain $$S$$ that is in some partition $$A_i$$ is in this set. Recall that $$A_i$$ was defined as $$[(1/i), 1]$$. 

The above statement seems self-evident for rational numbers: for any rational number $$m/n, m &gt; 0 \wedge n &gt; 0$$, we know that there is a coarse-grained bound such that $$1/n$$ is less than or equal to $$m/n$$ and therefore $$m/n$$ is in $$A_n$$. But what about irrational numbers? For this to work, we would need a theorem that gives rational bounds on irrational numbers. This seems like something that ought to be out there, but I'm not sure where to look. I suspect hard-core PL theory, such as work on PCF, would have something to say about this. Number theory and/or real analysis would also be good candidates. 

In any case, we are trying to make the argument that for every number on the real interval $$(0,1]$$, there exists at least one sub-interval with rational endpoints, and that the union of these sub-intervals gives us back the interval $$(0, 1]$$ exactly. We don't miss any numbers on the interval $$(0, 1]$$. We might make some argument about the compactness of the interval.

The second statement defines intersection in this context. Here we would make an argument about the uniqueness of the intervals. That is, if $$i\not = j$$, then $$A_i \not = A_j$$. Every sub-interval is unique by virtue of its lower bound. However, the upper bound (1) is included in every interval. Therefore, the only interval all sub-intervals could share is the point 1.

It seems the point of the example was to show that countably infinite sets do occur and that the principles of set theory still hold. Defining the sample space as the reals appeals to practical considerations not fully explored in the text: we often model phenomena we measure as having infinite precision, but we know that our instruments can only be finitely precise. A countably infinite partition seems like it would introduce less error into a calculation than a finite one, i.e., a histogram. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2480</wp:post_id>
		<wp:post_date>2015-01-05 11:53:16</wp:post_date>
		<wp:post_date_gmt>2015-01-05 16:53:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>stats-review-infinite-sets</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="casellaberger"><![CDATA[casella&amp;berger]]></category>
		<category domain="category" nicename="stats"><![CDATA[stats]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Coq Blocked</title>
		<link>http://blogs.umass.edu/etosch/2015/01/07/coq-blocked/</link>
		<pubDate>Wed, 07 Jan 2015 17:28:05 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2574</guid>
		<description></description>
		<content:encoded><![CDATA[Some time ago <a href="http://cs.umass.edu/~cibelemf">Cibele</a> and I were working on a fun logic project that involved encoding classical logic in Coq. Perhaps it seems odd that we would write a logic language in language that has a logic already encoded in it, but this is a classic application of PL goodness (i.e., nonsense): we used Gallina as our meta language to encode the primitives of our target language (propositional logic, for starters). You can see the fruits of our efforts <a href="https://github.com/etosch/logic">here</a>.

Our initial goal was to be able to prove the first homework assignment using just Coq. We needed to define concepts such as suitability, satisfiability, validity, etc. We ran into problems during our first pass because we tried to define everything as a function, which doesn't lend itself to proofs. 

In order to proof even the most simple theorems, we needed to define the concept of suitability. We started by defining atomic formulae and assignments:

<code>
Inductive atomic :=
| A : nat -&gt; atomic. 

Definition assignment : Set := list (atomic * bool).
</code>

Many of things we wanted to prove relied on an assignment being suitable for a formula. However, we did not want to have to traverse the assignment or prove suitability or a number of trivial cases each time we needed suitability. Our initial pass used option types extensively, but this made many of the proofs cumbersome and unruly. It also lead us astray on more complicated proofs. So, we defined some concepts with dependent types to clean things up a bit:

<code>
Fixpoint in_assignment n (a : alist) : Prop :=
match a with
| nil =&gt; False
| (h,_)::t =&gt; if beq_nat n h
then True
else in_assignment n t
end.
 
Lemma in_empty : forall a, in_assignment a nil -&gt; False.
intros; compute in H; apply H.
Qed.
 
Fixpoint find_assignment n (a : alist) : in_assignment n a -&gt; bool :=
match a with
| nil =&gt; fun pf =&gt; match (in_empty n) pf with end
| (h, tv)::t =&gt; if beq_nat h n
then fun _ =&gt; tv
else find_assignment n t
end. 
</code> 

The above defines a function, <code>find_assignment</code> that gets around using option types. If we could get the above to work, then we could use <code>in_assignment</code> to define suitability and replace <code>in_assignment n a</code> with <code>suitable n a</code>. You can see a discussion of the problem at this <a href="https://gist.github.com/etosch/76f56c1114418339d6e0">gist</a>, where we simplified assignments to be an associative list, so we wouldn't have to worry about atomic types. 

The main problem, as discussed on the gist, is that we needed to apply a proof in the inductive case of <code>find_assignment</code>. We couldn't figure out how to do this because we were focused on finding errors in <code>find_assignment</code>. The problem with our code was actually in <code>in_assignment</code>: we were using <code>beq_nat</code> to evaluate down to a boolean, rather than using a proposition that employed sumbool (which would give us a bunch of decidibility theorems for free). ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2574</wp:post_id>
		<wp:post_date>2015-01-07 12:28:05</wp:post_date>
		<wp:post_date_gmt>2015-01-07 17:28:05</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>coq-blocked</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="coq"><![CDATA[Coq]]></category>
		<category domain="category" nicename="formal-methods"><![CDATA[Formal Methods]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Notes from Charles Sutton&#039;s Talk</title>
		<link>http://blogs.umass.edu/etosch/2015/01/12/notes-from-charles-suttons/</link>
		<pubDate>Mon, 12 Jan 2015 21:31:53 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2588</guid>
		<description></description>
		<content:encoded><![CDATA[About a month ago, <a href="http://homepages.inf.ed.ac.uk/csutton/">Charles Sutton</a> stopped by UMass to give a talk called "<a href="https://people.cs.umass.edu/~mlfriend/pmwiki/pmwiki.php?n=Main.StatisticalAnalysisOfComputerProgram">Statistical Analysis Of Computer Programs</a>." Here are my lightly-edited, cleaned-up (ish) notes on the talk (this approach inspired by <a href="http://ezyang.tumblr.com/">ezyang</a>'s amazing note-taking abilities):

conceit -- text as language
(bad metaphor)
computer program = precise set of instructions
people -- more aspects (social interactions)
language a good metaphor as a mean of human communication

when writing code, others may want to use later
next person might be you!
research:
-- lots of open source code online
-- lots of implicit knowledge in how to write software -- libraries api, avoid bugs
-- easy to read, easy to maintain
take implicit knowledge and make explicit
writing code in a new library, look at what people did in the past
suggest patterns
key insight: means of communication
regularities of natural language (NL) may be found in programming languages (PL)
no new machine learning 
apply existing techniques to statistical NLP, new patterns
coding conventions
-- names, formatting
summarization
-- get a compressed representation of long verbose source files
mining idioms
-- small syntactics patterns in code
describe how we use them

NL coding conventions
local-&gt;global movement of abstraction
what kinds of coding conventions?
examples:
junit example
create input stream in java
create an identifier
name of input stream
maybe know the type of name someone would use who contributed to the junit project
formatting--e.g. braces
coding convention is a syntactic constraint beyond that imposed by the languages grammar
programmers themselves decide to impose on top of what the compiler requires them to do
developers care a lot
style checkers style guide

small amount of research in software engineering
how they use these in software engineering...

go through conventions, find out what's important -- big commercial (microsoft) projects

threads different aspects of code committed

38 percent related to conventions rather than functionality
(why i hate code review!)

why not just run a formatter over the code?
corner cases -- it doesn't handle for you
renaming variables to be more consistent  (<a href="http://www.jsnice.org/">jsnice</a>)
review time -- can talk more about the functionality
where do conventions come from?
- implicit from code base
one programmer starts, others pick it up
emergent quality 
mores, rather than laws
large number of software constraints, well modeled with statistical machine learning
even with a lot of programming experiences, won't know about how things are named
coding convention inference problem --&gt; why not use machine translation to take my conventions and change them to your conventions?

eclipse plugin called devstyle

click on your identifier, will give you a list of other names
renaming suggestions
how should class objects be named
some disagreement with conventions
we can suggest a name
go through a region and rank names
block of code someone wants to add to the project

how large of a corpus would you need?

what kind of technology do you use inside the scoring function?
n-gram language model

smoothing, taking into account the constraints of the compiler/language conventions
constraint is library of changes
only renaming, not generating syntactically correct (would not be syntactically correct)

pull together all uses of the identifier

look at the set of all other names that have been used anywhere in previous or succeeding context
ask ngram language model of joint prob. of entire file -- sounds really expensive
-- actually using any ngram centered about the thing we want to rename

<a href="http://ciir.cs.umass.edu/~dietz/">laura</a>: coreference analysis on the code
-- knowing that i and i are the same gives you a nonlinear language mode
-- can you get something more robust
-- tapping the compiler for name resolution
-- how do you corporate into the model (only incorporate into the suggestions -- done post hoc)

score by ngram model, threshold so user doesn't see terrible suggestions
side effect of architecture:
don't want names to be very common
system does not choose really common names
sympathetic uniqueness principle
have variable program entity, give unusual name
have a very domain specific thing, such as smoothing, choose an appropriate name, considered appropriate big statistical properties

in training set, if we have an id that occurs infrequently, give ? for unknown (deals with tails of the distribution)
suggestion process for alternatives, use known token
whatever context, use rare word, don't suggest change

what happens if you have a common word, but there should be an unusual -- don't know if there is an answer to this common/question

like adding a new table in a conditional random field (CRP)

formatting conventions
encoding spacing decisions as tokens (indexed by location, foo)

use same framework or suggestions on this basis

does this thing work?
evaluation methodology
automatic evaluation:
-- doesn't say what this is (is it the generative thing, idempotency -- thats how you would explain)
-- should really being doing a case study no user study or human evaluation or whatever

don't want low precision win this tool because people wont use it
95% accuracy -- basically, what google does
can do this for other types of identifiers, but its harder
sympathetic uniqueness -- do we rename everything as i?
x axis is similar, how often do we make revisions
y axis -- element of surprise -- things that were rare, what percentage did we incorrectly try to say were something else
set threshold high, no suggestions, no new names
set threshold low, rename everything

methods, variables, and types are very different
variables and types back off
methods are much more surprising all of the time

naturalize tools

final thing -- test on github, submit patches
look at top suggestions on top 5 suggestions
submitted 18 patches
14 accepted
do programmers really care about this kind of name
suggest that their exceptions be renamed from e to t?
t was okay
throwable (t) to e
people accept t
evidence that programmers think about and care about
paper on arxiv -- fowkes, ranca, allamanis, lapata, sutton

question: exists bad users? 
- AI complete question

question: hard metrics for successful renaming -- most people like it better?
- programmers are picky
- does it actually make programs easier to maintain

question: fancier language model better?
- yes, think so
- types are names recovering -- very conventional (very GP)
- java -- names are 1:1 correspondence to class

question: run on dynamic languages?
-- no results
-- corpus of java, c, python
don't know if its run on python -- i don't think it would work as well on python because it is not as redundant as java

new topic:
autofolding to summarize code

summarize, compress out java boilerplate 
use code folding (which obviously uses the fact that we know blocks are denoted by braces)

is the summary just folding?

difference audiences
task based vs non task based
experience versus novice
expert in project 
first look problem -- opening single file first time, get overview

TASSAL -- tree based autofolding software summarization algorithm
start with file, parse, say we want to compress certain types -- block statements, comments

foldable tree -- subset of AST that contains nodes we could consider following

file  bag of words -- split identifiers by camel case
some of these are going to be generic java stuff
some are concepts used throughout the project

topic models -- find characterizing words for files and packages; tried method as leaves, but this was too sparse

for each node, pick a mixing distribution

single topic per file -- packages or other levels of abstraction &lt;-- wonder if you could use this for refactoring

-- fit the model
this will give us for each token in the source file an indicator variable whether this thing is generated from java, package, file (how characteristic)

think of optimization problem
vector u binary vector
each element indicates whether node is folded
-- okay, so topic models for summarization, dug
look at all tokens assigned to file via generative process
empirical distribution of nodes included in the summary

constraints within budget for length of the summary

tree consistency
-- if a node is included in the summary, must include parents

optimise via greedy algorithm

question: what about naming or other conventions that are drawn from different natural language distributions?
-- clusters of developers that following different conventions -- cluster developers together with both formatting naming -- models don&#039;t work as well

question: what if i have multiple devs collaborating ON THE SAME FILE?
-- topic per continent
-- run topic per content
-- where does z come from?

taking topic models and applying to name? not done yet

look at example topics
example columns are topics
three background topics (e.g. get string baclue name type object i)
projects: spring, bigbluebutton
files: datasourceutils, qualsp

to evaluate:
create gold standard 
folded files manually to measure precision and recall
compare with 
javadocs -- always include, add random
shallowest to deep
expand nodes in order of length
heuristic, but thats all eclipse is doing -- comparing with state of the art

second:
show summaries to developers
6 developers, avg. 4 years industrial experience
-- rate conciseness and usefulness 
these are more concise and useful
automatic summaries from TASSAL were better than any of the other baselines


third thing:
mining idioms from code using existing NLP tools

what are code idioms?

example: reading into a buffer, iterating over an array 
-- are these all things that are encapsulated by other abstractions?
opening resources/context -- common pattern
need meta variables 

code idiom is a syntactic code fragment that recurs frequently across software projects and has a single semantic purposes

-- wondering if you could learn to match ASTs from one language to another (from one that has these higher level abstractions to one that doesn&#039;t)

idiom-related tools -- intellij and eclipse

no way of identifying which idioms are useful (presumably to add them to the IDEs -- how do you find new ones)

other types of code patterns
-- surface level -- code clones copy past code garments
api patterns usage patterns of methods

idiom mining problem --
can i find these templates?

use a probabilistic grammar
CFG slides, pCFG slide

use tree substitution grammar -- generalization of a tree joining grammar
non-terminal can expand into a tree instead of a list of terminals and nonterminals

can make a probabilistic version

tree substitution grammar over tree nodes and regular expansions
represents a family of idioms

this will allows us to represent these idioms

input: probabilistic tree substitution grammar within
take a corpus of ASTs
learn the grammar
every tree rule in the TSG that i learn i treat as an idiom

convert into a textual representation 

build a library of idioms to show developers
how do we infer the grammar?

maximum likelihood conditioned on the pTSG rules

previous work from sharon goldwater et al
-- infer what these trees are, pick list of trees that best explain the corpus
-- number of possible things i could put in theta are intractable, maximum likelihood is degenerate, pick 1:1 rules to trees

don&#039;t make tree fragments too big
put a prior on probabilistic grammars
if you&#039;re going to add another tree, this is what the idiom would like
get a joint distribution over pCFGs and source files
dist. over dist. of parse trees

given a corpus code of code to get a distribution over probabilistic grammars over trees i&#039;ve inferred
type-based MCMC from liang et al; (think this is from liangs GP-like work)


some questions i didn&#039;t catch

mined idioms
iterator, loop through lines, logger for class, string constant
get patterns you would actually find
get something from actual APIs
e.g. database transaction (opening a resource and cleaning up properly)
get the distance between two points in ??
jsoup get mhtl

lots of work in SE in API mining
no syntactically nested things

take a held out set of files
percentage of AST nodes explained by the ---/
existing method for clone detection
completely duplicated ?

copy paste phenomenon

idioms we find occur across projects much more often?
SE perspective -- dozens of papers in SE about copy past clones

if these things are really idioms, maybe they will occur more often in example code -- actually what happens
from a data set of regular projects on github, we find 22% of idioms found are actually used in examples, higher in stack overflow

finally, can do a co-occurrence matrix -- how are idioms used across different projects
eclipse snipmatch
-- open source addition to eclipse -- manually took 44 snippets, stuff worked or something
cant put all the idioms in the tool, so many found
considered this as validation that the thing works
interesting that there i was one idiom used that is considered bad practice

exploiting that source code is a means of human communication

maybe surprising to people who are from a different background, that you would need to train the model

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2588</wp:post_id>
		<wp:post_date>2015-01-12 16:31:53</wp:post_date>
		<wp:post_date_gmt>2015-01-12 21:31:53</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>notes-from-charles-suttons</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Using 3x</title>
		<link>http://blogs.umass.edu/etosch/2015/01/25/using-3x/</link>
		<pubDate>Mon, 26 Jan 2015 03:45:30 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2596</guid>
		<description></description>
		<content:encoded><![CDATA[A few months ago, Emery pointed out a new project being advertised on Stanford's CS web page. It's called <a href="http://cs.stanford.edu/research/3x-workbench-executing-exploratory-experiments">3x</a> and describes the system:

<blockquote>
3X is an open-source software tool to ease the burden of conducting computational experiments and managing data analytics. 3X provides a standard yet configurable structure to execute a wide variety of experiments in a systematic way, avoiding repeated creation of ad-hoc scripts and directory hierarchies. 3X organizes the code, inputs, and outputs for an experiment. The tool submits arbitrary numbers of computational runs to a variety of different compute platforms, and supervises their execution. It records the returning results, and lets the experimenter immediately visualize the data in a variety of ways. Aggregated result data shown by the tool can be drilled down to individual runs, and further runs of the experiment can be driven interactively. Our ultimate goal is to make 3X a smart assistant that runs experiments and analyzes results semi-automatically, so experimenters and analysts can focus their time on deeper analysis. Two features toward this end are under development: visualization recommendations and automatic selection of promising runs.
</blockquote>

The <a href="https://github.com/netj/3x">github repository</a> describes a system for planning and re-running experiments. The tool manages inputs and outputs and produces a factorial design on the basis of the inputs. It isn't clear to me whether a more sophisticated design is allowed. 

I attempted to run a 3x experiment over the bocado work. The idea was to run with and without tracing, at varying sizes of number of functions sampled. Installing the tool seemed fine, but getting it to work on my machine was another problem. I tried both the executable provided and building from source. Initially the problem seemed to be with the custom file-watcher the author built. An error was being thrown in the GUI script when I tried to start that up after specifying the experiment. When I tried running the experiment with `3x run`, nothing appeared to happen. After some fiddling and starting again from scratch, I was able to get something running, although it didn't look like my program was running, nor did the GUI appeared to work. There was limited debug information, so after a while I just gave up. The documentation for this tool includes screen shots of the GUI, which makes me think it works somewhere. 

I would definitely be interested in using this tool in conjunction with some of our tools. I noticed in <a href="https://github.com/netj/3x/issues/28">one of the issue comments</a> that there appears to be another, similar tool called <a href="http://neuralensemble.org/sumatra/">Sumatra</a>, which I will check out in the future.

Of interest to us is the last statement on the Stanford page, stating that the goal of 3x is for it to operate "semi-automatically." It promises development of "visualization recommendations and automatic selection of promising runs". I am interested in seeing if/how that pans out.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2596</wp:post_id>
		<wp:post_date>2015-01-25 22:45:30</wp:post_date>
		<wp:post_date_gmt>2015-01-26 03:45:30</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>using-3x</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Speeding up SurveyMan analyses</title>
		<link>http://blogs.umass.edu/etosch/2015/02/09/speeding-up-surveyman-analyses/</link>
		<pubDate>Mon, 09 Feb 2015 18:58:29 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2607</guid>
		<description></description>
		<content:encoded><![CDATA[A major bottleneck in some of our analyses is that we need to resample survey responses. Let $$n$$ denote the number of responses we've seen. Let $$m$$ denote the number of questions in the survey. $$b_i$$ is the number of bootstrap iterations. $$b_s$$ is a list of bootstrap samples. $$scores$$ is a list of scores. Our resampling approach is as follows:

1. For each response $$r$$ in the list of all responses ($$O(n)$$):
&nbsp;&nbsp;&nbsp; a. $$srs \gets$$ All other survey responses that have answered at least all the questions in $$r$$ ($$O(n)$$).
&nbsp;&nbsp;&nbsp; b. For $$sr$$ in $$srs$$ ($$O(n)$$):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i. Truncate $$sr$$ so we are only looking at the set that overlaps with $$r$$ ($$O(m)$$).
&nbsp;&nbsp;&nbsp; c. For $$i \gets 1$$ to $$b_i$$ ($$O(b_i)$$):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i. Randomly select $$|srs|$$ samples from $$srs$$ and add to the list $$b_s$$ ($$O(n)$$).
&nbsp;&nbsp;&nbsp; d. For $$b$$ in $$b_s$$ ($$O(b_i)$$):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i. Compute the scores for $$b$$ and add to the list $$scores$$ ($$O(n)$$).
&nbsp;&nbsp;&nbsp; e. Sort $$scores$$ in ascending order ($$O(b_i\log b_i)$$).
&nbsp;&nbsp;&nbsp; f. Return true or false if $$r$$'s score falls outside the appropriate density ($$O(1)$$).



<h3>Reminder: randomization and equivalence are non-trivial</h3>
The randomization of SurveyMan introduces some non-trivial complications to the above process. I've <a href="http://blogs.umass.edu/etosch/2014/07/31/defining-equality/">written</a> <a href="http://blogs.umass.edu/etosch/2014/07/31/metrics-to-answer-our-research-questions/">before</a> <a href="http://blogs.umass.edu/etosch/2014/06/19/what-does-it-mean-for-a-survey-to-be-correct-a-first-stab-at-formalizing-surveyman/">about</a> how (1) relies on defining the notion of question equivalence carefully. When we have variants, if we attempt to match on the literal question, we may not have a statistically significant number of samples to compare. Consider the prototypicality survey. In that case, we had 16 blocks, each having 4 variants. This means we have up to $$4^{16}$$ distinct surveys! Resampling won't help us at all in that case.

<h3>Can we resample fewer times?</h3>
Although resampling becomes possible when we collapse variants into a single question, it's still time-consuming. Calculating scores for the phonology survey -- which had almost 100 questions, and for which we gathered about 300 responses -- takes upwards of 25 minutes. It would be nice if we could do this faster. 

The current resampling code is very naive. As written above, we truncate the response lists first and then resample. In order to truncate, we first compare our response $$r$$ with our $$n$$ survey responses. Resampling involves drawing from the pool of $$n$$ survey responses $$b_i$$ times. When I am impatient, I set $$b_i$$ to 500. When I want to do things the right way, I set $$b_i$$ to 2000. Clearly the number of $$b_i$$ dominates the computation. We end up with $$O(n(n + nm + 2b_i n + b_i \log b_i))$$ = $$O(b_i n^2 + b_i\log b_i)$$ running time.

<h4>Caching</h4>
Would things be any better if we only computed the bootstrap samples once, and performed the truncation later? Let's consider the following alternative algorithm:

1. For $$i \gets 1$$ to $$b_i$$ ($$O(b_i)$$):
&nbsp;&nbsp;&nbsp; a. Randomly select $$n$$ samples from the responses and add to the list $$b_s$$ ($$O(n)$$).
2. For each response $$r$$ in the list of all responses ($$O(n)$$):
&nbsp;&nbsp;&nbsp; a. For $$b$$ in $$b_s$$ ($$O(b_i)$$):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i. $$srs \gets$$ All other survey responses that have answered at least all the questions in $$r$$ ($$O(n)$$).
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ii. For $$sr$$ in $$srs$$ ($$O(n)$$):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A. Truncate $$sr$$ so we are only looking at the set that overlaps with $$r$$ ($$O(m)$$).
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B. Compute the scores for $$b$$ and add to the list $$scores$$ ($$O(n)$$).
&nbsp;&nbsp;&nbsp; b. Sort $$scores$$ in ascending order ($$O(b_i\log b_i)$$).
&nbsp;&nbsp;&nbsp; c. Return true or false if $$r$$'s score falls outside the appropriate density ($$O(1)$$).

The above gives us a running time of $$O(b_in + n(b_i(n + n(n + m)) + b_i\log b_i))$$  = $$O(b_in + b_in^2 + b_in^3 + b_inm + b_i\log b_i)$$ = $$O(b_in^3)$$. Yikes! Even though we are only computing the bootstrap sample once, we need to iterate over it. This iteration occurs in an outer loop, causing a blowup in the time to compute. 

There are clearly more subtle analyses we can do. The first approach only computes the bootstrap sample over the truncated responses, which are often fewer than the total number of responses. We might be concerned about the garbage collector when we recompute new samples. 

Another concern I have with caching is that it introduces a bias. We are essentially reusing the same data (the bootstrap samples) and may run into multiple comparison issues. Of course, the point of the bootstrap simulation is that it approximates the true distribution and therefore this should be less of an issue the more bootstrap iterations we have. 

<h4>Parallelizing</h4>
Another approach to speed things up would be to try to parallelize the score computation. When all of the analysis was written in Clojure, this would have been fine, since nothing was mutable. However, the Java implementation mutates scores in the original SurveyResponse object. We could get around this by completely decoupling the truncated view from the original SurveyResponse. I might do this to see if it makes a difference.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2607</wp:post_id>
		<wp:post_date>2015-02-09 13:58:29</wp:post_date>
		<wp:post_date_gmt>2015-02-09 18:58:29</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>speeding-up-surveyman-analyses</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Automated Hypothesis Generation Based on Mining Scientific Literature</title>
		<link>http://blogs.umass.edu/etosch/2015/02/09/automated-hypothesis-generation-based-on-mining-scientific-literature/</link>
		<pubDate>Mon, 09 Feb 2015 22:52:27 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2659</guid>
		<description></description>
		<content:encoded><![CDATA[The title of this blog post is a paper that recently appeared at KDD. Here is <a href="http://delivery.acm.org/10.1145/2630000/2623667/p1877-spangler.pdf?ip=128.119.40.193&amp;id=2623667&amp;acc=ACTIVE%20SERVICE&amp;key=73B3886B1AEFC4BB.0404F0890BAA435B.316E9A8C8A786537.4D4702B0C3E38B35&amp;CFID=620569742&amp;CFTOKEN=62130492&amp;__acm__=1423247674_97b9de9f9761bccc12bcf13b137080cd">a link that probably won't work</a>. This post will be a running commentary on the paper as I read it.

<h3>Abstract</h3>
"Current search technologies typically find many relevant documents, but they do not extract and organize the information content of these documents or suggest new scientific hypotheses based on this organized content."

Okay, so there are a few big ideas here wrapped up in one. Information retrieval and information extraction are two sides of the same coin. This is what <a href="http://cs.umass.edu/~jfoley">John</a> does.  Organization of the content is also related to what his group does and is certainly on the cutting edge of IR/IE. The step of suggesting hypotheses is more traditional AI, and I wonder how we extend from the more empirical, metrics-based approaches of the aformentioned fields to the more theoretical, rules-based approaches of hypothesis generation.

"KnIT...further reasons upon these data to generate novel and experimentally testable hypotheses."

Cool!

"KnIT combines entity detection with neighbor-text feature analysis and with graph-based diffusion of information to
identify potential new properties of entities that are strongly implied by existing relationships."

Wow, sounds good. Entity detection is certainly hot right now, and folks from CIIR are very into it. I don't know anything about the graph-based diffusion of information bit, but it sounds like it uses information-theoretic metrics to tell how far an effect/feature/something reaches?

<h3>Introduction</h3>
"Even recognizing new questions that should be asked can be a challenge. Instead, only a sliver of the relevant knowledge guides hypotheses: an approach that is deeply wasteful. This fundamental bottleneck is pervasive in biology and representative of every area of human endeavor in which there is a mushrooming mismatch between raw information and our analytic abilities. "

"Our goal is to accelerate scientific progress by combining mining, visualization, and analytics, with the hope to integrate all available content, identify the facts that are relevant to a given query, and from these facts suggest hypotheses that are new, interesting, testable and likely to be true."

Lofty! Also, similar to some goals we have. 

Their running case study will be the protein p53, which is a tumor suppresor.

KnIT stands for "Knowledge Integration Toolkit." It has three phases: exploration, interpretation, and analysis.

Exploration involves some kind of extraction to generate features. The features index entities. Presumably the search space is the space of entities.

Interpretation involves connecting entities into a graph. There are some higher dimensional visualization techniques that can be used to highlight "interesting" areas, e.g., coloring. "Interesting" parts of the graph are subgraphs that denote relationships between entities that have not previously been discovered.

Analysis involves "globally diffus[ing] annotation information among entities" in order to rank them and select out the best entities for experimentation. I'm not sure what this means -- experimentation is only partially over entities. Presumably the experiment is to test a causal relationship between entities? If that's the case, then the graph representation needs a richer link structure than just "these are related."
NOTE: the relationship in this case is whether a kinase "phosphorylates" p53, so it is a specific causal relationship.

They cite past work on hypothesis generation, and automatic inference of known formulae. However "hypothesis generation from unstructured text has been a hit-or-miss manual process."

<h3>Impact on society</h3>
Lofty. Our brains are limited. We aren't google (or cyborgs yet). Invoke Big Data. 

<h3>The Problem of p53 Kinases</h3>
High-level discussion of p53. The takeaway: "knowing which proteins are modified by each kinase, and therefore which kinases would make good drug targets, is a difficult and unsolved problem."

<h3>Representing Kinases</h3>
They mine papers (especially abstracts) for keywords, essentially. These define the feature space (bag of words). They use a knowledge base that contains the canonical name and all known synonyms. The system then queries a paper database with a giant OR for all synonyms and extracts features from those papers' abstracts. They removed data that explicitly mentioned p53, since they were interested in discovering new relationships (rather than discovering those that had already been discovered and published). They represent each document (abstract) as a weighted normalized vector and remove stop words. They use tf-idf for their weights.

After this first pass, they calculate kinase centroids in the feature space. Then they calculate the Euclidean distance between every centroid. This gives some numeric sense of how "close" two kinases are. However, it is not immediately clear how even a domain expert would interpret these numbers.

They consider displaying the information as a network graph, but this model isn't quite right, since network graphs rely on nodes being connected or not connected, and here every node is "connected." They note that they could threshhold the distance to denote connectivity, but this would confer an interval interpretation to the distance, when ratio is really all that's warranted. 

They discuss visualization and HCI issues: how does one best communicate the data? They want to pare down the information sufficiently so that the user is not overloaded. They decide that the features they want are (a) minimal connectivity (minimize information presented), (b) tree-like (for the ease of navigation and the relative relationships between levels), and (c) low-arity (not sure what their argument is about -- they just don't want to have a linked list or something). They choose as the root the feature vector that is closest to the average of all the centroids and call this property "typicality."

<h3>Selecting Candidate p53 Kinases</h3>
They need a ranking for the kinases and use graph diffusion to accomplish this:

"Graph diffusion is a semi-supervised learning approach for classification based on labeled and unlabeled data. It takes known information (initial labels) and then constrains the new labels to be smooth in respect to a defined structure (e.g. a network)."

Their known data are the p53s. The unknown ones are the other kinases.

They define a kinase network as the top 10 most closely related kinases. This number was chosen via cross validation. They are minimizing the error in predicted values less a smoothing term. They end up with a convex optimization problem. They show some ROC curves for their cross validation.

<h3>Results</h3>
They did three studies:
<ol>
	<li>They did a retrospective analysis that looked at whether the model could predict discoveries after a particular date, given publications that occurred strictly before that date. This is a nice baseline, but might be a bit misleading -- the interesting parts of science are the big leaps, not the incremental discoveries. The predictions in the retrospective study should be a minimum requirement, and not even a proof of concept. </li>
	<li>They compared the approach with human-driven state of the art discovery approaches and looked at areas for improvement.</li>
	<li>Their third study involved searching for discoveries at scale (the real promise of this work).</li>
</ol>

I skimmed the rest of the section, which was filled with domain details and reports on how things went. Without a comparative study, it's hard to believe any "conclusion," and besides they state up front that this is a proof of concept anyway.

<h3>Conclusion and Future Work</h3>
Repetition of the intro. ENTITIES.


 

Does this approach generalize? There was still a lot of domain knowledge baked in. Only time will tell...]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2659</wp:post_id>
		<wp:post_date>2015-02-09 17:52:27</wp:post_date>
		<wp:post_date_gmt>2015-02-09 22:52:27</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>automated-hypothesis-generation-based-on-mining-scientific-literature</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Smarter scheduling in SurveyMan</title>
		<link>http://blogs.umass.edu/etosch/2015/06/02/smarter-scheduling-in-surveyman/</link>
		<pubDate>Tue, 02 Jun 2015 14:50:25 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2667</guid>
		<description></description>
		<content:encoded><![CDATA[Conventional wisdom (and testimonials from researchers who have been burned) says that time of day can introduce bias into crowdsourced data collection. Right now, SurveyMan posts a single HIT per survey, requesting $$n$$ assignments. If we collect $$n$$ assignments and find that they are low quality, we ask for more by extending the HIT. 

What happens if we get $$n$$ valid responses in the first hour of posting? Is the distribution of responses going to be the same as if we had posted $$n$$ hits, distributed throughout the day? If I am posting surveys about American politics, I will want have them available when the largest number of American Turkers are active. However, if I am asking for annotations, do I need to be conscious of potential differences? The question of bias is insidious because we don't know precisely when it applies. <a href="http://www.andrewmao.net/">Andrew Mao</a> has written about <a href="http://www.andrewmao.net/2014/06/when-to-schedule-coordinated-work-on-amazon-mechanical-turk">scheduling tasks during peak AMT worker hours</a>. However, there's still a lot domain knowledge and planning involved. Planning properly requires constant vigilance, since it's not even clear that peak worker hours will remain the same over time: <a href="http://link.springer.com/chapter/10.1007/978-3-319-16268-3_39#page-1">a recent paper</a> found that the alleged biases in the mechanical turk population had either sorted themselves out or had been overstated. Conversely, Ipierotis et al. <a href="http://www.behind-the-enemy-lines.com/2015/04/demographics-of-mechanical-turk-now.html">established</a> <a href="demographics.mturk-tracker.com/#/gender/all">an AMT demographic tracker</a>, which can help identify subtle population biases.

Regardless of whether or not biases exist, most machine learning models that use AMT data account for this in some way. There is typically some unknown bias term drawn from a reasonably well-behaved distribution that can then be marginalized. When demographers and pollsters tackle this issue, they typically know something about the underlying population and account for uneven sampling with this prior information. However, when we don't know anything about what the underlying population is supposed to look like, or if we have little prior information for our variables of interest, we may be in a bit of a bind.

<h3>Toward automatic detection of population differences</h3>
As an alternative to these approaches, I am implementing a prototype scheduler in SurveyMan that dynamically tests for biases. Let's start with the basic assumption that there are no biases in data collection and that people answer our HITs within an hour of our posting. Since we cannot be sure of our assumption, we post a HIT with $$n/2$$ assignments at $$t_0$$ and $$n/2$$ assignments at $$t_{12}$$, where the subscripts indicate hours from the start of data collection. We schedule these two batches 12 hours apart in an attempt to get a kind of maximum difference in populations: as the researcher, if I am kicking off a survey at this time, chances are people who share demographic features to me will also be awake and working when I am working. However, 12 hours from now, I expect to be asleep and it might happen that the people who are taking my survey are quite different.

First challenge: how do we even tell if there are differences in the survey responses? 

<h4>Approach 1: Check for differences in the distributions observed for each question.</h4>
We look at the responses generated for a set of questions at two different times and calculate whether the distributions are significantly different. Since we will probably end up with a bunch of low-powered comparisons, we are likely to detect a difference. However, since we know the number of questions we'll ask (and therefore the number of comparisons we'll make) <em>a priori</em>, we should be able to model our false positive rate. 

How many questions must be different for us to consider the populations fundamentally different? What happens if we find a significant difference between the responses for a particular question, but this question doesn't have an impact on the analyses we might do? For example, suppose that we find different responses for a control question, but no difference in the questions of interest. Should we run the survey again? 

Maybe one way of thinking about this approach is that it's like an AutoMan approach, but in batch mode. I like to tell people that the way we came up with the idea for SurveyMan started out as a way to deal with batches of AutoMan tasks that converged to a distribution, rather than a point. Looking for individual differences in questions is a related problem, but it doesn't really leverage running things in batch.

<h4>Approach 2: Look for differences in correlations.</h4> 
For small numbers of questions, differences in distributions may suffice, but for a more complex survey, a more informative measure might be to look for differences in correlations between questions. This may do a better job of highlighting "important" differences in populations. Since it is very unlikely that we would find correlation coefficients that are exactly the same, we would need to be careful about how we might compare discovered correlations. How much variation should we expect? What's our baseline? Zero correlation seems silly; is there a more meaningful baseline? Surely the baseline would depend on the survey itself. 


If we expect there to be fluctuations in the demographics of AMT workers, why don't we just post our surveys in slow progression -- maybe one per hour? In addition to the troubles caused by the underlying AMT system (we get a boost when we first post; after a certain amount of time, engagement tapers off), we waste time doing this. It also isn't clear what the scale of variation is -- should we post over the course of a day, a week, a month, or a year? Some AMT demographic surveys run for at least one year. Clearly this is infeasible for many other types of research (e.g., the work we'd been doing with the linguists). 


]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2667</wp:post_id>
		<wp:post_date>2015-06-02 10:50:25</wp:post_date>
		<wp:post_date_gmt>2015-06-02 14:50:25</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>smarter-scheduling-in-surveyman</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="crowdsourcing"><![CDATA[crowdsourcing]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="surveyman-2"><![CDATA[surveyman]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Rage Against the Machine Learning</title>
		<link>http://blogs.umass.edu/etosch/2015/04/29/rage-against-the-machine-learning/</link>
		<pubDate>Wed, 29 Apr 2015 14:29:15 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2677</guid>
		<description></description>
		<content:encoded><![CDATA[Woah, it's been a while since I actually made a blog post. I'm going to to try to make a blog post every day until I head off for my Facebook internship in June (super psyched!). I'm putting it in writing, so that's how you know I'm definitely going to break my word...

I owe the above title to <a href="http://www.brynosaurus.com/">Bryan Ford</a>, who listened to me raging against Machine Learning this afternoon. Bryan is visiting for our first <a href="http://plasma.cs.umass.edu/emery/systems-lunch.html">systems lunch</a> in a while. It's been a long hiring season...

Anyway, back to the complaint at hand. Today was the final machine learning class of the semester and people were presenting their projects. One of the groups wasn't really a group, so much as two students working loosely on the same approach to different problems. At the end of their presentation, I asked what commonalities there were between the two projects, other than both using CRFs to model the problem. They said that they were both trying to show that some kind of regularization would cause improvement on the state of the art of their respective data sets. I then asked if they were using the same base code. The students responded that they were not: one was implemented in Matlab and the other in Scala. They had reasons for this (mainly, the Scala student uses Scala in his lab, and he seemed to have scared off the Matlab student from using Scala because of its "learning curve" -- see, Scala, you seriously need some better PR). I was troubled that they weren't using comparable backends, since this meant they would definitely not be able to generalize without modeling the impact of measurement on their results. I didn't ask if they were implementing the state of the art in their systems, so as to make them comparable (although I should have). In the end, I thought maybe it wouldn't have that much of an impact.

At the end of the class, I picked up my final "mini-project." We were given some sample output to test against and for my implementation of a one-vs-all multiclass classifier using a polynomial kernel, I was only able to get a test accuracy of 78.70%, versus the expected test accuracy of 87.3%. The TA (whom I don't mean to criticize -- he's done a great job in this class, and I really appreciate it!) had written "too low, but I couldn't find errors in your code. May be some small bugs." Now, I had originally implemented other versions of other parts of this project; they too were also too low. Those older versions of the code were written using for-loops. After porting my loops over to matrix multiplications, I was able to improve the accuracy of my test set to equal that of the provided checkpoints. All this is to say that I strongly suspect the offending code (then one-vs-all multiclass classifier using a polynomial kernel) is semantically equivalent to a matrix-operation-only version, but will produce "better" results. This (of course!) is an exercise in dealing with numeric stability. 

<h2>The futility of comparisons?</h2>
So of course looking at my homework and reflecting on my question for my colleagues, I felt confirmation that since we can't even compare semantically equivalent computations when written in the same language, how can we compare across languages? Part of the reason why people use libraries like BLAS is because they are very efficient, but it also has the effect of determinism when comparing against other implementations -- we don't have faithful models of computation to provide meaningful comparisons between implementations when people use different floating point operations on different architectures. These issues are clearly tied to problems of reproducibility that have been getting a lot of buzz in our community lately.

<h2>Topical: Measurement</h2>
Immediately before class I had seen <a href="http://andrewgelman.com/2015/04/28/whats-important-thing-statistics-thats-not-textbooks/">Andrew Gelman's post on how we undervalue measurement in statistics</a>. Although numerical analysis has been tackled by a lot of people who are way smarter than I am, it still causes problems for practitioners. With the rise of deep learning, big data, and data science, more and more people are using tools whose shortcomings are non-obvious. As a computer scientist, I am aware that floating point numbers are problematic, and that compounded floating point error can be a problem. I was still shocked by how much of a problem it was. I expected a small amount of variation, but was surprised that my result deviated as much as is did. Of course, the problem could still be in the code itself -- after all, it isn't statically verified. However, I still noticed differences in test accuracy for the other code that I converted from loops to matrix operations. These differences were on the order of a few percentage points. Although an order of magnitude smaller than the difference in the kernelized one-vs-all classifier, the difference of two percentage points or so actually makes a result publishable (at least in some ML/NLP conferences I've seen). 

I wonder if we would get some variation in the test accuracy using loops just from randomizing the layout of some junk under the surface. Lisps have been partially addressing numerical stability by having ratio types. There must be some languages that do some semi-symbolic computations, simplifying and deferring floating point operations until a value is needed. If there were true, they would be able to return some bound on the error of that computation. 

<h2>Rise of the Machines</h3>
So is there a way to convert loops to matrix operations automatically? I asked <a href="http://people.cs.umass.edu/~jfoley/index.html">John</a> if he had ever heard of such a thing and he said that there's "vectorization" (a term I must have heard before, but have forgotten due to my advanced age), but he isn't aware of any compiler optimizations for as high a level as I was suggesting. Since the <a href="http://julialang.org/">Julia</a> folks seem to be at the forefront of numerical computing right now (or are at least getting a lot of inflated press about it right now), I thought I'd look into what they have to say, but it doesn't look like it.  Are there constrained cases where we could do this? Maybe there's a FFTW-style meta-programming approach someone could employ.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2677</wp:post_id>
		<wp:post_date>2015-04-29 10:29:15</wp:post_date>
		<wp:post_date_gmt>2015-04-29 14:29:15</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>rage-against-the-machine-learning</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Participation and Contribution in Crowdsourced Surveys</title>
		<link>http://blogs.umass.edu/etosch/2015/05/12/participation-and-contribution-in-crowdsourced-surveys/</link>
		<pubDate>Tue, 12 May 2015 13:15:05 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2689</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0120521&amp;representation=PDF">Participation and Contribution in Crowdsourced Surveys</a>, a recent PLOSOne article, discusses some interesting approaches to crowdsourced surveys. Not only are the answers crowdsourced, but the questions themselves are also crowdsourced. The surveys are seeded with a small number of questions and later augmented with questions supplied by respondents. These questions are curated by hand and presented to respondents in a random order.

<h2>Approach and comparison with <a href="http://www.ipeirotis.com/wp-content/uploads/2014/01/fp267-ipeirotis.pdf">Quizz</a></h2>
The authors accomplished this by setting up three separate websites to collect the data. The only one that is still active is for <a href="http://energyminder.net/">Energy Minder</a> which is an ongoing research project. The other two surveys were about BMI and personal finance. 

The motivation for this work is very similar to <a href="http://www.ipeirotis.com/wp-content/uploads/2014/01/fp267-ipeirotis.pdf">Quizz</a>. The authors state:

<blockquote>
The crowdsourcing method employed in this paper was motivated by the hypothesis that there are many questions of scientific and societal interest, for which the general (non-expert) public has substantial insight. For example, patients who suffer from a disease, such as diabetes, are likely to have substantial insight into their condition that is complementary to the knowledge of scientifically-trained experts. The question is, how does one collect and combine this non-expert knowledge to provide scientifically-valid insight into the outcome of interest.
</blockquote>

Like Quizz, their system eschews financial incentives for survey completion. Unlike Quizz, new questions are added by the users themselves, rather than by a system. In Quizz, the objective is to complete a knowledge base -- responses to questions are point estimates. In this system, the questions serve as features designed to predict a variable of interest, whether it be energy consumption, BMI, or the amount an individual has in their savings. The paper does not explicitly state the measurement level of the outcome variable; it isn't clear if, for example, energy consumption is a binary variable (high/low), a categorical variable defined by buckets, or a real-valued prediction of kWh.

<h2>Questions, Observations, Insights</h2>
<ul>
<li>Are there any baseline/ground-truth studies? All three surveys ask questions that could be influenced by a variety of biases (e.g., Vermonters are hippies who desire a low energy footprint, biases against overweight and poor people, etc.).</li>
	<li> One big advantage of using crowdsourced questions is that they can give insights for how to get around social desirability bias. This isn't discussed in the paper, but would of interest to social scientists.</li>
	<li>Early in the paper they state, "...a real-time machine learning module continuously builds models from this growing store of data to predict the outcome variable, and the resulting predictions are shown to the users." The machine learning module they refer to is Hod Lipson's symbolic regression package. It's not clear to me when the predictions are shown. Aren't there methodological issues with telling the respondent what you're trying to predict? Although this can sometimes be the case, social desirability and other biases may have a significant impact on the outcome variable.</li>
<li> Related work: <a href="http://research.microsoft.com/en-us/um/people/livshits/papers/pdf/popl15a.pdf">Program Boosting</a> uses GP and crowdsourcing.</li>
<li>"If the user decides to submit their own question, they are then required to respond to it. We have found that being forced to immediately respond to the question helps users to recognize when their questions are unclear or confusing." Do they have revision data for questions? Or do respondents just re-enter the question if it isn't clear? Is there feedback on question clarity, or is this something that the human curator determines? It's not clear to me how this works, but this data might be an interesting feature to use in quality control.</li>
<li>Beyond surveys, this is an interesting way to collect features for some other task. The questions are basically features here.</li>
<li>Problems of propagation of error could be connected to issues we're looking at in Automan vis a vis multiple comparisons.</li>
<li>The learning module: could we use these techniques to build up blocks dynamically? Learn blocks?</li>
</ul>

<h2>Criticisms</h2>
<ul>
<li>The validation checks (e.g. valid ranges) are a very weak adversarial model. Since there are no financial incentives for this survey, a greater threat to validity are inattentive respondents. </li>
<li>I'd like to see a stronger comparison with the active learning literature. There are issues of compounded error when using stepwise regression and the kind of user-generated question dogfooding fu that's happening here. I suspect the active learning literature addresses some of these issues and would give insight into how to have greater statistical validity.</li>
<li>Testing for a correlation coefficient different from 0 is too sensitive. This hardly ever happens. To guard against this, or at least establish a kind of prior on false correlations, the authors could inject seemingly unrelated questions into the survey. Of course, there is some probe bias here that could cause unintended consequences, so it would have to be thought out carefully. I'm just not satisfied with, "The lack of correlation between participation and contribution falsifies the hypothesis that a higher level of participation is indicative of interest in and knowledge of the subject area."<li>
<li>Also asked above: what's the baseline? What's to stop the system from predicting the most common answer, given the class? How does this perform against a naive Bayes or decision tree classifier?</li>
<li>I would like to see some regularlization in the modelling. Symbolic regression can be very sensitive to outliers. I'm not sure what's in this implementation, though. The paper would benefit from a discussion of regularization.</li>
</ul>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2689</wp:post_id>
		<wp:post_date>2015-05-12 09:15:05</wp:post_date>
		<wp:post_date_gmt>2015-05-12 13:15:05</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>participation-and-contribution-in-crowdsourced-surveys</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>&quot;On the expressive power of programming languages&quot;</title>
		<link>http://blogs.umass.edu/etosch/2015/05/17/on-the-expressive-power-of-programming-languages/</link>
		<pubDate>Sun, 17 May 2015 20:10:20 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2699</guid>
		<description></description>
		<content:encoded><![CDATA[In response to a <a href="https://www.youtube.com/watch?v=k4t8KisRznQ">Keanu moment</a> I recently had, <a href="http://people.cs.umass.edu/~arjun/home/">Arjun</a> recommended I read <a href="http://www.sciencedirect.com/science/article/pii/016764239190036W">On the expressive power of programming languages</a>. The paper is divided into roughly two parts: in the first part, Felleisen sets up the formal framework for describing the expressiveness of programming languages. In the second part, he illustrates the repercussions with Scheme.

Let $$\mathbb{F}_1,\ldots,\mathbb{F}_n,\ldots$$ be a set of constructs we wish to add to a language $$\mathcal{L}_0$$. Let this new language be denoted by $$\mathcal{L}_1$$. These languages have equivalent expressiveness if (a) adding the constructs does not change the semantics of the subset of $$\mathcal{L}_1$$ that can be expressed with just the constructs in $$\mathcal{L}_0$$ and (b) the newly added constructs can be expressed using <i>local changes</i> in the program. An important enforcement of the locality requirement is that the mapping between the original language and the extension must be the identity on the formulae in the language and a homomorphism on the local connectors. The formalism that he defines enforces these requirements. The main conclusion of the paper is that "...an increase in expressive power comes at the expense of less 'intuitive' semantic equivalence relations."

The proofs are quite nice and clear. It's a journal article, so it's long, but thorough. There's an interesting discussion in the second half about how introducing <code>abort</code> and <code>call/cc</code> mess up the semantics of the program. I had had difficulty imagining concrete examples to illustrate some of the issues that showed up as abstractions in the first half, so the Scheme section was helpful. 

It isn't clear to me whether there was a big, definitive result that came out of this work. I'll have to investigate more.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2699</wp:post_id>
		<wp:post_date>2015-05-17 16:10:20</wp:post_date>
		<wp:post_date_gmt>2015-05-17 20:10:20</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-the-expressive-power-of-programming-languages</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>This one time, I wrote something in Perl</title>
		<link>http://blogs.umass.edu/etosch/2015/05/19/this-one-time-i-wrote-something-in-perl/</link>
		<pubDate>Wed, 20 May 2015 01:02:32 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2713</guid>
		<description></description>
		<content:encoded><![CDATA[I was just looking at some old crappy code I wrote four years ago, to show <a href="http://blogs.umass.edu/mmcma0/">Molly</a> that one should have no shame when posting code online. While looking at my own HMM code, I ran across <a href="https://github.com/etosch/HMM/blob/master/scripts/treebank.pl">this code</a>. I have no idea what it does. I just remember wanting to use Perl for string munging crap. Yikes!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2713</wp:post_id>
		<wp:post_date>2015-05-19 21:02:32</wp:post_date>
		<wp:post_date_gmt>2015-05-20 01:02:32</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>this-one-time-i-wrote-something-in-perl</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Counterfactual Estimation and Optimization of Click Metrics for Search Engines</title>
		<link>http://blogs.umass.edu/etosch/2015/10/17/counterfactual-estimation-and-optimization-of-click-metrics-for-search-engines/</link>
		<pubDate>Sat, 17 Oct 2015 21:46:57 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2822</guid>
		<description></description>
		<content:encoded><![CDATA[So <a href="http://eytan.github.io/">Eytan</a> suggested <a href="http://research.microsoft.com/pubs/210171/paper-tr.pdf">this paper</a> as a reference for our work on static analysis for PlanOut, but I was recently thinking about how some of the ideas might be ported to SurveyMan. 

Here is a breakdown of the process/logic of this paper:
<ol>
	<li>There are two majors problems for people who study user behavior and search engine foo. First of all, the metrics they are trying to optimize cannot be computed ahead of time -- they depend on user behavior, and this behavior is highly contextual. As an example, they cite relevance judgements as inherently flawed, because they have no notion of context. They describe how annotators will rank celebrity's home pages as highly relevant, but search logs reveal different user behavior. Secondly, to compute the true causal effect, researchers need to know "counterfactual" information -- i.e., what the person would have done under different conditions that did not actually occur. This information is needed in order to make decisions about features: people want to know if a particular feature tweak will lead to more promising behavior, such as clicking on ads. Counterfactual estimation is limited in this context because many features interact, the dimension of the factor tuple may be high, and there may not be sufficient replicates in the data to account for covariates. One solution is to take into account a sample of actual user behavior.</li>
	<li>Of course, this can all be solved by running randomized experiments. However, experiments are expensive! They take time and money! Can something else be done?</li>
	<li>When attempting offline estimations, we need a proxy, since we cannot actually measure the things we want to measure! They don't say this in the paper, but presumably there is some kind of bound on the difference between the proxy and the true metric of interest (but if there isn't, it's a really bad proxy!)</li>
	<li>Solution: use the search log to generate counterfactuals using some kind of model, and run A/B tests on the generated data set.</li>
	<li>Solution: for the model, use contextual bandits. What's a contextual bandit, you ask? It's a vanilla bandit that uses information from the environment to adapt the weights of the arms. I like to think of it as "JITting" the bandit algorithm, but I'm not sure that analogy actually holds.</li>
	<li>The formalism is a tuple of observed context and a deferred observed reward, plus an action (I say deferred because, as in the usual bandit framework, the reward is not observed until an action is chosen. So, the action causes a state transition.)</li>
	<li>The goal is to maximize the expectation of the joint distribution of the environment and actions. Note that this is a different goal from [what I understand of] the traditional bandit framework: we might think of the traditional bandit framework as optimizing the conditional distribution of the action, conditioned on the reward. Here, the users actually control the context (or at least elements of it), so it take this context into account when making choices.</li>
	<li>Their technique relies on randomly selecting the data (i.e. the tuple of context and reward). One thing that is not clear to me is what the authors do in the face of sparsity -- what happens when they don't cover all the possible tuple values? Their comment that this technique only works when all of the propensity scores are greater than zero implies that this leads to some nasty effects.</li>
	<li>So it sounds like they:
<ol>
<li>Mine the search logs for all combinations of contexts and *potential* rewards chosen, assuming they are drawn from some unknown distribution (recall: for any individual, they only have one $$(x, r_a)$$, corresponding to the action actually chosen). Note that even though we can't fill in $$K-1$$ cells of the reward vector, this doesn't matter when sampling the tuple.</li>
	<li>Compute the propensity scores for actions. Note that we don't know this! The probability of taking an action is not known. It may not be independent from the context, and there may be high variance in distribution of the actions chosen by all respondents across all actions for a particular context. The authors work backwards to infer propensity scores -- they use the logs to check that the expected value of a possible action's score is equal to the score of complement of the action. Some other methods for estimating propensity scores are also suggested. Note that they basically trying to simulate user behavior. In PlanOut, the propensity scores are typically controlled by the system (although for bandits, there is an external process that needs to be reasoned about).</li>
	<li>Use these propensity scores to simulate behavior and compute the reward foo.</li>
</ol>
	</li>
<li> The rest of the paper describes estimating confidence intervals and gives some real-life examples that use domain expertise to help with calculations.
</ol>

So what I was thinking for SurveyMan was that this issue of power came up when I gave a talk at <a href="http://cs.union.edu/seminar/tosch.html">Union College</a>. Of course with these potentially very large number of randomizations, statistical power becomes an issue. However, there might be a way to bootstrap some simulations using known data and interpolating the unknown data. I'll need to revisit the interpolation literature, since that's basically what I'd be doing. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2822</wp:post_id>
		<wp:post_date>2015-10-17 17:46:57</wp:post_date>
		<wp:post_date_gmt>2015-10-17 21:46:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>counterfactual-estimation-and-optimization-of-click-metrics-for-search-engines</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Crowdsourcing system basics</title>
		<link>http://blogs.umass.edu/etosch/2015/11/04/crowdsourcing-system-basics/</link>
		<pubDate>Wed, 04 Nov 2015 16:39:59 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?p=2831</guid>
		<description></description>
		<content:encoded><![CDATA[Some time ago I started writing up the requirements for a sound crowd sourcing system. Last year I wrote about Amazon's report on using verification tools at scale and it got me thinking about the kinds of abstractions one would need to formally verify a crowdsourcing system.

Before implementing any verification system, it's important to to know what the basic guarantees are -- what invariants we should define, so that we may define what it means for the system to be correct? Crowdsourcing is used in a variety of contexts, and different crowdsourcing systems serve different labor markets. In particular, I will focus on general human intelligence tasks and explicitly will not address specialized markets.

In particular, I would like to address three rough categories of tasks: point estimation tasks, distribution estimation tasks, and causal inference tasks.
<h3>Point estimation tasks</h3>
Point estimation tasks are those that have a human-verifiable single solution. They are the kinds of tasks that AutoMan is designed to handle. These tasks cover annotation tasks, data cleaning, and spammy tasks such as following a link to vote for something. The unifying feature of these tasks is that another human can evaluate them. These are the kinds of tasks that may eventually be performed by machines, but currently still require some curation by humans. Point estimation tasks use agreement to evaluate the quality of responses returned.

<b>Correctness</b>: If a task cannot be verified by a human curation (i.e., agreement cannot be reached), then the program is incorrect. The inherent truth of the underlying system allows us to use traditional statistical methods for convergence in the limit (e.g., hypothesis testing).

<b>Requirements</b>: The main requirements of a crowdsourcing system that handles these tasks are that there is no collusion between respondents and that each sample is unique. That is, each time the system requests a response, that response must come from a different person.
<h3>Distribution estimation tasks</h3>
Distribution estimation tasks are those that may not converge to a point, but whose response distribution we expect to stabilize for a given point in time with sufficient data. In point estimation tasks, this would denote an error, which brings up an important distinction in how we model and reason about correctness for different crowdsourcing tasks: there is no single canonical criterion for correctness. How we classify tasks is critically important. It is possible that a researcher will consider a particular task to be a point estimation task, but in truth it is a distribution estimation task. Mis-classification is, of course, a bug, but it's a peculiar kind of bug -- one that can only be both caused and detected by a human. Distribution estimation tasks are the kinds of tasks that SurveyMan is designed to handle.

<b>Correctness</b>: This problem is a bit tricky -- in SurveyMan, we try to do as much offline analysis as possible, to bound our certainty of classification, given what we know about about the instrument used to collect data. In particular, when using surveys, questionnaires, or collections of polls to collect data, there may be subtle biases introduced via the framing of the problem and imprecision in natural language. The kinds of tasks completed and questions answered are understood to not have a "gold standard." These tasks are typically administered as collections of tasks or questions. The tasks or questions that comprise the single survey or questionnaire or otherwise complex task each have varying degrees of variability -- individuals may change their opinions over the course of a survey, they may change their country of residence between surveys, and they may change their gender over their lifetime. Each item we try to measure has some degree of impermanence and my current research attempts to use these distinctions to help with our analyses. In any case, the point of this section is that correctness for these tasks differs dramatically from correctness for point estimation tasks.

<b>Requirements</b>: We assume that these tasks have been designed for a general audience. The main requirements are that (1) respondents do not collude, (2) every qualified respondent is equally likely to have been exposed to the task (i.e., every qualified respondent has had the opportunity to accept or ignore the task), (3) the system permits an adaptive pricing strategy. Requirement (2) is a bit tricky for several reasons. Firstly, we need some way of gather demographic information. AMT has basic demographic information (most importantly for Amazon -- country of employment) and allows requesters to filter respondents via a qualifications mechanism. If the task requires qualifications that AMT cannot express (e.g., handedness), or if we are working on a system that does not have qualifications, the complex structure of the task allows requesters to write a "short circuit" into the task.
<h3>Causal Inference Tasks</h3>
Causal inference tasks can be viewed as a combination of the previous two tasks. In some ways they are more easily described and tested than the others. However, they have other complications that will be instrumental in our requirements specification for crowdsourcing systems. Causal inference tasks are those that measure the effect of a particular treatment. The <a href="https://en.wikipedia.org/wiki/Stroop_effect">Stroop effect</a> is a classic observation from psychology that requires randomized exposure to two types of stimuli in within-subjects test. The infrastructure from AutoMan and SurveyMan can be used to express these tasks, with some additional logic provided by our in-progress project, ExperiMan. I have been meaning to look into how the <a href="http://www.cs.berkeley.edu/~kjamieson/resources/next.pdf">NEXT</a> <a href="https://github.com/nextml/NEXT">system</a> for active learning works because I think there might be some insights to be gained by this.

<b>Correctness</b>: The standard way to measure a causal effect is to run an experiment and to estimate the effect of counterfactual exposures. Estimation requires certain randomization prerequisites be met and that the quantity being estimated be congruent with the assignment procedure.

<b>Requirements</b>: The main requirement is that assignment to treatment and control is unbiased. The unbiased guarantee should be similar to the one for point estimation. We would also like to be able to do some more sophisticated assignment procedures that would be difficult to manage in standard crowdsourced experiments, such as conditional random assignment on the basis of demographic information learned over time. One way to handle this is to break the experiment into smaller experiments (i.e. assignment procedures) and express them as chained tasks. This is like the qualifications mechanism, only broader and requiring that we have more complete information about respondents.

Note that we do not list longitudinal tasks above. A fantastic extension of a crowdsourcing system would be to support panels, for time-varying tasks.
<h2>System Components</h2>
AMT already has some of the major components we list here. What follows is a discussion of how we might design a crowdsourcing system, if the objective for the system designers were principled analyses, rather than profit.
<h3>Scheduling</h3>
When scheduling tasks, we need some way of differentiating <b>temporal scale</b>. Some questions or tasks are not always relevant. Some questions have a particular lifetime. Other questions are important over longer periods of time (this brings us back to the longitudinal task mentioned above).

We already know of some challenges with scheduling imposed by systems such as AMT:
<ul>
	<li><b>UI Design</b> has a huge impact on what users see. A good system balances the users needs with the requester needs. The users are loosely maximizing some utility function relating perhaps to their targeted rate of compensation (citation, someone? I've heard that people do this, but have not read the papers myself), or enjoyment. The requesters want the job done at the cheapest rate possible. We generally know the requirements of the requesters -- inferring the requirements of the users is typically a much harder task. Fundamentally, we are talking about a matching problem, though. At the moment, systems such as AMT just post the most recent jobs. This (amongst other shortcomings in the system) leads to degenerate behavior in the workers -- they participate in offline discussions about tasks, which compromises requesters' desire for no collusion and/or independence in responses. A <b>matching algorithm</b> between users and workers would help. It need not be sophisticated -- just a random sample of the top $$n$$ posts for which the user is qualified. This is, in fact, a requirement for the above classes of tasks!</li>
	<li><b>Centralized/shared data</b> would be useful for requesters. This would reduce repetition across tasks and corroborate results, leading to more <b>more robust results via replication</b>. An easy way to accomplish this would be to embed the system in an existing social media system, where users explicitly provide significant information about themselves, and where they reveal information about themselves implicitly.</li>
	<li><b>Searching for work</b>: It is not clear to me whether users ought to be allowed to explicitly search for work. However, a search mechanism might also alert us to when collusion occurs</li>
	<li><b>Payment</b> Aside from the obvious technical/legal challenges of paying workers, we also want the payment system to not interfere with the revelation of users' utility functions. In particular, we do not want the payment system to mask user behavior. There was some work a few years ago that argued that paying people more did not result in better work, and I've written before about how I think that this is an artifact of the overly discretized way AMT is set up. Since then, I've seen: (1) Google Consumer Surveys, which uses surveys as a paywall, (2) some other Google Survey App that asks me marketing questions on my Android phone and gives me Play Credits, which I use to buy overpriced e-versions of comedienne's memiors, (3) Quizz, another Google system that uses hypochondriacs' specialized knowledge of WebMD to answer health questions, but doesn't pay them and (4) some Facebook survey system that I didn't actually get to play around with when I was there and have never been exposed to, probably because I spend so little time on Facebook.</li>
	<li><b>Rating systems for requesters</b>: Clearly the biggest problem with AMT has been that there is no recourse for the rejection of work and lemon requesters are everywhere. This has lead to people using Turkopticon and other fora to discuss HITs. The lack of an anonymous system and the lack of an anonymous rating system for specific tasks pushes people to discuss tasks online, which undermines our no-collusion and independence requirements</li>
	<li><b>Redefining units</b>: For all of the discussion about the importance of no-collusion and independence, there may be cases where we want interaction between units. Of course, in these cases, the unit would be the interacting pair. Currently AMT does not allow this kind of interaction, and I am unsure if any other systems have these built in.</li>
</ul>
What's really provocative to me is that this is another extension of the human-in-the-loop way of looking at programming languages and systems. Of course here at UMass we've seen Lori Clarke et al. use process languages to formalize human-in-the-loop processes in the medical domain. What differentiates a process language from a programming language? The process language describes what should happen -- it's a fine model to use for the system as-in, in dynamic analyses. The real benefit to using programming languages is in the static analyses. There's always some meta-process or pipeline for a particular task. Embedding assumptions into programming languages is a way to push as much of the decision-making as possible as early in the pipeline as possible. It allows us to "see into the future" and constrain our actions so that we only do things that are sound with regard to the specification.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2831</wp:post_id>
		<wp:post_date>2015-11-04 11:39:59</wp:post_date>
		<wp:post_date_gmt>2015-11-04 16:39:59</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>crowdsourcing-system-basics</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[20775]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Quotes!</title>
		<link>http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=1912</link>
		<pubDate>Thu, 31 Jul 2014 22:59:07 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=1912</guid>
		<description></description>
		<content:encoded><![CDATA[[["My estimated time (in minutes)","Price (in USD)"],["5","300"],["10","300"],["15","300"],["20","300"],["25","300"],["45","300"],["55","450"],["60","450"]]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1912</wp:post_id>
		<wp:post_date>2014-07-31 18:59:07</wp:post_date>
		<wp:post_date_gmt>2014-07-31 22:59:07</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>closed</wp:ping_status>
		<wp:post_name>quotes</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>tablepress_table</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>_tablepress_export_table_id</wp:meta_key>
			<wp:meta_value><![CDATA[6]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_options</wp:meta_key>
			<wp:meta_value><![CDATA[{"last_editor":20775,"table_head":true,"table_foot":false,"alternating_row_colors":true,"row_hover":true,"print_name":false,"print_name_position":"above","print_description":false,"print_description_position":"below","extra_css_classes":"","use_datatables":true,"datatables_sort":true,"datatables_filter":true,"datatables_paginate":true,"datatables_lengthchange":true,"datatables_paginate_entries":10,"datatables_info":true,"datatables_scrollx":false,"datatables_custom_commands":""}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_visibility</wp:meta_key>
			<wp:meta_value><![CDATA[{"rows":[1,1,1,1,1,1,1,1,1],"columns":[1,1]}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Thirteen Question Quiz</title>
		<link>http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=2278</link>
		<pubDate>Mon, 03 Nov 2014 05:33:25 +0000</pubDate>
		<dc:creator><![CDATA[etosch]]></dc:creator>
		<guid isPermaLink="false">http://blogs.umass.edu/etosch/?post_type=tablepress_table&#038;p=2278</guid>
		<description></description>
		<content:encoded><![CDATA[[["1"," The center of the Earth is very hot.","<b>True<\/b> | False","True"],["","One plus one is three.","True | <b>False<\/b>","False"],["2","The continents on which we live have been moving their locations for millions of years and will continue to move in the future.","<b>True<\/b> | False","True"],["3","Does the Earth go around the Sun, or does the Sun go around the Earth?","<b>Earth around Sun<\/b> |\nSun around Earth","Earth around Sun"],["","Strawberries are red.","<b>True<\/b> | False","True"],["4","All radioactivity is man-made.","True | <b>False<b>","False"],["5","Electrons are smaller than atoms.","<b>True<\/b> | False","True"],["","There are five hours in a day.","True | <b>False<\/b>","False"],["6","Lasers work by focusing sound waves.","True | <b>False<\/b>","False"],["7","The universe began with a huge explosion.","<b>True<\/b> | False","True"],["8","What is your gender?","Male | Female",""],["9","What is your age?","18-24 | 25-34 | 35-44 | 45-54 | 55-64 | 65 or older",""],["10","What is your highest level of education?","Didn't finish high school | Finished high school | Some college | Finished college | Graduate\/professional degree",""]]]]></content:encoded>
		<excerpt:encoded><![CDATA[The quiz questions as reported in http://www.cs.dartmouth.edu/farid/downloads/publications/pus14.pdf.]]></excerpt:encoded>
		<wp:post_id>2278</wp:post_id>
		<wp:post_date>2014-11-03 00:33:25</wp:post_date>
		<wp:post_date_gmt>2014-11-03 05:33:25</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>closed</wp:ping_status>
		<wp:post_name>thirteen-question-quiz</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>tablepress_table</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>_tablepress_export_table_id</wp:meta_key>
			<wp:meta_value><![CDATA[7]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_options</wp:meta_key>
			<wp:meta_value><![CDATA[{"last_editor":20775,"table_head":false,"table_foot":false,"alternating_row_colors":true,"row_hover":true,"print_name":false,"print_name_position":"above","print_description":false,"print_description_position":"below","extra_css_classes":"","use_datatables":true,"datatables_sort":true,"datatables_filter":true,"datatables_paginate":true,"datatables_lengthchange":true,"datatables_paginate_entries":10,"datatables_info":true,"datatables_scrollx":false,"datatables_custom_commands":""}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_tablepress_table_visibility</wp:meta_key>
			<wp:meta_value><![CDATA[{"rows":[1,1,1,1,1,1,1,1,1,1,1,1,1],"columns":[1,1,1,1]}]]></wp:meta_value>
		</wp:postmeta>
	</item>
</channel>
</rss>
